encoding="utf-8" The Electrical Engineering Handbook       Third Edition    Broadcasting and Optical Communication     TechnologyThis page intentionally left blank The Electrical Engineering Handbook Series  Series Editor Richard C. Dorf University of California, Davis  Titles Included in the Series The Handbook of Ad Hoc Wireless Networks, Mohammad Ilyas The Avionics Handbook, Cary R. Spitzer The Biomedical Engineering Handbook, Third Edition, Joseph D. Bronzino The Circuits and Filters Handbook, Second Edition, Wai-Kai Chen The Communications Handbook, Second Edition, Jerry Gibson The Computer Engineering Handbook, Vojin G. Oklobdzija The Control Handbook, William S. Levine The CRC Handbook of Engineering Tables, Richard C. Dorf The Digital Signal Processing Handbook, Vijay K. Madisetti and Douglas Williams The Electrical Engineering Handbook, Third Edition, Richard C. Dorf The Electric Power Engineering Handbook, Leo L. Grigsby The Electronics Handbook, Second Edition, Jerry C. Whitaker The Engineering Handbook, Third Edition, Richard C. Dorf The Handbook of Formulas and Tables for Signal Processing, Alexander D. Poularikas The Handbook of Nanoscience, Engineering, and Technology, William A. Goddard, III,   Donald W. Brenner, Sergey E. Lyshevski, and Gerald J. Iafrate The Handbook of Optical Communication Networks, Mohammad Ilyas and   Hussein T. Mouftah The Industrial Electronics Handbook, J. David Irwin The Measurement, Instrumentation, and Sensors Handbook, John G. Webster The Mechanical Systems Design Handbook, Osita D.I. Nwokah and Yidirim Hurmuzlu The Mechatronics Handbook, Robert H. Bishop The Mobile Communications Handbook, Second Edition, Jerry D. Gibson The Ocean Engineering Handbook, Ferial El-Hawary The RF and Microwave Handbook, Mike Golio The Technology Management Handbook, Richard C. Dorf The Transforms and Applications Handbook, Second Edition, Alexander D. Poularikas The VLSI Handbook, Wai-Kai ChenThe Electrical Engineering Handbook               Third Edition                   Edited by               Richard C. Dorf    Circuits, Signals, and Speech and Image Processing    Electronics, Power Electronics, Optoelectronics,      Microwaves, Electromagnetics, and Radar    Sensors, Nanoscience, Biomedical Engineering,               and Instruments Broadcasting and Optical Communication Technology Computers, Software Engineering, and Digital Devices    Systems, Controls, Embedded Systems, Energy,               and Machines  The Electrical Engineering Handbook                                              Third Edition                Broadcasting and Optical Communication                               Technology                                                     Edited by                                  Richard C. Dorf                                     University of California                                    Davis, California, U.S.A.                                                     Boca Raton   London   New York                              A CRC title, part of the Taylor & Francis imprint, a member of the                             Taylor & Francis Group, the academic division of T&F Informa plc.7338_Discl.fm  Page 1  Thursday, November 17, 2005  3:26 PM                     Published in 2006 by                   CRC Press                   Taylor & Francis Group                    6000 Broken Sound Parkway NW, Suite 300                   Boca Raton, FL 33487-2742                    © 2006 by Taylor & Francis Group, LLC                   CRC Press is an imprint of Taylor & Francis Group                   No claim to original U.S. Government works                   Printed in the United States of America on acid-free paper                   10987654321                   International Standard Book Number-10: 0-8493-7338-7 (Hardcover)                    International Standard Book Number-13: 978-0-8493-7338-1 (Hardcover)                    Library of Congress Card Number 2005054345                   This book contains information obtained from authentic and highly regarded sources. Reprinted material is quoted with                   permission, and sources are indicated. A wide variety of references are listed. Reasonable efforts have been made to publish                   reliable data and information, but the author and the publisher cannot assume responsibility for the validity of all materials                   or for the consequences of their use.                    No part of this book may be reprinted, reproduced, transmitted, or utilized in any form by any electronic, mechanical, or                   other means, now known or hereafter invented, including photocopying, microfilming, and recording, or in any information                   storage or retrieval system, without written permission from the publishers.                     For permission to photocopy or use material electronically from this work, please access www.copyright.com                   (http://www.copyright.com/) or contact the Copyright Clearance Center, Inc. (CCC) 222 Rosewood Drive, Danvers, MA                   01923, 978-750-8400. CCC is a not-for-profit organization that provides licenses and registration for a variety of users. For                   organizations that have been granted a photocopy license by the CCC, a separate system of payment has been arranged.                    Trademark Notice: Product or corporate names may be trademarks or registered trademarks, and are used only for                   identification and explanation without intent to infringe.                                                   Library of Congress Cataloging-in-Publication Data                            Broadcasting and optical communication technology / edited by Richard C. Dorf.                                p. cm.                             Includes bibliographical references and index.                             ISBN 0-8493-7338-7                             1.  Optical communications. 2.  Broadcasting. 3.  Telecommunication. I. Dorf, Richard C. II. Title.                            TK5103.59.B76 2005                           621.382--dc22                                                                      2005054345                                                                           Visit the Taylor & Francis Web site at                                                                          http://www.taylorandfrancis.com                                     Taylor & Francis Group              and the CRC Press Web site at                               is the Academic Division of Informa plc.   http://www.crcpress.com                                                                       Preface   Purpose The purpose of The Electrical Engineering Handbook, 3rd Edition is to provide areadyreferencefor the practicing engineer in industry, government, and academia, as well as aid students of engineering.The third edition has anew look and comprisessix volumes including:   Circuits, Signals, and Speech and Image Processing   Electronics, Power Electronics, Optoelectronics, Microwaves, Electromagnetics, and Radar   Sensors, Nanoscience, Biomedical Engineering, and Instruments   Broadcasting and Optical Communication Technology   Computers, Software Engineering, and Digital Devices   Systems, Controls, Embedded Systems, Energy, and Machines   Each volume is edited by Richard C. Dorf, and is acomprehensive format that encompasses the many aspects of electrical engineering with articles from internationally recognized contributors. The goal is to provide the most up-to-date information in the classical ﬁelds of circuits, signal processing, electronics, electromagnetic ﬁelds, energydevices, systems, and electrical effects and devices, while covering the emerging ﬁelds of communications, Nanotechnology, biometrics, digital devices, computer engineering,systems, and biomedical engineering.Inaddition, the ﬁnal section provides acomplete compendium of information regarding physical, chemical, and materials data, as well as widely inclusiveinformation on mathematics. Many articles from this volume and the other ﬁve volumes havebeen completely revised or updated to ﬁt the needs of today,and manynew chapters have been added.   The purpose of Broadcasting and Optical Communication Technology is to provide aready reference to subjects in the ﬁeld of communications, including broadcasting,equalization, optical communication, computer networks, ad hoc wireless networks, information theory, satellites and aerospace, digital video processing, and mobile communications. Hereweprovide the basic information for understanding these ﬁelds. We also provide information about bandwidth modulation, phase-lockedloops, telemetry, and computer-aided design and analysis of communication systems.  Organization The information is organized into twomajor sections. The ﬁrst section encompasses 13 chapters and the last section summarizes the applicable mathematics, symbols, and physical constants.   Most articles include threeimportant and useful categories: deﬁning terms, references, and further infor- mation. Deﬁning terms are key deﬁnitions and the ﬁrst occurrence of each term deﬁned is indicated in boldface in the text. The deﬁnitions of these terms are summarized as alist at the end of each chapter or article. The references provide alist of useful books and articles for follow-up reading.Finally, further information provides some general and useful sources of additional information on the topic.  Locating  Your  Topic Numerous avenues of access to information are provided. Acomplete table of contents is presented at the front of the book. In addition, an individual table of contents precedes both sections. Finally,each chapterbegins with its own table of contents. The reader should look over these tables of contents to become familiar with the structure,organization, and content of the book. Forexample, see Section I: Communications, then Chapter 1: Broadcasting,and then Chapter 1.1: Modulation and Demodulation. This tree-and-branch table of contents enables the reader to move up the tree to locate information on the topic of interest.   Twoindexes havebeen compiled to provide multiple means of accessing information: the subject index and index of contributing authors. The subject index can also be used to locate keydeﬁnitions. The page on which the deﬁnition appears for each key (deﬁning) term is clearly identiﬁed in the subject index.   The Electrical Engineering Handbook, 3rd Edition is designed to provide answers to most inquiries and direct the inquirer to further sources and references. We hope that this handbook will be referred to often and that informational requirements will be satisﬁed effectively.  Acknowledgments This handbook is testimonytothe dedication of the Board of Advisors, the publishers, and my editorial associates. Iparticularly wish to acknowledge at Taylor &Francis Nora Konopka, Publisher;Helena Redshaw, Editorial Project Development Manager; and Marsha Hecht, Project Editor.Finally,Iam indebted to the supportofElizabeth Spangenberger,Editorial Assistant.                                                                        Richard  C. Dorf                                                                           Editor-in-Chief                                                    Editor-in-Chief   Richard C. Dorf,  professor of electrical and computer engineering at the UniversityofCalifornia, Davis, teaches graduate and undergraduate courses in electrical engineering in the ﬁelds of circuitsand control systems. He earned aPh.D.inelectrical engineering from the U.S. Naval Postgraduate School, an M.S. from the University of Colorado,and aB.S. from Clarkson University. Highly concerned with the discipline of electrical engineering and its wide value to social and economic needs, he has written and lectured internationally on the contributions and advances in electrical engineering.   Professor Dorf has extensiveexperience with education and industryand is professionally active in the ﬁelds of robotics, automation, electric circuits, and communications. He has servedasavisiting professor at the UniversityofEdinburgh, Scotland; the Massachusetts Institute of Technology; Stanford University; and the UniversityofCalifornia, Berkeley.   Professor Dorf is aFellowofThe Institute of Electrical and Electronics Engineers and aFellow of the American Societyfor Engineering Education. Dr.Dorf is widely known to the profession for his Modern Control Systems, 10th Edition (Addison-Wesley,2004) and The International Encyclopedia of Robotics (Wiley, 1988). Dr.Dorf is also the co-author of Circuits, Devices and Systems (with Ralph Smith), 5th Edition (Wiley, 1992), and ElectricCircuits, 7th Edition (Wiley,2006). He is also the author of TechnologyVentures (McGraw- Hill, 2005) and The Engineering Handbook, 2nd Edition (CRCPress, 2005).This page intentionally left blank                                                     AdvisoryBoard   Frank Barnes                  William  Kersting             Richard S. Sandige UniversityofColorado          NewMexicoState University     California Polytechnic State Boulder,Colorado              Las Cruces,New Mexico           University                                                             San Luis Obispo,California Joseph Bronzino TrinityCollege                Vojin Oklobdzia Hartford, Connecticut         UniversityofCalifornia, Davis Leonard Shaw                               Davis, California             Polytechnic University Wai-Kai Chen                                                Brooklyn, NewYork UniversityofIllinois Chicago,Illinois                               John  V. Oldﬁeld              John W.  Steadman Delores Etter                 Syracuse University           UniversityofSouth Alabama United States Naval Academy   Syracuse, NewYork             Mobile, Alabama Annapolis, Maryland  Lyle Feisel                   Banmali Rawat                 R. Lal Tummala State UniversityofNew York    UniversityofNevada            Michigan State University Binghamton, NewYork           Reno,Nevada                   East Lansing,MichiganThis page intentionally left blank                                                                   Contributors   Stella N. Batalama                KurtL.Kosbar                       Todd R. Reed SUNY at Buffalo                   University of Missouri             University of Hawaii Buffalo,New York                  Rolla, Missouri                    Manoa, Hawaii  Almon H. Clegg                    CarlG.Looney                       Richard B. RobrockII Consultant                        University of Nevada                                                                      Bell Laboratories Inc. Highland, Utah                    Reno,Nevada                                                                      Bedminster,New Jersey Thomas    M.  Cover               Steven L.  Maddy Stanford University               SpectraLink Corporation            Martin S. Roden Stanford, California              Boulder,Colorado                   California State University                                                                      Los Angeles, California Thomas E. Darcie                  RobertJ.Marks II AT&T Bell Laboratories            University of Washington           MatthewN.O. Sadiku Holmdel, NewJersey                Seattle, Washington                Prairie View A&M University                                                                      Prairie View,Texas John N. Daigle                    Stan McClellan University of Mississippi         Hewlett Packard Company            Stanley Salek Oxford,Mississippi                Plano,Texas                                                                      Harnett &Edison, Inc.                                                                      San Francisco,California Daniel  F. DiFonzo                Sarhan M. Musa Planar Communications Corporation Prairie View A&M University Rockville, Maryland               Prairie View,Texas                 Apostolis K. Salkintzis                                                                      Motorola Richard C. Dorf                   Joseph C. Palais                   Athens, Greece University of California          Arizona State University Davis, California                 Tempe, Arizona                     Remzi Seker                                                                      University of Arkansas Stephen Horan                     Nikos Passas                         at Little Rock NewMexico State University        University of Athens               Little Rock, Arkansas Las Cruces, NewMexico             Athens, Greece                                                                      Ronald J. Tallarida Mohammad Ilyas                    H.  VincentPoor                    Temple University Florida Atlantic University       Princeton University               Philadelphia, Pennsylvania Boca Raton, Florida               Princeton, NewJersey                                                                      Moncef B.   Tayahi Dimitri Kazakos                   HaoliQian                                                                      University of Nevada University of Idaho               Marvell Semiconductor Inc. Moscow, Idaho                     Sunnyvale, California              Reno,Nevada  Reza Khosravani                   Banmali S. Rawat                   Charles  W. Therrien Sonoma State University           University of Nevada               Naval Postgraduate School RohnertPark, California           Reno,Nevada                        Monterey,CaliforniaJoy  A. Thomas                    Sergio  Verdu´                     Jerry C. Whitaker Stratify                          Princeton University               Advanced Television Systems Mountain View,California          Princeton, NewJersey                 Committee                                                                      Washington, D.C.  William   H. Tranter              Zhen   Wan                         Alan  E. Willner Virginia Polytechnic Institute    UniversityofTexas                  University of Southern California Blacksburg,Virginia               Richardson, Texas                  Los Angeles, California                                                                  Contents   SECTION       ICommunications    1  Broadcasting      1.1  Modulation and Demodulation Richard C. Dorfand Zhen Wan ............................................ 1-1      1.2  Radio Broadcasting JerryC.Whitaker ...................................................................................... 1-10      1.3  Television Systems JerryC.Whitaker ........................................................................................ 1-24      1.4  High-Deﬁnition Television Martin S. Roden ............................................................................ 1-38      1.5  Digital Audio Broadcasting StanleySalek and Almon H. Clegg ............................................. 1-43    2  Equalization  Richard C. Dorfand Zhen Wan .................................................................................. 2-1    3  Optical Communication      3.1  LightwaveTechnologyfor Video Transmission Thomas E. Darcie .......................................... 3-1      3.2  Long Distance Fiber Optic Communications Joseph C. Palais ............................................... 3-10      3.3  Photonic Networks Alan E. Willner and Reza Khosravani ...................................................... 3-18    4  Computer   Networks      4.1  Computer Communication Networks John N. Daigle .............................................................. 4-1      4.2  Local AreaNetworks Sarhan M. Musa and Matthew N.O. Sadiku ........................................ 4-14      4.3  The Intelligent Network Richard B. Robrock II ........................................................................ 4-23      4.4  Mobile Internet Apostolis K. Salkintzis and Nikos Passas ........................................................ 4-32      4.5  QualityofService in Packet-Switched Networks Stan McClellan and Remzi Seker ............. 4-50    5  Ad  HocWireless  Networks   Mohammad Ilyas ............................................................................. 5-1    6  Information Theory      6.1  Signal Detection H. Vincent Poor ................................................................................................ 6-1      6.2  Noise Carl G. Looney .................................................................................................................. 6-10      6.3  Stochastic Processes Carl G. Looney .......................................................................................... 6-23      6.4  The Sampling Theorem Robert J. Marks II .............................................................................. 6-34      6.5  Channel Capacity Sergio Verdu´ .................................................................................................. 6-41      6.6  Data Compression JoyA.Thomas and Thomas M. Cover ...................................................... 6-49    7  Satellites and Aerospace Daniel F. DiFonzo .................................................................................. 7-1    8  Digital Video Processing Todd R. Reed ......................................................................................... 8-1    9  LowSample SupportAdaptiveParameter Estimation and      Packet-Data Detection      for Mobile Communications    Haoli Qian, Stella N. Batalama, and Dimitri Kazakos ............. 9-110   Bandwidth  Efﬁcient Modulation in Optical Communications   Moncef B. Tayahi and      Banmali S. Rawat .................................................................................................................................... 10-1  11   Phase-Locked Loop    Steven L. Maddy ...........................................................................................11-1  12   Telemetry  Stephen Horan .................................................................................................................. 12-1  13   Computer-Aided   Design and Analysis of CommunicationSystems      William H. Tranter and Kurt L. Kosbar ................................................................................................13-1  SECTION       II  Mathematics,       Symbols,    and  Physical    Constants  Introduction   Ronald J. Tallarida ............................................................................................................... II-1      Greek Alphabet ......................................................................................................................................... II-3      International System of Units (SI) ......................................................................................................... II-3      Conversion Constants and Multipliers .................................................................................................. II-6      Physical Constants .................................................................................................................................... II-8      Symbols and Terminologyfor Physical and Chemical Quantities ...................................................... II-9      Credits..................................................................................................................................................... II-13      Probabilityfor Electrical and Computer Engineers Charles W. Therrien ...................................... II-14  Indexes  Author Index  .................................................................................................................................................... A-1  Subject Index ..................................................................................................................................................... S-1                                                                                I Communications          1Broadcasting   R.C. Dorf, Z. Wan, J.C. Whitaker,M.S. Roden, S. Salek,            A.H. Clegg ...................................................................................................................................... 1-1             Modulation and Demodulation * Radio Broadcasting * Television Systems * High-Deﬁnition             Television * Digital Audio Broadcasting         2Equalization   R.C. Dorf, Z. Wan ............................................................................................... 2-1             Linear Transversal Equalizers * Nonlinear Equalizers * Linear Receivers * Nonlinear Receivers         3Optical Communication   T.E. Darcie, J.C. Palais, A.E. Willner,R.Khosravani................... 3-1             LightwaveTechnologyfor Video Transmission * Long Distance Fiber Optic             Communications * Photonic Networks         4Computer   Networks  J.N. Daigle, S. Musa, M.N.O. Sadiku, R.B. Robrock II,            A.K. Salkintzis, N. Passas, S. McClellan, R. Seker....................................................................... 4-1             Computer Communication Networks * Local AreaNetworks * The Intelligent Network *             Mobile Internet * QualityofService in Packet-Switched Networks         5AdHoc    Wireless Networks M. Ilyas....................................................................................... 5-1             Introduction * Applications and Opportunities * Challenges * Summaryand Conclusions         6Information Theory   H. Vincent Poor,C.G. Looney, R.J. Marks II, S. Verdu´ ,            J.A. Thomas, T.M. Cover............................................................................................................... 6-1             Signal Detection * Noise * Stochastic Processes * The Sampling Theorem * Channel             Capacity * Data Compression         7Satellites and Aerospace D.F.DiFonzo ................................................................................... 7-1             Introduction * Satellite Applications * Satellite Functions * Satellite Orbits and Pointing             Angles * Communications Link * System Noise Temperature and G/T * Digital             Links * Interference * Some Particular Orbits * Access and Modulation * Frequency             Allocations * Satellite Subsystems * Trends         8Digital  Video Processing T.R. Reed......................................................................................... 8-1             Introduction * Some Fundamentals * The Perception of Visual Motion * Image Sequence             Representation * The Computation of Motion * Image Sequence Compression * Conclusions         9Low Sample   SupportAdaptive Parameter Estimation and Packet-Data Detection            for Mobile Communications H. Qian, S.N. Batalama, D. Kazakos .................................... 9-1             Introduction * Basic Signal Model * Data Processing with Known Input Statistics *             Auxiliary-Vector (AV) Filters * Disjoint Parameter Estimation and Packet-Data Detection *             Joint Parameter Estimation and Packet-Data Detection * Concluding Remarks        10  Bandwidth Efﬁcient Modulation in Optical Communications            M.B. Tayahi, B.S. Rawat ............................................................................................................. 10-1             Introduction * Bandwidth Efﬁcient Modulation (BEM) Concept * Transmission Impairments and             TechnologyLimitations * Practical Bandwidth Efﬁcient Modulations * Conclusion        11  Phase-Locked Loop S.L. Maddy............................................................................................. 11-1             Introduction * Loop Filter * Noise * PLL Design Procedures * Components * Applications                                                                                       I -1I -2                                                                              Communications           12  Telemetry   S. Horan.................................................................................................................. 12-1               Introduction * Basic Concepts * Data Measurements * Data Channels * Data Transmission               Formats * Data Processing          13  Computer-Aided   Designand  Analysis of Communication   Systems              W.H.  Tranter,K.L. Kosbar........................................................................................................... 13-1               Introduction * The Role of Simulation * Motivation for the UseofSimulation * Limitations of               Simulation * Simulation Structure * The InterdisciplinaryNatureofSimulation *               Model Design * Low-Pass Models * Pseudorandom Signal and Noise Generators * Transmitter,               Channel, and Receiver Modeling * Symbol Error Rate Estimation * Validation of Simulation               Results * ASimple Example Illustrating Simulation Products * Conclusions                                                                                                           1                                                                              Broadcasting                                          1.1   Modulation and Demodulation      .............................................. 1 -1                                               Modulation  * Superheterodyne Technique *                                               Pulse-Code Modulation  * Frequency-Shift Keying *                                               M -aryPhase-Shift Keying * QuadratureAmplitude                                              Modulation                                        1.2   Radio Broadcasting   ............................................................. 1 -10 Richard C. Dorf                                              AM   Radio Broadcasting * Shortwave Broadcasting *  University of California                     FM  Radio Broadcasting * AM Broadcast Antenna Systems * Zhen   Wan                                   FM  Broadcast Antenna Systems                                        1.3   Television Systems  .............................................................. 1 -24 University of Texas                                              Scanning Lines and Fields * Interlaced Scanning Fields *  Jerry C. Whitaker                            Synchronizing Video Signals * Television IndustryStandards *  Advanced Television Systems                  Transmission Equipment  * Television Reception Committee                              1.4   High-Deﬁnition    Television ................................................... 1 -38 Martin S. Roden                              History * Traditional TV Standards * HDTV Formats *                                              Signal Processing California State University            1.5   Digital Audio Broadcasting   .................................................. 1 -43  Stanley Salek                                The  Need for DAB * DABSystem Design Goals  * Hammett  & Edison, Inc.                      Historical Background * Technical Overview of DAB *                                               Audio Compression and SourceEncoding   * System Example:  Almon H. Clegg                               Eureka-147/DAB  * System Example:iBiquity FM IBOC  * Consultant                                   System Example: iBiquity AM IBOC   1.1       Modulation and Demodulation RichardC.Dorf and Zhen                    Wan  Modulation    is the process of impressing the source information onto          abandpass signal    with  acarrier  frequency  f c .This bandpass signal is called the modulated signal  s ( t ), and the baseband source signal is called the modulating signal   m ( t ). The modulated signal could be represented by                                                s ð t Þ¼Ref g ð t Þ e j o ctgð1                                   : 1 Þ  or,equivalently,                                           s ð t Þ¼R ð t Þ cos ½ o c t þ y ð t Þ                                ð 1 : 2 Þ  and                                        s ð t Þ¼x ð t Þ cos o c t   y ð t Þ sin o c t                           ð 1 : 3 Þ                                                                                                                  1 -1                                                                                                                                                                              1   TABLE 1.1    Complex Envelope   Functions for Various Types of Modulation                                                                                                     -2                                                         Corresponding Quadrature                         Corresponding Amplitude and                                                              Modulation                                      Phase Modulation   Type of           Mapping Modulation       Functions g [ m ]               x ( t )                        y ( t )                  R ( t )            y ( t )      Linearity        Remarks                                                                                                                                                                                                                                                              0 ;   m ð t Þ >   1 AM            1 þ m ( t )1þ             m ( t )0                                                   j 1 þ m ð t Þj                           L b     m ( t ) . –1 required for                                                                                                                       180– ; m ð t Þ 5   1                                                                                                                                                       envelope detection                                                                                                                                                                                                                                                            0 ;   m ð t Þ 4 0 DSB-SC        m ( t )               m ( t )0                                                       j m ð t Þj                               LCoherent detection                                                                                                                       180– ; m ð t Þ 5 0                                                                                                                                                       required                 jD m ( t ) PM            e p                   cos[D p m ( t )]                sin[D p m ( t )]               1                 D p m ( t )NL                  D p is the phase                                                                                                                                                       deviation constant                                                                                                                                                       (radian/volts)                   R t                jDf  m ð s Þ d s          R t                            R  t                                           R t FM            e     1               cos D f   1 m ð s Þ d s         sin D f   1 m ð s Þ d s        1                 D f   1 m ð s Þ d s    NL      D f is the frequency                                                                                                                                                       deviation constant                                                                                                                                                       (radian/volt-sec)                                                                                                    p ﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃ SSB-AM-SCa    m ð t Þ 6 j mm ^ ð t Þ m ( t )                        6 mm ^ ð t Þ                    ½ m ð t Þ 2 þ½mm ^ ð t Þ 2 tan  1 ½ 6 mm ^ ð t Þ = m ð t Þ  LCoherent detection                                                                                                                                                        required             Communication Optical and Broadcasting         a       jDp ½ m ð t Þ 6 j mm ^ ð t Þ  7 D p mm ^ ð t Þ       7 D p mm ^ ð t Þ                7 D p mm ^ ð t Þ SSB-PM        e                     e      cos½ D p m ð t Þ         e     sin½ D p m ð t Þ         e                 D p m ( t )NL                  R                      R                               R                              R                   t                      t           R                   t          R                   t              R        a       jDf  ½ m ð s Þ 6 j mm ^ ð s Þ d s 7 D f mm ^ ð s Þ d s t 7 D f mm ^ ð s Þ d s t      7 D f mm ^ ð s Þ d s t SSB-FM        e     1               e      1    cos D f   1 m ð s Þ d s e   1  sin D f   1 m ð s Þ d s e   1         D f   1 m ð s Þ d s    NL SSB-EVa       e f ln½ 1 þ m ð t Þ 6 j lnln^ ½ 1 þ m ð t Þ g ½ 1 þ m ð t Þ cosf lnln^ ½ 1 þ m ð t Þ g 6 ½ 1 þ m ð t Þ sinf lnln^ ½ 1 þ m ð t Þ g 1 þ m ( t ) 6 lnln^ ½ 1 þ m ð t Þ  NL m ( t ) . –1 is required                                                                                                                                                       so that the ln will                                                                                                                                                       have areal value                                     p ﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃ nop                  ﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃ nop              ﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃ       a       ð 1 = 2 Þfln½ 1 þ m ð t Þ 6 j lnln^ ½ 1 þ m ð t Þ g 1 ^            1 ^                                   1 ^ SSB-SQ        e                       1 þ m ð t Þ cos 2 lnln½ 1 ¼ m ð t Þ  6 1 þ m ð t Þ sin 2 lnln½ 1 þ m ð t Þ  1 þ m ð t Þ 6 2 lnln½ 1 þ m ð t Þ  NL m ( t ) . –1 is required                                                                                                                                                       so that the ln will                                                                                                                                                       have areal value                                                                                                    p ﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃ                                                                                                       2      2           1 QM            m 1 ( t ) þ jm2 ( t ) m 1 ( t )                       m 2 ( t )                       m 1 ð t Þþm 2 ð t Þ tan ½ m 2 ð t Þ = m 1 ð t Þ  LUsed in NTSC                                                                                                                                                       color television:                                                                                                                                                       requires coherent                                                                                                                                                       detection                                                                                                                           1    1 R   x ð l Þ   L ¼ linear,NL ¼ nonlinear,[ˆ .] is the Hilbert transform (i.e., –908 phase-shifted version)of[·]. The Hilbert transform is xx^ ð t Þ¼D x ð t Þ ¼ 1 d l                                                                                                                         * p t  p    1 t   l   a Useuppersigns for uppersideband signalsand lower signs for lower sideband signals.   b In the strict sense, AM signalsare not linear because the carrierterm does not satisfy the linearity (superposition) condition.                                           Te                                                                                                                                                                               chnology   Source: L.W.Couch, Digital and Analog Communication Systems,New York: Macmillan, 1990. With permission.Broadcasting                                                                        1 -3   where o c ¼ 2 p f c .The complex envelope is                                g ð t Þ¼R ð t Þ e j y ð t Þ ¼ x ð t Þþjyð t Þð1        : 4 Þ  and g ( t )isafunction of the modulating signal m ( t ). That is,                                       g ð t Þ¼g ½ m ð t Þ   Thus g [·]performs amapping operation on m ( t ). The particular relationship that is chosen for g ( t )interms of m ( t )deﬁnes the type of modulation used.   In Table 1.1, examples of the mapping function g ( m )are given for the following types of modulation:      . AM: amplitude modulation     . DSB-SC: double-sideband suppressed-carrier modulation     . PM: phase modulation     . FM: frequency modulation     . SSB-AM-SC: single-sideband AM suppressed-carrier modulation     . SSB-PM: single-sideband PM     . SSB-FM: single-sideband FM     . SSB-EV: single-sideband envelope-detectable modulation     . SSB-SQ: single-sideband square-law-detectable modulation     . QM: quadraturemodulation  Modulation In Table 1.1, ageneralized approach maybetaken to obtain universal transmitter models that may be reduced to those used for aparticular modulation type. We also see that there are equivalent models which correspond to different circuit conﬁgurations, yetthey may be used to produce the same type of modulated signal at their outputs. It is up to communication engineers to select an implementation method that will optimize performance, yet retain low cost based on the state of the artincircuit development.   There are two canonical forms for the generalized transmitter.Figure1.1 is an AM-PM type circuit as described in Equation (1.2). In this ﬁgure,the baseband signal processing circuit generates R ( t )and y ( t )from m ( t ). The R and y are functions of the modulating signal m ( t )asgiven in Table 1.1 for the particular modulation type desired.   FIGURE 1.1 Generalized transmitter using the AM-PM generation technique. ( Source: L.W.Couch, Digital and Analog Communication Systems,New York: Macmillan, 1990, p. 279. With permission.)1 -4                                   Broadcasting and Optical Communication Technology    Figure1.2 illustrates the second canonical form for the generalized transmitter.This uses in-phase and quadrature-phase (IQ) processing.Similarly,the formulas relating x ( t )and y ( t )are shown in Table 1.1, and the baseband signal processing may be implemented by using either analog hardwareordigital hardwarewith software.The remainder of the canonical form utilizes radio frequency (RF) circuitsasindicated.   Anytypeofsignal modulation (AM, FM, SSB,QPSK, etc.) may be generated by using either of these two canonical forms. Both of these forms conveniently separate baseband processing from RF processing.  SuperheterodyneTechnique Most receivers employthe superheterodyne receiving technique (see Figure1.3). This technique consists of either down-converting or up-converting the input signal to some convenient frequency band, called the intermediate frequency (IF) band, and then extracting the information (or modulation) by using the appropriate detector.This basic receiver structureisused for the receptionofall types of bandpass signals, such as television, FM, AM, satellite, and radar signals.   FIGURE 1.2 Generalized transmitter using the quadrature generation technique. ( Source: L.W.Couch, Digital and Analog Communication Systems,New York: Macmillan, 1990, p. 280. With permission.)   FIGURE 1.3 Superheterodyne receiver.(Source: L.W.Couch, Digital and Analog Communication Systems,New York: Macmillan, 1990, p. 281. With permission.)Broadcasting                                                                        1 -5    If the complex envelope g ( t )isdesired for generalized signal detection or for optimum reception in digital systems, the x ( t )and y ( t )quadraturecomponents, where x ( t ) 1 jy( t ) ¼ g ( t ), may be obtained by using quadratureproduct detectors, as illustrated in Figure1.4. x ( t )and y ( t )could be fed into asignal processor to extract the modulation information. Disregarding the effects of noise, the signal processor could recover m ( t ) from x ( t )and y ( t )(and, consequently,demodulate the IF signal) by using the inverseofthe complex envelope generation functions given in Table 1.1.   The generalized modulation techniques are shown in Table 1.1. In digital communication systems, discrete modulation techniques are usually used to modulate the sourceinformation signal. Discrete modulation includes:      . PCM ¼ pulse-codemodulation     . DM ¼ differential modulation     . DPCM ¼ differential pulse-code modulation     . FSK ¼ frequency-shift keying     . PSK ¼ phase-shift keying     . DPSK ¼ differential phase-shift keying     . MPSK ¼ M -aryphase-shift keying     . QAM ¼ quadrature amplitude modulation   FIGURE 1.4 IQ (in-phase and quadrature-phase) detector.(Source: L.W.Couch, Digital and Analog Communication Systems,New York: Macmillan, 1990, p. 284. With permission.)                 TABLE 1.2 Performance of aPCM System with Uniform Quantizing and                No Channel Noise                                                           RecoveredAnalog                                                           Signal Power-to-                Number of                 Bandwidth of     Quantizing Noise                Quantizer   Length of the  PCMSignal        Power Ratios                  Levels     PCM Word,     (First Null                                                  n                 Used, M      n (bits)    Bandwidth)    ( S/N) pk out ( S/N) out                     21                       2 B         10.8        6.0                     42                       4 B         16.8       12.0                     83                       6 B         22.8       18.1                    16           48B                      28.9       24.1                    32           510           B          34.9       30.1                    64           612           B          40.9       36.1                   128           714           B          46.9       42.1                   256           816           B          52.9       48.2                   512           918           B          59.0       54.2                  1024          10           20B          65.0       60.2                   a B is the absolute bandwidth of the input analog signal.1 -6                                   Broadcasting and Optical Communication Technology  Pulse-Code   Modulation PCM  is essentially analog-to-digital conversion of aspecial type, wherethe information contained in the instantaneous samples of an analog signal is represented by digital words in aserial bit stream.The PCM signal is generated by carrying out three basic operations: sampling, quantizing,and encoding (see Figure1.5). The sampling operation generates aﬂat-top pulse amplitude modulation (PAM) signal. The quantizing convertsthe actual sampled value into the nearest of the M amplitude levels. The PCM signal is obtained from the quantized PAMsignal by encoding each quantized sample value into a digital word.  Frequency-Shift  Keying The FSK signal can be characterized as one of two different types. One type is called discontinuous-phase FSK since y ( t )isdiscontinuous at the switching times. The discontinuous-phase FSK signal is represented by                                    A cosð o t þ y Þ  for t in time interval when abinary 1issent           s ð t Þ¼ c     1    1                                                    ð 1 : 5 Þ                  A c cosð o 2 t þ y 2 Þ for t in time interval when abinary 0issent  where f 1 is called the mark (binary1)frequency and f 2 is called the space (binary0)frequency.The other type is continuous-phaseFSK. The continuous-phase FSK signal is generated by feeding the data signal into a frequency modulator,asshown in Figure1.6(b). This FSK signal is represented by                                          Z         t                            s ð t Þ¼A c cos o c t þ D f m ð l Þ d l                                                     1 or                                     s ð t Þ¼Ref g ð t Þ e j o ctgð1                   : 6 Þ                               FIGURE 1.5 APCM  transmission system.Broadcasting                                                                                     1 -7   FIGURE 1.6  Generation of FSK. ( Source: L.W.Couch, Digital and Analog Communication Systems,New York: Macmillan, 1990, p. 337. With permission.)   FIGURE 1.7  Detection of FSK. ( Source: L.W.Couch, Digital and Analog Communication Systems,New York: Macmillan, 1990, p. 344. With permission.)  where                                                       j y ð t Þ                                            g ð t Þ¼A c e                                       ð 1 : 7 Þ                                               Z t                                   y ð t Þ¼D f   m ð l Þ d l for FSK                            ð 1 : 8 Þ                                               1  Detection of FSK is illustrated in Figure1.7.  M  -aryPhase-Shift Keying If the transmitter is aPMtransmitter with an M -leveldigital modulation signal, MPSK is generated at the                                                                                 j y (t) transmitter output. Aplot of the permitted values of the complex envelope, g ( t ) ¼ A c e ,would contain M1 -8                                   Broadcasting and Optical Communication Technology  points, one value of g(acomplex number in general) for each of the Mmultilevelvalues, corresponding to the Mphases that y is permitted to have.   MPSK  can also be generated using twoquadrature carriers modulated by the x and y components of the complex envelope (instead of using aphase modulator)                                           j y ð t Þ                                g ð t Þ¼A c e ¼ x ð t Þþjyð t Þð1                     : 9 Þ  wherethe permitted values of x and y are                                       x i ¼ A c cos y i                            ð 1 : 10Þ                                        y i ¼ A c sin y i                           ð 1 : 11Þ  for the permitted phase angles y i , i ¼ 1, 2, ... , M ,ofthe MPSK signal. This is illustrated by Figure1.8, where the signal processing circuit implements Equation (1.10) and Equation (1.11).   MPSK, where M ¼ 4, is called quadrature-phase-shift-keyed (QPSK) signaling.  Quadrature   Amplitude   Modulation Quadraturecarrier signaling is called quadratureamplitude modulation (QAM). In general, QAMsignal  constellations are not restrictedtohaving permitted signaling points only on acircle (of radius A c ,aswas the   FIGURE 1.8 Generation of QAMsignals. ( Source: L.W.Couch, Digital and Analog Communication Systems,New York: Macmillan, 1990, p. 346. With permission.)Broadcasting                                                                        1 -9  TABLE 1.3 Spectral Efﬁciency for QAMSignaling with Raised Cosine-Roll-OffPulse Shaping                                                         R bits= s  Number of     Size of                              Z ¼                                                        B  Hz   Levels,       DAC,                                    T M (symbols)    ‘ (bits)    r ¼ 0.0   r ¼ 0.1    r ¼ 0.25    r ¼ 0.5    r ¼ 0.75    r ¼ 1.0      211.00                            0.909      0.800       0.667      0.571      0.500     422.00                            1.82       1.60        1.33       1.14       1.00     833.00                            2.73       2.40        2.00       1.71       1.50     16           44.00                3.64       3.20        2.67       2.29       2.00     32           55.00                4.55       4.0         3.33       2.86       2.50    DAC ¼ digital-to-analog converter.    Z ¼ R / B T ¼ ‘ /2 bits/s per hertz.   r is the roll-offfactor of the ﬁlter characteristic.   Source: L.W.Couch, Digital and Analog Communication Systems,New York: Macmillan 1990, p. 350. With permission.  case for MPSK). The general QAMsignal is                               s ð t Þ¼x ð t Þ cos o c t   g ð t Þ sin o c t        ð 1 : 12Þ  where                                g ð t Þ¼x ð t Þþjyð t Þ¼R ð t Þ e j y ð t Þ         ð 1 : 13Þ   The generation of QAMsignals is shown in Figure 1.8. The spectral efﬁciency for QAMsignaling is shown in Table 1.3.  Deﬁning   Terms Modulation:  The process of impressing the source information onto abandpass signal with acarrier       frequency f c .Itcan be expressed as                                     s ð t Þ¼Ref g ð t Þ e j o ctg       where g ( t )isafunction of the modulating signal m ( t ). That is,                                       g ð t Þ¼g ½ m ð t Þ        g [·]performs amapping operation on m ( t ). The particular relationship that is chosen for g ( t )interms      of m ( t )deﬁnes the type of modulation used. Superheterodyne receiver: Most receivers employthe superheterodyne receiving technique, which      consists of either down-converting or up-converting the input signal to some convenient frequency      band, called the intermediate frequency band, and then extracting the information (or modulation) by      using an appropriate detector.This basic receiver structureisused for the reception of all types of      bandpass signals, such as television, FM, AM, satellite, and radar signals. References L.W.Couch, Digital and Analog Communication Systems,New York: Prentice-Hall, 1995. F. Dejager,‘‘Delta modulation of PCM transmission using a1-unit code,’’ Phillips Res. Rep.,no. 7, pp.442–466,      Dec. 1952.1 -10                                  Broadcasting and Optical Communication Technology  J.H. Downing, Modulation Systems and Noise,Englewood Cliffs, N.J.: Prentice-Hall, 1964. J. Dunlop and D.G. Smith, Telecommunications Engineering,London: VanNostrand, 1989. B.P. Lathi, ModernDigital and Analog Communication Systems,New York: CBS College, 1983. J.H. Park, Jr., ‘‘On binaryDPSK detection,’’ IEEE Trans. Commun.,COM-26, pp.484–486, 1978. M. Schwartz, Information Transmission, Modulation and Noise,New York: McGraw-Hill, 1980.  Further  Information The monthly journal IEEE Transactions on Communications describes telecommunication techniques. The performanceof M -aryQAM schemes is evaluated in its March 1991 issue, pp.405–408. The IEEE magazine IEEE Communications is avaluable source.   Another sourceis IEEE TransactionsonBroadcasting,which is published quarterly by The Institute of Electrical and Electronics Engineers, Inc.   The biweekly magazine Electronics Letters investigates the errorprobabilityofcoherent PSK and FSK systems with multiple co-channelinterferences in its April 11, 1991, issue, pp.640–642. Another relevant source regarding the coherent detection of MSK is described on pp.623–625 of the same issue. All subscriptions inquiries and orders should be sent to IEE Publication Sales, P.O. Box96, Stevenage, Herts, SG1 2SD,United Kingdom.  1.2     Radio   Broadcasting Jerry C.  Whitaker  Broadcasting has been around for along time. Amplitude modulation (AM) was the ﬁrst modulation system that permitted voicecommunications to take place. This simple modulation system was predominant throughout the 1920s and 1930s. Frequency modulation (FM) came into regular broadcast service during the 1940s. Television broadcasting,which uses amplitude modulation for the visual portion of the signal and frequency modulation for the aural portion of the signal, became available to the public in the mid-1940s. More recently,digital television (DTV)service has been launched in the United States and elsewhereusing the conventional television frequency bands and 6-MHz bandwidth of the analog system, but with digital modulation.  AM   Radio  Broadcasting AM  radio stations operate on 10-kHz channels spaced evenly from 540 to 1600 kHz. Various classes of stations havebeen established by the Federal Communications Commission (FCC) and agencies in other countries to allocate the available spectrum to given regions and communities. In the United States, the basic classes are clear, regional,and local.Current practiceuses the CCIR (international) designations as class A, B, and C, respectively.Operating power levels range from 50 kW for aclear channel station to as little as 250 Wfor a local station.  High-Level AM  Modulation High-levelanode modulation is the oldest and simplest way of generating ahighpowerAMsignal. In this system, the modulating signal is ampliﬁed and combined with the dc supply sourcetothe anode of the ﬁnal RF ampliﬁer stage. The RF ampliﬁer is normally operated class C. The ﬁnal stage of the modulator usually consists of apair of tubes operating class Binapush–pull conﬁguration. Abasic high-level modulator is shown in Figure1.9.   The RF signal is normally generated in alow-level transistorized oscillator.Itisthen ampliﬁed by one or moresolid-state or vacuum tube stages to provide ﬁnal RF drive at the appropriate frequency to the grid of theBroadcasting                                                                       1 -11                                                                    Modulated RF                                                                  output             RF input               signal                  Bias                   Modulation                input signal                                                          V cc                 FIGURE 1.9 Simpliﬁed diagram of ahigh-level, amplitude-modulated ampliﬁer.  ﬁnal class Campliﬁer.The audio input is applied to an intermediate powerampliﬁer (usually solid state) and used to drivetwo class B(or class AB) push–pull output devices. The ﬁnal ampliﬁers provide the necessary modulating power to drivethe ﬁnal RF stage. For100% modulation, this modulating power is equal to 50% of the actual carrier power.   The modulation transformer shown in Figure 1.9 does not usually carrythe dc supply current for the ﬁnal RF ampliﬁer.The modulation reactor and capacitor shown provide ameans to combine the audio signal voltage from the modulator withthe dc supply to the ﬁnal RF ampliﬁer.This arrangement eliminates the necessityofhaving dc current ﬂow throughthe secondaryofthe modulation transformer,which would result in magnetic losses and saturation effects. In some newer transmitter designs, the modulation reactor has been eliminated from the system, thanks to improvements in transformer technology.   The RF ampliﬁer normally operates class Cwith grid current drawn during positive peaks of the cycle. Typical stage efﬁciency is 75 to 83%. An RF tank following the ampliﬁer resonates the output signal at the operating frequency and, with the assistance of alow-pass ﬁlter,eliminates harmonics of the ampliﬁer caused by class Coperation.   This type of system was popular in AM broadcasting for manyyears, primarily because of its simplicity. The primarydrawback is low overall system efﬁciency.The class Bmodulator tubes cannot operate with greater than 50% efﬁciency.Still, withinexpensive electricity, this was not considered to be asigniﬁcant problem. As energycosts increased, however, more efﬁcient methods of generating high-power AM signals weredeveloped. Increased efﬁciency normally came at the expense of added technical complexity.   Pulse-Width Modulation Pulse-width modulation (PWM), also known as pulse-duration modulation (PDM), is one of the most popular systems developed for modern vacuum tube AM transmitters. Figure1.10 shows the basic PDM   scheme. The PDM  system works by utilizing asquare-waveswitching system, illustrated in Figure1.11.1 -12                                  Broadcasting and Optical Communication Technology                                                                  Tuning and                                                                matching circuits                RF drive                                  PDM                                 filter                                      Damping diode                                                     DC             PDM signal                                                  supply             FIGURE 1.10 The pulse-duration modulation (PDM) method of pulse-width modulation.                        Sine wave                        Square wave                        Triangle wave                        Input audio waveform                     +                     0                     −                      Sum of audio +triangle wave                                                             Threshold                       Width-modulated pulses                        FIGURE 1.11 The principles waveforms of the PDM system.    The PDM  process begins with asignal generator (see Figure1.12). A75-kHz sine waveisproduced by an oscillator and used to drive asquare-wavegenerator,resulting in asimple 75-kHz square wave. The square wave is then integrated, resulting in atriangular waveform that is mixed with the input audio in asumming circuit. The resulting signal is atriangular waveform that rides on the incoming audio.This compositesignal isBroadcasting                                                                       1 -13                   75 kHz      Square-                  Summing       Audio                              wave       Integrator                oscillator  generator                  circuit       input                           PDM output     Pulse         Threshold                                       amplifier      amplifier                        FIGURE 1.12 Block diagram of aPDM waveform generator.  then applied to athreshold ampliﬁer,which functions as aswitch that is turned on wheneverthe value of the input signal exceeds acertain limit. The result is astring of pulses in which the width of the pulse is proportional to the period of time the triangular waveform exceeds the threshold. The pulse output is applied to an ampliﬁer to obtain the necessarypower to drivesubsequent stages. Aﬁlter eliminates whatever transients mayexist after the switching process is complete.   The PDM  scheme is, in effect, adigital modulation system with the audio information being sampled at a 75-kHz rate. The width of the pulses contains all the audio information. The pulse-width-modulated signal is applied to a switch or modulator tube.The tube is simply turned on,toafully saturated state, or off in accordance with the instantaneous value of the pulse. When the pulse goes positive, the modulator tube is turned on and the voltage across the tube dropstoaminimum. When the pulse returns to its minimum value, the modulator tube turns off.   This PDM signal becomes the powersupply to the ﬁnal RF ampliﬁer tube. When the modulator is switched on, the ﬁnal ampliﬁer will experiencecurrent ﬂowand RF will be generated. When the switch or modulator tube goes off, the ﬁnal ampliﬁer current willcease. This system causes the ﬁnal ampliﬁer to operate in ahighly efﬁcient class Dswitching mode. Adcoffset voltage to the summing ampliﬁer is used to set the carrier (no modulation) levelofthe transmitter.   Ahighdegreeofthird-harmonic energywillexist at the output of the ﬁnal ampliﬁer because of the switching-mode operation. This energyiseliminated by athird-harmonic trap.The result is astable ampliﬁer that normally operates in excess of 90% efﬁciency.The powerconsumed by the modulator and its driver is usually afraction of afull class Bampliﬁer stage.   The damping diode shown in the previous ﬁgure is included to prevent potentially damaging transient overvoltages during the switching process. When the switching tube turns offthe supply current during a period when the ﬁnal ampliﬁer is conducting,the highcurrent through the inductors contained in the PDM ﬁlters could cause alarge transient voltage to be generated. The energyinthe PDM ﬁlter is returned to the power supply by the damping diode. If no alternativeroute is established, the energywill return by arcing throughthe modulator tube itself.   The PWM   system makes it possible to completely eliminate audio frequency transformers in the transmitter.The result is wide frequency response and low distortion. It should be noted that variations on this ampliﬁer and modulation scheme havebeen used by other manufacturers for both standard broadcast and shortwave service.  Digital Modulation Current transmitter design work for AM broadcsting has focused almost exclusively on solid-state technology. High-power MOSFET devices and digital modulation techniques havemade possible anew generation of energy-efﬁcient systems, withaudio performancethat easily surpasses vacuum tube designs.   Most solid-state AM systems operate in ahighly efﬁcient class Dswitching mode. Multiple MOSFET driver boards are combined through one of several methods to achieve the required carrier power.1 -14                                  Broadcasting and Optical Communication Technology  Shortwave   Broadcasting  The technologies used in commercial and government- TABLE 1.4 Operating Frequency Bands for Shortwave sponsored shortwave broadcasting are closely allied with Broadcasting those used in AM radio.However,shortwave stations            Frenquency        Meter Band usually operate at signiﬁcantly higher powers than AM Band     (kHz)              (m) stations.   International broadcast stations use frequencies A5,950–6,200                   49 ranging from 5.95 to 26.1 MHz. The transmissions are B9,500–9,775                 32                                                C11,700–11,975                     25 intended for reception by the general public in foreign D15,100–15,450            19 countries. Table 1.4 shows the frequencies assigned by E17,700–17,900             16 the Federal Communications Commission (FCC) for F21,450–21,750                    14 international broadcast shortwave service in the United G25,600–26,100            11 States. The minimum output power is 50 kW.Assign- ments are made for speciﬁc hours of operation at speciﬁc frequencies.   Very high-power shortwave transmitters havebeen installed to serve large geographical areas and to overcome jamming efforts by foreign governments. Systems rated for power outputs of 500 kW and moreare not uncommon. RF circuitsdesigned speciﬁcally for highpoweroperation are utilized.   Most shortwave transmitters havethe unique requirement for automatic tuning to one of several preset operating frequencies. Avarietyofschemes exist to accomplish this task, including multiple exciters (each set to the desiredoperating frequency)and motor-controlled variable inductors and capacitors. Tune-up at each frequency is performed by the transmitter manufacturer.The settings of all tuning controls are stored in memory. Automatic retuning of ahigh-power shortwavetransmitter can be accomplished in less than 30 seconds in most cases. Power Ampliﬁer  Types Shortwave technologyhas advanced signiﬁcantly within the last 5years, thanks to improved semiconductor devices. High-powerMOSFETsand   other components havemade  solid-state shortwave transmitters operating at 500 kW and more practical. The majorityofshortwave systems now in use, however, use vacuum tubes as the power-generating element. The efﬁciency of apower ampliﬁer/modulator for shortwave applications is of critical importance. Because of the power levels involved, low efﬁciency translates into higher operating costs.   Older,traditional tube-typeshortwavetransmitters typically utilize one of the following modulation systems:      . Doherty ampliﬁer     . Chireixoutphasing modulated ampliﬁer     . Dome modulated ampliﬁer     . Terman-Woodyard modulated ampliﬁer  FM  Radio  Broadcasting FM  radio stations operate on 200-kHz channels spaced evenly from 88.1 to 107.9 MHz. In the United States, channels below 92.1 MHz are reservedfor noncommercial, educational stations. The FCC has established three classiﬁcations for FM stations operating east of the Mississippi River and four classi- ﬁcations for stations west of the Mississippi. Powerlevels range from ahighof100 kW effective radiated power (ERP) to 3kWorless for lowerclassiﬁcations. The ERP of astation is afunction of transmitter poweroutput (TPO) and antenna gain. ERP is determined by multiplying these twoquantities together and allowing for line loss.   Atransmitting antenna is said to have‘‘gain’’ if, by design, it concentrates useful energyatlow radiation angles, rather than allowing asubstantial amount of energytoberadiated abovethe horizon (and be lostBroadcasting                                                                       1 -15  in space). FM and TV transmitting antennas are designed to provide gain by stacking individual radiating elements vertically.   At ﬁrst examination, it might seem reasonable and economical to achievelicensed ERP using the lowest transmitter power output possible and highest antenna gain. Other factors, however,come into playthat make the most obvious solution not always the best solution. Factors that limit the use of high-gain antennas include:      . Effects of high-gain designs on coverage areaand signal penetration     . Limitations on antenna size because of tower restrictions, such as available vertical space, weight, and       windloading     . Cost of the antenna Stereo broadcasting is used almost universally in FM radio today.Introduced in the mid-1960s, stereo has contributed in large parttothe success of FM radio.The left and right sum (monophonic) information is transmitted as astandardfrequency-modulated signal. Filters restrict this main channel signal to amaximum of about 17 kHz. Apilot signal is transmitted at lowamplitude at 19 kHz to enable decoding at the receiver. The left and right difference signal is transmitted as an amplitude-modulated subcarrier that frequency- modulates the main FM carrier.The center frequency of the subcarrier is 38 kHz. Decoder circuits in the FM receiver matrix the sum and difference signals to reproducethe left and right audio channels. Figure 1.13 illustrates the baseband signal of astereoFMstation.  Modulation Circuits Early FM transmitters used reactance modulators that operated at low frequency.The output of the modulator was then multiplied to reachthe desired output frequency.This approach was acceptable for monaural FM transmission but not for modern stereosystems or other applications that utilize subcarriers on the FM broadcast signal. Modern FM systems all utilize what is referred to as direct modulation.That is, the frequency modulation occurs in amodulated oscillator that operates on acenter frequency equal to the desired transmitter output frequency.Instereo broadcast systems, acompositeFMsignal is applied to the FM  modulator.   Various techniques havebeen developed to generate the direct-FM signal. One of the most popular uses a variable-capacity diode as the reactiveelement in the oscillator.The modulating signal is applied to the diode, which causes the capacitanceofthe device to varyasafunction of the magnitude of the modulating signal. Variations in the capacitancecause the frequency of the oscillator to vary. Again,the magnitude of the frequency shift is proportional to the amplitude of the modulating signal, and the rate of frequency shift is equal to the frequency of the modulating signal.   The direct-FM modulator is one element of an FM transmitter exciter,which generates the composite FM waveform. Ablock diagram of acomplete FM exciter is shown in Figure 1.14. Audio inputs of various types (stereo left and right signals, plus subcarrier programming,ifused) are buffered, ﬁltered, and preemphasized beforebeing summed to feed the modulated oscillator.Itshould be noted that the oscillator is not normally coupled directly to acrystal, but afree-running oscillator adjusted as closely as possible to the carrier frequency of the transmitter.The ﬁnal operating frequency is carefully maintained by an automatic frequency control system employing a phase-locked loop (PLL) tied to areferencecrystal oscillator or frequency synthesizer.   Asolid-state class Campliﬁer follows the modulated oscillator and raises the operating power of the FM signal to 20 to 30 W. One or more subsequent ampliﬁers in the transmitter raise the signal powerto several hundred watts for application to the ﬁnal powerampliﬁer stage. Nearly all current high-power FM transmitters utilize solid-state ampliﬁers up to the ﬁnal RF stage, which is generally avacuum tube for operating powers of 20 kW and above. All stages operate in the class Cmode. In contrast to AM systems, each stage in an FM power ampliﬁer can operate class Cbecause no information is lost from the frequency-modulated signal due to amplitude changes. As mentioned previously,FMisaconstant- power system.1 -16                                  Broadcasting and Optical Communication Technology                           FIGURE 1.13 Composite baseband stereo FM signal.                             FIGURE 1.14 Block diagram of an FM exciter.  AuxiliaryServices Modern FM  broadcast stations are capable of not only broadcasting stereoprogramming,but one or more subsidiarychannels as well. These signals, referred to by the FCC as SubsidiaryCommunications Authorization (SCA) services, are used for the transmission of stock market data, background music, control signals, and other information not normally partofthe station’s main programming.These services do not provide the same range of coverage or audio ﬁdelityasthe main stereo program; however,they perform apublic service and can represent avaluable sourceofincome for the broadcaster.   SCA systems provide efﬁcient use of the available spectrum. The most common subcarrier frequency is 67 kHz, althoughhigher subcarrier frequencies maybeutilized. Stations that operate subcarrier systems are permitted by the FCC to exceed (by asmall amount) the maximum 75-kHz deviation limit under certain conditions. The subcarriers utilize lowmodulation levels, and the energyproduced is maintained essentially within the 200-kHz bandwidth limitation of FM channel radiation.  FM  Power Ampliﬁers Most high-power FM transmitters manufactured today employ cavitydesigns. The 1/4-wavelength cavityis the most common. The design is simple and straightforward. Anumber of variations can be found in different transmitters but the underlying theoryofoperation is the same. The goal of anycavityampliﬁer is to simulate aresonant tank circuit at the operating frequency and provide ameans to couple the energyinthe cavity to the transmission line. Because of the operating frequencies involved (88 to 108 MHz), the elements of the ‘‘tank’’take on unfamiliar forms.   Atypical 1/4-wavecavityisshown in Figure1.15. The plate of the tube connects directly to the inner section (tube) of the plate-blocking capacitor.The blocking capacitor can be formed in one of several ways. In at least one design, it is made by wrapping the outside surface of the inner tube conductor with multiple layers of insulating ﬁlm. The exhaust chimney/inner conductor forms the other element of the blocking capacitor.The cavitywalls form the outer conductor of the 1/4-wavetransmission line circuit. The dc plate voltage is applied to the PA (power ampliﬁer) tube by acable routed inside the exhaust chimney and inner tube conductor.Broadcasting                                                                       1 -17           FIGURE 1.15 Physical layout of acommon type of 1/4-wavePAcavityfor FM broadcast service.  In this design, the screen-contactﬁngerstock ring mounts on ametal plate that is insulated from the grounded-cavity deck by ablocking capacitor.This hardware makes up the screen-blockerassembly.The dc screen voltage feeds to the ﬁngerstock ring from underneath the cavitydeck through an insulated feedthrough.   Some transmitters that employthe 1/4-wavecavitydesign use agrounded-screen conﬁguration in which the screen contact ﬁngerstock ring is connected directly to the grounded cavitydeck. The PA cathode then operates at belowground potential (i.e., at anegativevoltage), establishing the required screen voltage for the tube.   Coarse tuning of the cavityisaccomplished by adjusting the cavitylength. The top of the cavity(the cavity shorting deck) is fastened by screws or clamps and can be raised or lowered to set the length of the assembly for the particular operating frequency.Fine-tuning is accomplished by avariable-capacityplate- tuning control built into the cavity. In the example, one plate of this capacitor,the stationaryplate, is fastened to the inner conductor just above the plate-blocking capacitor.The movable tuning plate is fastened to the cavitybox,the outer conductor,and is mechanically linked to the front-panel tuning control. This capacity shunts the inner conductor to the outer conductor and varies the electrical length and resonant frequency of the cavity.  AM   Broadcast Antenna Systems Vertical polarization of the transmitted signal is used for AM broadcast stations because of its superior groundwavepropagation, and because of the simple antenna designs that it affords. The Federal Communications Commission  (FCC) and  licensing authorities in other countries haveestablished classiﬁcations of AM stations with speciﬁed powerlevels and hours of operation. Protection requirements set forth by the FCC specify that some AM stations (in the United States) reducetheir transmitter powerat sunset, and return to full power at sunrise. This method of operation is based on the propagation characteristics of AM band frequencies. AM signals propagate further at nighttime than during the day.   The different day/nightoperating powers are designed to provide each AM station with aspeciﬁed coverage areathat is free from interference.Theoryrarely translates into practiceinsofar as coverage is concerned, however,because of the increased interference that all AM stations suffer at nighttime.   The tower visible at anyAMradio station transmitter site is only half of the antenna system. The second element is aburied ground system. Current on atower does not simply disappear;rather,itreturns to Earth1 -18                                  Broadcasting and Optical Communication Technology  through the capacitancebetween the Earth and the tower.Ground losses are greatly reduced if the tower has a radial copper groundsystem. Atypical singletower ground system is made up of 120 radial ground wires, each 140 electrical degrees long (at the operating frequency), equally spaced out from the tower base. This is often augmented with an additional 120 interspersed radials 50 ft long.  Directional AM Antenna  Design When  anondirectional antenna with agiven power does not radiate enoughenergytoserve the station’s primaryservice area, or radiates too much energytowardother radio stations on the same or adjacent frequencies, it is necessarytoemploy adirectional antenna system. Rules set out by the FCC and regulatory agencies in other countries specify the protectionrequirements to be provided by various classes of stations, for both daytime and nighttime hours. These limits tend to deﬁne the shape and size of the most desirable antenna pattern.   Adirectional antenna functions by carefully controlling the amplitude and phase of the RF currents fed to each tower in the system. The directional pattern is afunction of the number and spacing of the towers (vertical radiators), and the relative phase and magnitude of their currents. The number of towers in a directional AM array can range from twotosix, or even moreinacomplex system. One tower is deﬁned as the reference tower.The amplitude and phase of the other towers are measured relative to this reference.   Acomplex  network of powersplitting,phasing,and antenna couplingelements is required to make a directional system work. Figure 1.16 shows ablock diagram of abasic two-tower array.Apower divider network controls the relativecurrent amplitude in each tower.Aphasing network provides control of the phase of each tower current, relative to the referencetower.Matching networks at the base of each tower couplethe transmission line impedancetothe base operating impedanceofthe radiating towers.   In practice, the system shown in the ﬁgure would not consist of individual elements. Instead, the matching network, power dividing network, and phasing network would all usually be combined into asingle unit, referred to as the phasor.  Antenna  PatternDesign The pattern of anyAMdirectional antenna system (array)isdetermined by anumber of factors, including:      . Electrical parameters (phase relationship and current ratio for each tower)     . Height of each tower     . Position of each tower with respect to the other towers (particularly with respect to the referencetower)   A directional array consists of two or moretowers arranged in aspeciﬁc manner on aplot of land. Figure1.17 shows atypical three-tower array,aswell as the pattern such an array would produce. This is an in- line array ,meaning that all the elements (towers) are in line with one another.Notice that the major lobe is centered on the same line as the line of towers, and that the pattern nulls ( minima)are positioned symmetrically about the line of towers, protecting co-channel stations Aand Battrue bearings of 3158 and 458 ,respectively.           FIGURE 1.16 Block diagram of an AM directional antenna feeder system for atwo-tower array.Broadcasting                                                                       1 -19   FIGURE 1.17 Radiation pattern generated with athree-tower in-line directional array using the electrical parameters and orientation shown.        FIGURE 1.18 Radiation pattern produced when the array of Figure 1.13 is rotated to anew orientation.    Figure1.18 shows the same array,exceptthat it has been rotated by 108 .Notice that the pattern shape is not changed, but the position of the major lobe and the nulls followthe line of towers. Also noticethat the nulls are no longer pointed at the stations to be protected.   If this directional antenna system were constructed on agigantic turntable, the pattern could be rotated without affecting the shape. But, to accomplish the required protections and to havethe major lobe(s) oriented in the right direction, there is only one correct position. In most cases, the position of the towers will1 -20                                  Broadcasting and Optical Communication Technology  be speciﬁed with respect to asingle reference tower.The location of the other towers willbegiven in the form of adistanceand bearing from that reference. Occasionally,areferencepoint, usually the center of the array, will be used for ageographic coordinate point.  Bearing The bearing or azimuth of the towers fromthe reference tower or point is speciﬁed clockwise in degrees from true north.The distinction between true and magnetic north is vital. The magnetic NorthPole is not at the true or geographic NorthPole. (In fact, it is in the vicinityof74 8 north, 1018 west, in the islands of northern Canada.) The differencebetween magnetic and true bearings is called variation or magnetic declination. Declination, aterm generally used by surveyors, varies for different locations. It is not aconstant. The Earth’s magnetic ﬁeld is subject to anumber of changes in intensityand direction. These changes take placeoverdaily, yearly,and long-term (or secular)periods. The secular changes result in arelatively constant increase or decrease in declination over aperiod of manyyears.  Antenna  Monitoring System Monitoring the operation of an AM directional antenna basically involves measuring the powerinto the system, the relative value of currents into the towers, their phase relationships, and the levelsofradiated signal at certain monitoring points some distancefrom the antenna. Figure1.19 shows ablock diagram of atypical monitoring system for athree-tower array.For systems with additional towers, the basic layout is extended by adding more pickup elements, sample lines, and amonitor with additional inputs.  Phase/Current Sample Loop Twotypes of phase/current sample pickup elements are commonly used: the sample loop and torroidal current transformer (TCT). The sample loop consists of asingle turn unshielded loop of rigid construction, with a ﬁxed gap at the open end for connection of the sample line. The device must be mounted on the tower near the point of maximum current. The loop can be used on towers of both uniform and nonuniform crosssection. It must operate at tower potential, except for towers of less than 130 electrical degrees height, wherethe loop can be operated at ground potential.   When the sample loop is operated at tower potential, the coax from the loop to the base of the tower is also at tower potential. To bring the sample line across the base of the tower,asample line isolation coil is used.                   FIGURE 1.19 Atypical three-tower directional antenna monitoring system.Broadcasting                                                                       1 -21                  FIGURE 1.20 Three possible circuit conﬁgurations for phase sample pickup.    Ashielded torroidal current transformer can also be used as the phase/current pickup element. Such devices offer several advantages over the sample loop,including greater stabilityand reliability.Because they are located inside the tuning unit cabinet or house, TCTs are protected from wind, rain, ice,and vandalism.   Unlikethe rigid, ﬁxed sample loop,torroidal current transformers are available in several sensitivities, ranging from 0.25 to 1.0 Vper ampereoftower current. Tower currents of up to 40 Acan be handled, providing amoreusable range of voltages for the antenna monitor.Figure1.20 shows the various arrangements that can be used for phase/current sample pickup elements. Sample Lines The selection and installation of the sampling lines for adirectional monitoring system are important factors in the ultimate accuracyofthe overall array.   With critical arrays (antennas requiring operation within tight limits speciﬁed in the station license), all sample lines must be of equal electrical length and installed in such amanner that corresponding lengths of all lines are exposed to equal environmental conditions.   While sample lines may be run above ground on supports (if protected and properly grounded), the most desirable arrangement is direct burial using jacketed cable. Burial of sample line cable is almost astandard practicebecause proper burial offers good protection against physical damage and amorestable temperature environment. The Common Point The powerinput to adirectional antenna is measured at the phasor common point.Power is determined by the direct method:                                          P ¼ I 2 R  where P is the power in watts (W), I is the commonpoint current in amperes(A), and R is the common point resistance in ohms ( O ).1 -22                                  Broadcasting and Optical Communication Technology  Monitor Points Routine monitoring of adirectional antenna involves the measurement of ﬁeld intensityatcertain locations away from the antenna, called monitor points.These points are selected and established during the initial tune- up of the antenna system. Measurements at the monitor points should conﬁrm that radiation in prescribed directions does not exceed avalue that would cause interferencetoother stations operating on the same or adjacent frequencies. The ﬁeld intensitylimits at these points are normally speciﬁed in the station license. Measurements at the monitor points may be required on aweekly or amonthly basis, depending on several factors and conditions relating to the particular station. If the system is not acritical array,quarterly measurements may be sufﬁcient.  Folded Unipole Antenna The folded unipole antenna consists of agrounded vertical structure withone or more conductors folded back parallel to the side of the structure. It can be visualized as ahalf-wave folded dipole perpendicular to the ground and cut in half (see Figure1.21). This design makes it possible to provide awiderange of resonant radiation resistances by varying the ratio of the diameter of the folded-back conductors in relation to the tower.Top loading can also be used to broaden the antenna bandwidth. Aside view of the folded unipole is shown in Figure1.22.   The folded unipole antenna can be considered amodiﬁcation of the standard shunt-fed system. Instead of a slant wire that leaves the tower at an approximate 458 angle (as used for shunt-fed systems), the folded unipole antenna has one or morewires attached to the tower at apredetermined height. The wiresare supported by standoffinsulators and run parallel to the sides of the tower down to the base.   The tower is grounded at the base. The folds, or wires, are joined together at the base and driven through an impedancematching network. Depending on the tuning requirements of the folded unipole, the wiresmay be connected to the tower at the top and/or at predetermined levels along the tower with shorting stubs.   The folded unipole can be used on tall (1308 or greater) towers. However,ifthe unipole is not divided into twoparts, the overall efﬁciency (unattenuated ﬁeld intensity) will be considerably lowerthan the normally expected ﬁeld for the electrical height of the tower.  FM  Broadcast  Antenna   Systems The propagation characteristics of VHF FM radio are much differentthan for MF AM. There is essentially no difference between day and night FM propagation. FM stations haverelatively uniform day and night service areas with the same operating power.   FIGURE 1.21 The folded unipole antenna can be thought of as a1/2-wave folded dipole antenna perpendicular to the ground and cut in half.Broadcasting                                                                       1 -23    Awide varietyofantennas is available for use in the FM broadcast band. Nearly all employ circular polariza- tion. Although antenna designs differ from one manu- facturertoanother,generalizations can be made that apply to most units.  Antenna  Types There are three basic classes of FM broadcast transmit- ting antennas in use today: ring stub and twisted ring, shunt-and series-fed slanted dipole,and multi-armshort helix.While each design is unique, all havethe following items in common:      . The antennas are designed for sidemounting to a       steel tower or pole.     . Radiating elements are shunted across acommon       rigid coaxial transmission line.     . Elements are placed along the rigid line everyone       wavelength.                                                FIGURE 1.22 The folds of the unipole antennaare     . Antennas with one to seven baysare fed from the       bottom of the coaxial transmission line. arranged either near the legs of the tower or near the                                                faces of the tower.     . Antennas with more than seven bays are fed from       the center of the array to provide more       predictable performanceinthe ﬁeld.     . Antennas generally include ameans of tuning out reactances after the antenna has been installed       through the adjustment of variable capacitiveorinductiveelements at the feed point.   Figure1.23 shows ashunt-fed slanted dipole antenna that consists of twohalf-wave dipoles offset 908 .The twosets of dipoles are rotated 22.58 (from their normal plane) and are delta-matched to provide a50-O impedanceatthe radiator input ﬂange. The lengths of all four dipole arms can be matched to resonance by mechanical adjustment of the end ﬁttings. Shunt-feeding (when properly adjusted) provides equal currents in all four arms.   Wide-band panel antennas are afourth common type of antenna used for FM broadcasting. Panel designs share some of the characteristics listed previously,but are intended primarily for specialized installations in which twoormore stations will use the antenna simultaneously.Panel antennas are larger and more complex than other FM antennas, but offer the possibilityfor sharedtower spaceamong several stations and custom coverage patterns that would be difﬁcult or even impossible with more common designs.   The ideal combination of antenna gain and transmitter powerfor aparticular installation involves the analysis of anumber of parameters. As shown in Table 1.5, avariety of pairings can be made FIGURE 1.23 Mechanical conﬁguration of one bay of a to achievethe same ERP.                      circularly polarized FM transmitting antenna.1 -24                                  Broadcasting and Optical Communication Technology                 TABLE 1.5 Various Combinations of Transmitter Power and Antenna                Gain that Will Produce100-kW ERP for an FM Station                No.BaysAntenna           Gain            Transmitter Power (kW)                  31.5888                                        66.3                 42.1332                                        49.3                 52.7154                                        38.8                 63.3028                                        31.8                 73.8935                                        27.0                 84.4872                                        23.5                10                   5.6800                     18.5                12                   6.8781                     15.3  1.3     Television    Systems Jerry C.  Whitaker  The technologyoftelevision is based on the conversion of light rays from still or moving scenes and pictures into electronicsignals for transmission or storage, and subsequent reconversion into visual images on ascreen. Asimilar function is provided in the production of motion picture ﬁlm; however,whereﬁlm records the brightness variations of acompletescene on asingle frame in ashortexposure no longer than afraction of a second, the elements of atelevision picture must be scanned one pieceatatime. In the television system, a scene is dissected into a frame composedofamosaic of picture elements (pixels). A pixel is deﬁned as the smallest area of atelevision image that can be transmitted within the parameters of the system. This process is accomplished by:      . Analyzing the image with aphotoelectric device in asequence of horizontal scans from the top to the       bottom of the image to produce an electric signal in which the brightness and color values of the       individual picture elements are represented as voltage levels of avideo waveform     . Transmitting the values of the picture elements in sequence as voltage levelsofavideo signal     . Reproducing the image of the original scene in avideo signal display of parallel scanning lines on a       viewing screen  Scanning  Lines  and Fields The image pattern of electrical charges on acamera tube target or CCD,corresponding to the brightness levels of ascene, are converted to avideo signal in asequential order of picture elements in the scanning process. At the end of each horizontal line sweep,the video signal is blanked while the beam returns rapidly to the left side of the scene to startscanning the next line. This process continues until the image has been scanned from top to bottom to complete one ﬁeld scan.   After completion of this ﬁrst ﬁeld scan, at the midpoint of the last line, the beam again is blanked as it returns to the top center of the target wherethe process is repeated to provide asecond ﬁeld scan. The spot size of the beam as it impinges upon the target must be ﬁne enoughtoleaveunscanned areas between lines for the second scan. The pattern of scanning lines covering the areaofthe target, or the screen of apicture display,is called a raster.  Interlaced Scanning   Fields Because of the half-line offset for the startofthe beam return to the top of the raster and for the startofthe second ﬁeld, the lines of the second ﬁeld lie in-between the lines of the ﬁrst ﬁeld. Thus, the lines of the twoare interlaced.The twointerlacedﬁelds constitute asingle television frame.Figure1.24 shows aframe scan with interlacing of the lines of twoﬁelds.Broadcasting                                                                       1 -25   FIGURE 1.24 The interlaced scanning pattern (raster) of the television image. ( Source: Electronic Industries Association.)    Reproduction of the camera image on acathode ray tube (CRT) or solid-state displayisaccomplished by an identical operation, with the scanning beam modulated in densitybythe video signal applied to an element of the electron gun or control element, in the case of asolid-state displaydevice. This control voltage to the displayvaries the brightness of each picture element on the screen.   Blanking of the scanning beam during the return traceisprovided for in the video signal by a ‘‘blacker-than-black’’pulse waveform. In addition, in most receivers and monitors another blanking pulse is generated from the horizontal and vertical scanning circuits and applied to the display system to ensureablack screen during scanning retrace. The retracelines are shown as diagonal dashed lines in Figure1.24.   The interlaced scanning format, standardized for monochrome and compatible color,was chosen primarily for twopartially related and equally important reasons:      . To eliminate viewer perception of the intermittent presentation of images, known as ﬂicker     . To reduce video bandwidth requirements for an acceptable ﬂickerthreshold level   Perception of ﬂickerisdependent primarily upon twoconditions:      . The brightness level of an image     . The relative area of an image in apicture   The 30-Hz transmission rate for afull 525-line television frame is comparable to the highly successful 24-frame-per-second rate of motion-picture ﬁlm. However,atthe higher brightness levels produced on television screens, if all 483 lines (525 less blanking) of atelevision image were to be presented sequentially as single frames, viewers would observe adisturbing ﬂicker in picture areas of highbrightness. Foracomparison, motion-picturetheaters on average produce ascreen brightness of 10 to 25 ft·L (footlambert), whereas a direct-view CRTmay haveahighlight brightness of 50 to 80 ft·L. It should be noted also that motion-picture projectors ﬂash twice per frame to reduce the ﬂicker effect.   Throughthe use of interlacedscanning,single ﬁeld images with one-half the vertical resolution capability of the 525-line system are provided at the highﬂicker-perception threshold rate of 60 Hz. Higher resolution of the full 483 lines of vertical detail is provided at the lower ﬂicker-perception threshold rate of 30 Hz. The result is arelatively ﬂickerless picture display at ascreen brightness of well over 50 to 75 ft·L, morethan double that of motion-picture ﬁlm projection. Both 60-Hz ﬁelds and 30-Hz frames havethe same horizontal resolution capability.   The second advantage of interlacedscanning,compared to progressivescanning,wherethe frame is constructed in one pass over the displayface(rather than in two through interlace), is areduction in video bandwidth for an equivalent ﬂicker threshold level. Progressive scanning of 525 lines would havetobe completed in 1/60 stoachieve an equivalent level of ﬂickerperception. This would require aline scan to be completed in half the time of an interlacedscan. The bandwidth then would double for an equivalent number of pixels per line.1 -26                                  Broadcasting and Optical Communication Technology    The standards adopted by the Federal Communications Commission (FCC) for monochrometelevision in the United States speciﬁed asystem of 525 lines per frame, transmitted at aframe rate of 30 Hz, with each frame composedoftwo interlacedﬁelds of horizontal lines. Initially in the development of television transmission standards, the 60-Hz powerline waveform was chosen as aconvenient referencefor vertical scan. Furthermore, in the event of couplingofpowerline hum into the video signal or scanning/deﬂection circuits, the visible effects would be stationaryand less objectionable than moving hum bars or distortion of horizontal-scanning geometry. In the United Kingdom and much of Europe, a50-Hz interlaced system was chosen for manyofthe same reasons. With improvements in television receivers, the power line referencewas replaced with astable crystal oscillator,rendering the initial reason for the frame rate a moot point.   The existing 525-line monochromestandards were retained for color in the recommendations of the National Television System Committee (NTSC) for compatible color television in the early 1950s. The NTSC system, adopted in 1953 by the FCC, speciﬁes ascanning system of 525 horizontal lines per frame, witheach frame consisting of twointerlaced ﬁelds of 262.5 lines at aﬁeld rate of 59.94 Hz. Forty-two of the 525 lines in each frame are blanked as black picture signals and reserved for transmission of the vertical scanning synchronizing signal. This results in 483 visible lines of picture information. Because the vertical blanking interval represents asigniﬁcant amount of the total transmitted waveform, the television industryhas sought ways to carryadditional data during the blanking interval. Such applications include closed captioning and system test signals.  SynchronizingVideo     Signals In monochrometelevision transmission, two basic synchronizing signals are provided to control the timing of picture-scanning deﬂection:      . Horizontal sync pulses at the line rate.     . Vertical sync pulses at the ﬁeld rate in the form of an interval of wide horizontal sync pulses at the ﬁeld       rate. Included in the interval are equalizing pulses at twice the line rate to preserveinterlace in each       frame between the even and odd ﬁelds (offset by ahalf line). In color transmissions, athird synchronizing signal is added during horizontal scan blanking to provide a frequency and phase referencefor color signal encoding circuits in cameras and decoding circuits in receivers. These synchronizing and reference signals are combined with the picture video signal to form a composite video waveform.   The scanning and color-decoding circuits in receivers must followthe frequency and phase of the synchronizing signals to produceastable and geometrically accurate image of the proper color hue and saturation.Any change in timing of successive vertical scans can impair the interlaceofthe even and odd ﬁelds in aframe. Small errors in horizontal scan timing of lines in aﬁeld can result in aloss of resolution in vertical line structures. Periodic errorsoverseveral lines that may be out of the range of the horizontal scan automatic frequency control circuit in the receiver will be evident as jagged vertical lines.  Television  Industry  Standards There are three primarycolor transmission standardsinuse today:      . NTSC (National Television SystemsCommittee): Used in the United States, Canada, Central America,       most of South America, and Japan. In addition, NTSC is used in various countries or possessions       heavily inﬂuenced by the United States.     . PAL (Phase Alternation each Line): Used in England, most countriesand possessions inﬂuenced by the       British Commonwealth, manywestern European countries and China. Variation exists in PALsystems.     . SECAM (Sequential Color with [Avec] Memory): Used in France, countries and possessions inﬂuenced       by France, the USSR (generally the former Soviet Bloc nations), and other areas inﬂuenced by Russia.Broadcasting                                                                       1 -27  The threestandards are incompatible for avariety of reasons (see Benson and Whitaker, 1991).   Television transmitters in the United States operate in threefrequency bands:      . Low-bandVHF (veryhighfrequency), channels 2through 6     . High-band VHF,channels 7through 13     . UHF (ultra-high frequency), channels 14 through 83 (UHF channels 70 through 83 currently are       assigned to mobile radio services)   Table 1.6 shows the frequency allocations for channels 2through83. Because of the wide varietyof operating parameters for television stations outside the United States, this section will focus primarily on TV transmission as it relates to the Unites States.   Maximum  power output limits are speciﬁed by the FCC for each type of service.The maximum effective radiated power (ERP) for low-band VHF is 100 kW;for high-band VHF it is 316 kW;and for UHF it is 5MW.  The ERP of astation is afunction of transmitter poweroutput (TPO) and antenna gain. ERP is determined by multiplying these twoquantities together and subtracting transmission line loss.   The second major factor that affects the coverage area of aTVstation is antenna height, known in the broadcast industryas height above average terrain (HAAT).HAATtakesinto consideration the effects of the geographyinthe vicinityofthe transmitting tower.The maximum HAATpermitted by the FCC for alow- or high-band VHF station is 1000 ft (305 m) east of the Mississippi River and 2000 ft (610 m) west of the Mississippi. UHF stations are permitted to operate with amaximum HAATof2000 ft (610 m) anywhere in the United States (including Alaska and Hawaii).             TABLE 1.6 Frequency Allocations for TV Channels 2through83inthe U.S.            Channel    Frequency    Channel    Frequency    Channel    Frequency           Designation Band, MHz   Designation Band, MHz  Designation Band, MHz                254–60                 30        566–572       58       734–740               360–66                 31        572–578       59       740–746               466–72                 32        578–584       60       746–752               576–82                 33        584–590       61       752–758               682–88                 34        590–596       62       758–764               7174–180               35        596–602       63       764–770               8180–186               36        602–608       64       770–776               9186–192               37        608–614       65       776–782              10        192–198       38        614–620       66       782–788              11        198–204       39        620–626       67       788–794              12        204–210       40        626–632       68       794–800              13        210–216       41        632–638       69       800–806              14        470–476       42        638–644       70       806–812              15        476–482       43        644–650       71       812–818              16        482–488       44        650–656       72       818–824              17        488–494       45        656–662       73       824–830              18        494–500       46        662–668       74       830–836              19        500–506       47        668–674       75       836–842              20        506–512       48        674–680       76       842–848              21        512–518       49        680–686       77       848–854              22        518–524       50        686–692       78       854–860              23        524–530       51        692–698       79       860–866              24        530–536       52        698–704       80       866–872              25        536–542       53        704–710       81       872–878              26        542–548       54        710–716       82       878–884              27        548–554       55        716–722       83       884–890              28        554–560       56        722–728              29        560–566       57        728–7341 -28                                  Broadcasting and Optical Communication Technology    The ratio of visual output powerto aural output power can varyfrom one installation to another; however, the aural is typically operated at between 10 and 20% of the visual power. This difference is the result of the reception characteristics of the two signals. Much greater signal strength is required at the consumer’sreceiver to recoverthe visual portion of the transmission than the aural portion. The aural poweroutput is intended to be sufﬁcient for good reception at the fringe of the station’s coverage area but not beyond. It is of no use for aconsumer to be able to receiveaTV station’s audio signal but not the video.   In addition to highpower stations, two classiﬁcations of low-power TV stations havebeen established by the FCC to meet certain communityneeds: They are:      . Translator: Alow-power system that rebroadcasts the signal of another station on adifferent channel.       Translators are designed to provide ‘‘ﬁll-in’’ coverage for astation that cannot reachaparticular       communitybecause of the local terrain. Translators operating in the VHF band are limited to 100 W       poweroutput (ERP), and UHF translators are limited to 1kW.     . Low-Power Television (LPTV): Aservice established by the FCC designed to meet the special needs of       particular communities. LPTV stations operating on VHF frequencies are limited to 100 WERP,and       UHF stations are limited to 1kW. LPTV stations originate their own programming and can be assigned       by the FCC to anychannel, as long as sufﬁcient protection against interferencetoafull-power station       is afforded.  Composite  Video The compositevideo waveform is shown in Figure1.25. The actual radiated signal is inverted, with modulation extending from the synchronizing pulses at maximum carrier level(100%) to reference picture white at 7.5%. Because an increase in the amplitude of the radiated signal corresponds to adecrease in picture brightness, the polarityofmodulation is termed negative.   FIGURE 1.25 The principal components of the NTSC color television waveform.(Source: Electronic Industries Association.)Broadcasting                                                                       1 -29    The term composite is used to denote avideo signal that contains:      . Pictureluminanceand chrominanceinformation     . Timing information for synchronization of scanning and color signal processing circuits   The negative-going portion of the waveform shown in Figure1.25 is used to transmit information for synchronization of scanning circuits. The positive-going portion of the amplitude range is used to transmit luminanceinformation representing brightness and, for color pictures, chrominance.   At the completion of each line scan in areceiver or monitor,ahorizontal synchronizing ( H - sync)pulse in the composite video signal triggers the scanning circuits to return the beam rapidly to the left of the screen for the startofthe next line scan. During the return time, ahorizontal blanking signal at alevel lower than that corresponding to the blackest portion of the scene is added to avoid the visibilityofthe retracelines. In a similar manner,after completion of each ﬁeld, avertical blanking signal blanks out the retrace portion of the scanning beam as it returns to the top of the picture to startthe scan of the next ﬁeld. The small-level difference between video referenceblack and blanking level is called setup.Setup is used as aguard band to ensureseparation of the synchronizing and video-information functions and adequate blanking of the scanning retracelines on receivers.   The waveforms of Figure1.26 show the various referencelevels of video and sync in the composite signal. The unit of measurement for video level was speciﬁed initially by the Institute of Radio Engineers (IRE). These IRE units are still used to quantify video signal levels. The primaryIRE values are given in Table 1.7.        FIGURE 1.26 Sync pulse widths for the NTSC color system. ( Source: Electronic Industries Association.)1 -30                                  Broadcasting and Optical Communication Technology  Color Signal Encoding To facilitate an orderly introduction of color tele- TABLE 1.7 Video and SyncLevels in IRE Units vision broadcasting in the United States and other                                                      Signal Level              IRE Level countries with existing monochromeservices, it was essential that the new transmissions be compatible. Reference white            100 In other words, colorpictures would provide  Blanking level width measurement  20 acceptable qualityonunmodiﬁed   monochrome   Color burst sine wavepeak         þ 20 to –20 receivers. In addition, because of the limited avail- Reference black          7.5                                              Blanking                          0 abilityofthe RF spectrum, another related require- Sync pulse width measurement –20 ment was the need to ﬁt approximately 2-MHz  Sync level                        –40 bandwidth of color information into the 4.2-MHz video bandwidth of the existing 6-MHz broadcasting channels with little or no modiﬁcation of existing transmitters. This is accomplished by using the band-sharing color signal system developed by the NTSC and by taking advantage of the fundamental characteristics of the eye regarding color sensitivity and resolution.   The video-signal spectrum generated by scanning an image consists of energyconcentrated near harmonics of the 15,734-Hz line scanning frequency.Additional lower-amplitude sideband components exist at multiples of 60 Hz (the ﬁeld scan frequency) from each line scan harmonic. Substantiallynoenergyexists halfway between the line scan harmonics, that is, at odd harmonics of one half line frequency.Thus, these blank spaces in the spectrum are available for the transmission of asignal for carrying color information and its sideband. In addition, asignal modulated with color information injected at this frequency is of relatively low visibility in the reproduced image because the odd harmonics are of opposite phase on successive scanning lines and in successive frames, requiring four ﬁelds to repeat. Furthermore, the visibilityofthe color video signal is reduced further by the use of asubcarrier frequency near the cutoffofthe video bandpass.   In the NTSC system, color is conveyed using two elements:      . Aluminancesignal     . Achrominancesignal The luminancesignal is derivedfrom components of the threeprimarycolors —red,green, and blue —in  the proportions for reference white, E y ,asfollows:                               E y ¼ 0 : 3 E R þ 0 : 59E G þ 0 : 11E B These transmitted values equal unityfor white and thus result in the reproduction of colors on monochrome receivers at the proper luminancelevel. This is known as the constant-luminance principle.   The color signal consists of twochrominancecomponents, I and Q ,transmitted as amplitude-modulated sidebands of two3.579545-MHz subcarriers in quadrature. The subcarriers are suppressed, leaving only the sidebands in the color signal. Suppression of the carriers permits demodulation of the color signal as two separate color signals in areceiver by reinsertion of acarrier of the phase corresponding to the desired color signal ( synchronous demodulation).   I and Q signals are composed of red, green, and blue primarycolor componentsproduced by color cameras and other signal generators. The phase relationship among the I and Q signals, the derived primaryand complementarycolors, and the color synchronizing burst can be shown graphically on a vectorscope display. The horizontal and vertical sweep signals on avectorscope are produced fromR-Y and B-Y subcarrier sine wavesinquadrature, producing acircular display. The chrominancesignal controls the intensityofthe display. Avectorscope display of an Electronic Industries Association (EIA) standardcolor bar signal is shown in Figure1.27.  Color-Signal Decoding Each of the twochroma signal carriers can be recovered individually by means of synchronous detection. Areferencesubcarrier of the same phase as the desiredchroma signal is applied as agate to abalanced demodulator.Only the modulation of the signal in the same phase as the referencewill be present inBroadcasting                                                                       1 -31   FIGURE 1.27 Vectorscope representation for chroma and vector amplitude relationships in the NTSC system. ( Source: Electronic Industries Association.)  the output. Alow-pass ﬁlter maybeadded to remove second harmonic components of the chroma signal generated in the process.  Transmission Equipment Television transmitters are classiﬁed in terms of their operating band, powerlevel, type of ﬁnal ampliﬁer stage, and cooling method. The transmitter is divided into two basic subsystems:      . The visual section, which accepts the video input, amplitude modulates an RF carrier,and ampliﬁes the       signal to feed the antenna system     . The aural section, which accepts the audio input, frequency modulates aseparate RF carrier and       ampliﬁes the signal to feed the antenna system The visual and aural signals are combined to feed asingle radiating system.  Transmitter Design Considerations Each manufacturer has aparticular philosophywithregardtothe design and construction of abroadcast TV transmitter.Some generalizations can, however,bemade with respect to basic system design.   When the poweroutput of aTVtransmitter is discussed, the visual section is the primaryconsideration. Output powerrefers to the peak power of the visual section of the transmitter ( peak of sync). The FCC-licensed ERP is equal to the transmitter power output minus feedline losses times the power gain of the antenna.   Alow-bandVHF station can achieve its maximum 100-kW poweroutput through awiderange of transmitter and antenna combinations. A35-kW transmitter coupled with again-of-4 antenna would work, as would a10-kW transmitter feeding an antenna with again of 12. Reasonable pairings for ahigh-band VHF station would range from atransmitter with apower output of 50 kW feeding an antenna with again of 8, to a 30kW transmitter connectedtoagain-of-12 antenna. These combinations assume reasonable feedline losses.1 -32                                  Broadcasting and Optical Communication Technology  To reach the exact powerlevel, minor adjustments are made to the power output of the transmitter,usually by afront panel power trim control.   UHF  stations that want to achieve their maximum licensed power output are faced with installing avery high-power transmitter.Typical pairings include atransmitter rated for 220 kW and an antenna with again of 25, or a110-kW transmitter and again-of-50 antenna. In the latter case, the antenna could pose asigniﬁcant problem. UHF antennas with gains in the region of 50 are possible, but not advisable for most installations because of the coverage problems that can result. High-gain antennas haveanarrowvertical radiation pattern that can reduce astation’s coverage in areas near the transmitter site.   At ﬁrst examination, it might seem reasonable and economical to achievelicensed ERP using the lowest transmitter poweroutput possible and highest antenna gain. Other factors, however,come into play that makethe most obvious solution not always the best solution. Factors that limit the use of high-gain antennas include:      . The effects of high-gain designs on coverage area and signal penetration     . Limitations on antenna size because of tower restrictions, such as available vertical space, weight, and       windloading     . The cost of the antenna   The amount of output powerrequired of atransmitter will haveafundamental effect on system design. Powerlevels dictate whether the unit will be of solid-state or vacuum-tube design; whether air,water,orvapor cooling must be used; the type of power supply required; the sophistication of the high-voltage control and supervisorycircuitry; and manyother parameters.   Solid-state devicesare generally used for VHF transmitters and for low-power UHF transmitters. Tetrodes mayalso be used in these ranges. As solid-state technologyadvances, the powerlevelspossible in areasonable transmitter design steadily increase. In the realm of highpowerUHF transmitters, the klystron is acommon poweroutput device. Klystrons use an electron bunching technique to generate highpower (55 kW from asingle tube is not uncommon) at microwave frequencies. The klystron, however,isrelatively inefﬁcient in its basic form. Astock klystronwith no efﬁciency-optimizing circuitrymightbeonly 40 to 50% efﬁcient, depending on the type of device used. Various schemes havebeen devised to improveklystron efﬁciency,the best known of which is beam pulsing.Two types of pulsing are in common used:      . Mod-anode pulsing,atechnique designed to reduce powerconsumption of the klystron during the color       burst and video portion of the signal (and thereby improveoverall system efﬁciency)     . Annular control electrode (ACE) pulsing,which accomplishes basically the same thing by incorporating       the pulsing signal into alow-voltage stage of the transmitter,rather than ahigh-voltage stage (as with       mod-anode pulsing).   Still another approach to improving UHF transmitter efﬁciency involves entirely new classes of vacuum tubes: the inductive output tube (IOT) and the multistage depressed collector (MSDC) klystron.The IOTisa device that essentially combines the cathode/grid structureofthe tetrodewith the drift tube/collector structure of the klystron. The MSDC klystron incorporates acollector assembly that operates at progressively lower voltage levels. The net effect for the MSDC is to recover energyfromthe electron stream rather than dissipating the energyasheat.  Elements of the Transmitter Atelevision transmitter can be divided into four major subsystems:      . The exciter     . Intermediate powerampliﬁer (IPA)     . Power ampliﬁer (PA)     . High-voltage power supply Figure1.28 shows the audio,video,and RF paths for atypical television transmitter.Broadcasting                                                                       1 -33                    FIGURE 1.28 Simpliﬁed block diagram of aVHF television transmitter.    The modulated visual intermediate frequency (IF) signal is band-shaped in avestigial sideband ﬁlter, typically asurface-acoustic-wave (SAW) ﬁlter.Envelope-delay correction is not required for the SAWﬁlter because of the uniform delay characteristics of the device. Envelope-delay compensation may,however,be needed for other parts of the transmitter.The SAWﬁlter provides manybeneﬁts to transmitter designers and operators. ASAW ﬁlter requires no adjustments and is stable with respect to temperatureand time. A color- notch ﬁlter is required at the output of the transmitter because imperfect linearity of the IPAand PA stages introduces unwanted modulation products.   The power ampliﬁer raises the output energyofthe transmitter to the desiredRFoperating level. Tetrodes in television service are operated in the class Bmode to obtain reasonable efﬁciency while maintaining alinear transfer characteristic. Class Bampliﬁers, when operated in tuned circuits, provide linear performancebecause of the ﬂywheel effect of the resonancecircuit. This allows asingle tube to be used instead of two in push-pull fashion. The bias point of the linear ampliﬁer is chosen so that the transfer characteristic at lowmodulation levels matches that at higher modulation levels. The plate (anode) circuit of atetrode PA is usually built around acoaxial resonant cavity, which provides astable and reliable tank circuit.   Solid state transmitters typically incorporate amassively parallel design to achieve the necessarypower levels. So-called powerblocks of 1kWorgreater are combined as required to meet the target transmitter power output. Most designs use MOSFETsrunning in aclass D(or higher) switching mode. Anyone of several combiner schemes may be used to couplethe powerblocks to the load. Depending on the design, high- reliabilityfeatures maybeincorporated into the transmitter,including automatic disconnection of failed power blocks and hot-changing of defective modules.1 -34                                  Broadcasting and Optical Communication Technology    UHF  transmitters using aklystron in the ﬁnal output stage must operate class A, the most linear but also most inefﬁcient operating mode for avacuum tube. Twotypes of klystrons havetraditionally been used: integralcavity and external cavity devices. The basic theoryofoperation is identical for each tube, but the mechanical approach is radically different. In the integral cavity klystron,the cavities are built into the device to form asingle unit. In the external cavity klystron,the cavities are outside the vacuum envelope and are bolted around the tube when the klystron is installed in the transmitter.Anumber of factors come into playin adiscussion of the relativemerits of integral vs. external cavitydesigns. Primaryconsiderations include operating efﬁciency,purchase price, and life expectancy.   Transmitters based on IOTorMSDC klystron ﬁnal tubes have much in common with traditional klystron- based systems. There are, however, anumber of signiﬁcant differences, including:      . Low-level video waveform precorrection circuitry     . Drive power requirements     . Power supply demands and complexity     . Fault/arc suppression and protection     . Cooling system design and complexity     . Overall system efﬁciency   The transmitter block diagram of Figure1.28 shows separate visual and aural PA stages. This conﬁguration is normally used for high-power transmitters. Low-power designs often use acombined mode ( common ampliﬁcation)inwhich the aural and visual signals are added prior to the PA.This approach offers asimpliﬁed system but at the cost of additional precorrection of the input video signal.   PA stages often are conﬁgured so that the circuitry of the visual and aural ampliﬁers is identical, providing backup protection in the event of avisual PA failure. The aural PA can then be reconﬁgured to amplify both the aural and the visual signals at reduced power.   The aural output stage of atelevision transmitter is similar in basic design to afrequency modulated (FM) broadcast transmitter.Tetrodeoutput devicesgenerally operate class C; solid-state devices operate in one of manypossible switching modes for highefﬁciency.The aural PA for aUHF transmitter may use aklystron, IOT, MSDC, tetrode, or agroup of solid-state power blocks.   Harmonic ﬁlters are employed to attenuate out-of-band radiation of the aural and visual signals to ensure compliance with FCC requirements. Filter designs varydepending upon the manufacturer;however,most are of coaxial construction utilizing L and C components housed within aprepackaged assembly.Stub ﬁlters are also used, typically adjusted to provide maximum attenuation at the second harmonic of the operating frequency of the visual carrier and the aural carrier.   The ﬁlteredvisual and aural outputs are fed to ahybrid diplexer where the twosignals are combined to feed the antenna. Forinstallations that require dual-antenna feedlines, ahybrid combiner with quadrature-phased outputs is used. Depending upon the design and operating power,the color-notch ﬁlter,aural and visual harmonic ﬁlters, and diplexer may be combined into asingle mechanical unit.  Antenna  System Broadcasting is accomplished by the emission of coherent electromagnetic wavesinfreespace from one or moreradiating-antenna elements that are excited by modulated RF currents. Although, by deﬁnition, the radiated energyiscomposed of mutually dependent magnetic and electric vector ﬁelds, conventional practice in television engineering is to measureand specify radiation characteristics in terms of the electric ﬁeld only.   The ﬁeld vectors maybepolarized horizontally,vertically,orcircularly.Television broadcasting,however, has used horizontal polarization for the majorityofinstallations worldwide. More recently interest in the advantages of circular polarization has resulted in an increase in this form of transmission, particularly for VHF channels. Both horizontal and circular polarization designs are suitable for tower-top or side-mounted installations. The latter option is dictated primarily by the existence of apreviously installed tower-top antenna. On the other hand, in metropolitan areas whereseveral antennas must be located on the same structure, either astacking or candelabra-type arrangement is feasible. Another approach to TV transmission involves combining the RF outputs of twoormorestations and feeding asingle wideband antenna.Broadcasting                                                                       1 -35  This approach is expensive and requires considerable engineering analysis to produce acombiner system that will not degrade the performanceofeither transmission system.  Television Reception The broadcast channels in the United States are 6MHz wide for transmission on conventional 525-line standards. The minimum signal levelatwhich atelevision receiver willprovide usable pictures and sound is called the sensitivity level.The FCC has set up twostandard signal level classiﬁcations, Grades Aand B, for the purpose of licensing television stations and allocating coverage areas. Grade Arefers to urban areas relatively near the transmitting tower;Grade Buse ranges from suburban to rural and other fringe areas anumber of miles from the transmitting antenna.   Many sizes and form factors of receivers are manufactured. Portable personal types include pocket-sized or hand-held models with picture sizes of 2to4in. diagonal for monochromeand 5to6in. for color.Large screen sizes are available in monochrome wherelow cost and light weight are prime requirements. However, except where portabilityisimportant, the majorityoftelevision program viewing is in color.The 19- and 27-in. sizes dominate the market.   Television receiver functions may be broken down into several interconnected blocks. With the increasing use of large-scale integrated circuits, the isolation of functions has become less obvious in the design of receivers. The typical functional conﬁguration of areceiver using atrigun picture tube is shown in Figure 1.29. Display Systems Color video displays may be classiﬁed under the following categories:      . Direct-view CRT     . Large-screen display, optically projected from aCRT     . Large-screen display, projectedfrom amodulated light beam     . Large-area display of individually driven light-emitting CRTs or incandescent picture elements     . Flat-panel matrix of transmissiveorreﬂective picture elements     . Flat-panel matrix of light-emitting picture elements   The CRTremains the dominant  type of displayfor both consumer and professional 525-/625-line television applications. Light-valve systems using amodulated light sourcehavefound wide application for                 FIGURE 1.29 Simpliﬁed schematic block diagram of acolor television receiver.1 -36                                  Broadcasting and Optical Communication Technology  presentations to large audiencesintheater environments, particularly wherehighscreen brightness is required. Matrix-driven ﬂat-panel displays are used in increasing numbers for television receivers and for portable projector units. Video and data projectors using LCDtechnologyhavegained wide acceptance.  Cathode Ray Tube  Display The direct-view CRTisthe dominant display device in television. The attributes offered by CRTs include the following:      . High brightness     . High resolution     . Excellent gray-scale reproduction     . Low cost compared to other types of displays From the standpoint of television receiver manufacturing simplicity and lowcost, packaging of the display device as asingle component is attractive.The tube itself is composedofonly threebasic parts: an electron gun, an envelope, and ashadow-mask phosphor screen.The luminanceefﬁciency of the electron optical system and the phosphor screen is high. Apeak beam current of under 1 m Aina25-in. tube will produce a highlight brightness of up to 100 ft·L. The major drawback is the power required to drive the horizontal sweep circuit and the highacceleratingvoltage necessaryfor the electron beam. This requirement is partially offset through generation of the screen potential and other lowervoltages by rectiﬁcation of the scanning ﬂyback voltage.   As consumer demands drive manufacturers to produce larger picture sizes, the weight and depth of the CRT and the higher power and voltage requirements become serious limitations. These are reﬂected in sharply increasing receiver costs. To withstand the atmospheric pressures on the evacuated glass envelope, CRTweight increases exponentially with the viewable diagonal. Nevertheless, manufacturers havecontinued to meet the demand for increased screen sizes with larger direct-view tubes. Improved versions of both tridot delta and in- line guns have been produced. The tridot gun provides small spot size at the expense of critical convergence adjustments for uniform resolution over the full-tube faceplate. In-line guns permit the use of aself- converging deﬂection yoke that will maintain dynamic horizontal convergence over the full face of the tube without the need for correction waveforms. The downside is slightly reduced resolution.  Deﬁning   Terms Aural:  The sound portion of atelevision signal. Beam  pulsing: Amethod used to control the poweroutput of aklystron in order to improve the operating      efﬁciency of the device. Blanking:  The portion of atelevision signal that is used to blank the screen during the horizontal and      vertical retrace periods. Composite  video: Asingle video signal that contains luminance, color,and synchronization information.      NTSC, PAL, and SECAM are all examples of compositevideo formats. Effective radiated power: The power supplied to an antenna multiplied by the relativegain of the antenna      in agiven direction. Equalizing pulses: In an encoded video signal, aseries of 2X line frequency pulses occurring during      vertical blanking, before and after the vertical synchronizing pulse. Differentnumbers of equalizing      pulses are insertedinto different ﬁelds to ensure that each ﬁeld begins and ends at the right time to      produce proper interlace.The 2X line rate also servestomaintain horizontal synchronization during      vertical blanking. External cavity klystron: Aklystron device in which the resonant cavities are located outside the vacuum      envelope of the tube. Field: One of the two (or more) equal parts of information into which aframe is divided in interlace video      scanning.Inthe NTSC system, the information for one picture is divided into two ﬁelds. Each ﬁeldBroadcasting                                                                       1 -37       contains one-half the lines required to produce the entire picture. Adjacent lines in the picture are      contained in alternate ﬁelds. Frame:  The information required for one complete picture in an interlaced video system. For the NTSC      system, thereare twoﬁelds per frame. H (horizontal): In television signals, H may refer to anyofthe following: the horizontal period or rate,      horizontal line of video information, or horizontal sync pulse. Hue:  One of the characteristics that distinguishes one color from another.Hue deﬁnes color on the basis of      its position in the spectrum (red, blue, green, yellow,etc.). Hueisone of the three characteristics of      television color.Hue is often referred to as tint.InNTSC and PALvideo signals, the hue information at      anyparticular point in the picture is conveyed by the corresponding instantaneous phase of the active      video subcarrier. Hum  bars: Horizontal black and white bars that extend over the entire TV picture and usually drift slowly      through it. Humbars are caused by an interfering power line frequency or one of its harmonics. Inductive output tube: An ampliﬁer device for UHF-TV signals that combines aspects of atetrode (grid      modulation) with aklystron (velocity modulation of an electronbeam). The result is amoreefﬁcient,      less expensivedevice for manyapplications. Integral cavity klystron: Aklystron device in which the resonant cavities are located inside the vacuum      envelope of the tube. Interlaced: Ashortened version of interlaced scanning (also called line interlace). Interlaced scanning is a      system of video scanning wherebythe odd- and even-numbered lines of apicture are transmitted      consecutively as twoseparate interleaved ﬁelds. IRE:  Aunit equal to 1/140 of the peak-to-peak amplitude of avideo signal, which is typically 1V.The 0IRE      point is at blanking level, with the sync tip at –40 IRE and white extending to 1 100 IRE. IRE stands for      Institute of Radio Engineers,anorganization preceding the IEEE, which deﬁned the unit. Klystron: An ampliﬁer device for UHF and microwave signals based on velocitymodulation of an electron      beam. The beam is directed through an input cavity,wherethe input RF signal polarityinitializes      a bunching effect on electrons in the beam. The bunching effect excites subsequent cavities, which      increase the bunching through an energyﬂywheel concept. Finally,the beam passes an output cavity      that couples the ampliﬁed signal to the load (antenna system). The beam falls onto acollector element      that forms the return path for the current and dissipates the heat resulting from electron beam      bombardment. Low-power TV (LPTV):   Atelevision service authorized by the FCC to serve speciﬁc conﬁned areas. An      LPTV station may typically radiate between 100 and 1000 Wofpower,covering ageographic radius of      10 to 15 mi. Multistage depressed collector (MSDC) klystron: Aspecially designed klystron in which decreasing      voltage zones cause the electron beam to be reduced in velocitybeforestriking the collector element. The      effect is to reduce the amount of heat that must be dissipated by the device, improving operating      efﬁciency. Pixel: The smallest distinguishable and resolvable areainavideo image. Apixel is asingle point on the      screen. The wordpixel is derived from picture element. Raster: Apredetermined pattern of scanning the screen of aCRT. Raster may also refer to the illuminated      area produced by scanning lines on aCRT when no video is present. Saturation: The intensity of the colorsinthe active picture,the voltage levels of the colors. Saturation      relates to the degree by which the eyeperceives acolor as departing from agrayorwhite scale of the      same brightness. A100% saturated colordoes not contain anywhite; adding white reduces saturation.      In NTSC and PALvideo signals, the color saturation at anyparticular instant in the picture is conveyed      by the corresponding instantaneous amplitude of the activevideo subcarrier. Scan:  One sweep of the target areainacamera tube or of the screen in apicture tube. Setup:  Avideo term relatingtothe speciﬁed base of an activepicture signal. In NTSC, the active picture      signal is placed 7.5 IRE units above blanking (0 IRE). Setup is the separation in level between the video      blanking and reference black levels.1 -38                                  Broadcasting and Optical Communication Technology  Synchronous detection: Ademodulation process in which the original signal is recovered by multiplying      the modulated signal by the output of asynchronous oscillator locked to the carrier. Translator: An unattended television or FM broadcast repeater that receives adistant signal and      retransmits the picture and/or audio locally on another channel. Vectorscope: An oscilloscope-typedevice used to displaythe color parameters of avideo signal. Avector-      scope decodescolor information into R-Yand B-Y components, which are then used to drive the X and      Y axis of the scope. The total lack of color in avideo signal is displayed as adot in the center of the      vectorscope. The angle, distancearound the circle, magnitude, and distance away from the center      indicate the phase and amplitude of the color signal.  References K.B. Benson and J. Whitaker (Eds.), Television Engineering Handbook,rev.ed., NewYork: McGraw-Hill, 1991. K.B. Benson and J. Whitaker, Television and Audio Handbook for Technicians and Engineers,New York:      McGraw-Hill, 1990. J. Whitaker, Radio Frequency Transmission Systems Handbook,Boca Raton: CRCPress, 1991. J. Whitaker, Electronic System Maintenance Handbook,Boca Raton: CRCPress, 1991.  Further  Information Additional information on the topic of television system technologyisavailable from the following sources:   Broadcast Engineering magazine, amonthly periodical dealing withtelevision technology. The magazine, published by Primedia Business located in Overland Park, Kan., is free to qualiﬁed subscribers.   The SocietyofMotion Pictureand Television Engineers, which publishes amonthly journal and holds conferences in the fall and winter.The SMPTE is headquarteredinWhite Plains, N.Y.   The SocietyofBroadcast Engineers, which holds technical conferenceisthroughout the year.The SBE is located in Indianapolis, Ind.   The National Association of Broadcasters, which holds an annual engineering conferenceand trade show in the spring.The NABisheadquarteredinWashington, D.C.   In addition, the following books are recommended: J. Whitakerand K.B. Benson (Eds.), Standard Handbook of Video and Television Engineering,4th ed.,      NewYork: McGraw-Hill, 2003. J. Whitaker and K.B. Benson (Eds.), Standard Handbook of Audio and Radio Engineering,2nd ed., NewYork:       McGraw-Hill, 2001. J. Whitaker and K.B. Benson (Eds.), Standard Handbook of Broadcast Engineering,New York: McGraw-Hill,      2005. National Association of Broadcasters Engineering Handbook,9th ed., Washington, D.C.: NAB, 1997.  1.4     High-Deﬁnition       Television Martin   S. Roden History When  standards were ﬁrst developed for television, few people dreamed of its evolution into atypeof universal communication terminal. While the historical standards are acceptable for entertainment video,they are not adequate for manyemerging applications. We had to evolve into ahigh-resolution standard. High- deﬁnition TV (HDTV)isaterm  applied to abroad class of systems in which developments havereceived worldwide attention.   HDTV  can trace its beginning to Japan during the late 1960s. In 1987, the United States FCC (Federal Communications Commission) was  petitioned by broadcasters to set aside frequencies for HDTVBroadcasting                                                                       1 -39  broadcasting.The FCC workedfor anumber of yearstodevelop standards. In the early 1990s, four competing standards weretested. Instead of choosing one of these, a Grand Alliance was formed to develop asingle set of standards. The FCC approvedthe resulting standardonDecember 24, 1996, and the ﬁrst receivers were introduced in the United States during 1998.  Traditional TV Standards We begin with abrief review of the traditional television standards.   Japan and NorthAmerica use the National Television Systems Committee (NTSC) standardthat speciﬁes 525 scanning lines per picture,aﬁeld rate of 59.94 per second (nominally 60 Hz), and 2:1 interlacedscanning (although there are about 60 ﬁelds per second, thereare only 30 new frames per second). The aspect ratio (ratio of width to height) is 4:3. The bandwidth of the television signal, including both video and sound, is 6MHz. In Europe and some countriesoutside of Europe, the phase-alternation line (PAL), or the sequential color,and memory(SECAM) standardare used. This speciﬁes 625 scanning lines per picture and aﬁeld rate of 50 per second. The bandwidth of this type of television signal is 8MHz.   HDTV systems signiﬁcantly increase the number of scan lines in aframe and change the aspect ratio to 16:9. Of course, if we were willing to startfrom scratch and abandon all existing television systems, we could set the bandwidth of each channel to anumber greater than 6(or 8) MHz, thereby achieving higher resolution. The Japanese Broadcasting Corporation (NHK[NAL2]) has done this in their HDTV system. This system permits 1125 lines per frame with 30 frames per second and 60 ﬁelds per second (2:1 interlaced scanning). The aspect ratio is 16:9. The system is designed for abandwidth of 10 MHz per channel. With the 1990 launching of the BS-3 satellite, twochannels weredevoted to this form of HDTV.Toﬁtthe channel within a10-MHz bandwidth (instead of the approximately 50 MHz that would be needed to transmit the HDTV signal using traditional techniques), bandwidth compression was required. It should be noted that the Japanese system primarily uses analog frequency modulation (FM) (the sound is digital). The approach to decreasing bandwidth is multiple sub-Nyquist encoding ( MUSE). The sampling belowNyquist lowers the bandwidth requirement, but moving images suffer from less resolution.   Europe began its HDTV project in mid-1986 withajoint initiative involving West Germany(RobertBosch GmbH), the Netherlands (NV Phillips), France (Thomson SA), and the United Kingdom (Thorn/EMI Plc.). The system, termed Eureka 95 or D2-MAC, has 1152 lines per frame, 50 ﬁelds per second, 2:1 interlaced scanning,and a16:9 aspect ratio.Amorerecent European standardspeciﬁes 1250 scanning lines at 50 ﬁelds per second. This is known as the Eureka EU95.Itissigniﬁcant to note the number of lines speciﬁed by Eureka EU95 is exactly twice that of the PALand SECAM standardcurrently in use. The ﬁeld rate is the same, so it is possible to devise compatible systems that would permit receptionofHDTV by current receivers (of course,withadapters and without enhanceddeﬁnition). The HDTV signal requires nominally 30 MHz of bandwidth.  HDTV Formats In the United States, the FCC ruled (in March 1990) that anynew HDTV system must permit continuation of service to contemporaryNTSC receivers. This signiﬁcant constraint applies to terrestrial broadcasting (as opposed to videodisk, videotape, satellite, and cable television). The HDTV signals can be sent on taboo channels,those not used in metropolitan areas, so adequate channel separation can be maintained. Thus, these currently unused channels would be used for simulcast signals. Sincethe HDTV system for the United States uses digital transmission, transmitter powercan be less than that used for conventional television—this reduces interference with adjacent channels. Indeed, in heavily populated urban areas (where manystations are licensed for broadcast), the HDTV signals must be severely limited in power.   There are atotal of 18 formats speciﬁed for digital television, and six of these fall in the HDTV category. The differencesbetween these formats depend on the resolution and on whether progressiveorinterlacedscanning is used.1 -40                                  Broadcasting and Optical Communication Technology    The interlaced scanning format evolved with the beginnings of broadcast television. The number of frames per second must be highenoughtoavoid the perception of ﬂicker.But the higher the number of frames per second, the higher the bandwidth of the resulting video signal. In interlaced scanning,the entirepicture area is painted twice for everyframe. On the ﬁrst scan, the odd numberedlines are traced, while on the second scan, the even numberedlines are ﬁlled in. Thus, the screen shows an image twicefor each frame. If, for example, you wanted to transmit 30 frames per second, you could do this with 60 scans per second. The human eye would not detect ﬂickeratthe 60 per second rate (i.e., youreye integrates the signal and ﬁll in between the scans), but at arate of 30 per second most people would detect the stroboscopic effect. Of course,inevery engineering decision thereisatrade off. While interlaced scanning savesbandwidth, it is less effective for rapid motion (i.e., if you werebionic, you would see atypeofcomb effect when objects moved, like slicing documents in aone-directional paper shredder). Also,with higher deﬁnition formats, the eye becomes more sensitivetocertain types of ﬂicker.   The alternative to interlaced scanning is progressive scanning.Inthis format, all lines are scanned in sequence,and the entireframe is scanned.   The threeformats most often used in HDTV are 720p,1080i, and 1080p.The number refers to the number of vertical lines, while the letter indicates whether the scanning is progressive or interlaced.   The 720p format forms the ﬁeld using 720 vertical lines. Each line contains 1280 pixels horizontally. Progressive scanning is used, and 60 complete pictures are sent everysecond. Althoughthe resolution quality of 720p is lower than the 1080 formats, it provides smooth motion.   The 1080i format forms the ﬁeld using 1080 vertical lines. Each line contains 1920 pixels horizontally. Interlaced scanning is used, just as in traditional TV,and 30 complete picturesare sent everysecond. 1080i represents acompromise between 720p and 1080p.Itprovides the highest possible resolution, but because it uses interlaced scanning,itdoes not do as well on motion rendition as the progressive scan formats. While this does not havethe motion rendition of 1080p,incertain applications your eyecannot tell the difference.When thereisverylittle motion, and when asignal is derived from atraditional motion picture ﬁlm are twoexamples of this. In the latter case, traditional celluloid ﬁlm uses 24 pictures per second ﬂashed on the screen 48 times per second (twice per picture).This was undoubtedly the original inspiration for interlaced scanning.  Signal Processing When acolor television signal is converted from analog to digital (A/D), the luminance, hue, and saturation signals must each be digitized using eightbits of A/D per sample. Therefore, digital transmission of conventional television requires anominal bit rate of about 216 megabits/s, while uncompressedHDTV nominally requires about 1200 megabits/s. If we weretouse adigital modulation system that transmits one bit per hertz of bandwidth, we see that the HDTV signal requires over 1GHz of bandwidth, yetonly 6MHz is allocated. Signiﬁcant data compression is required.   In the early 1990s, four digital HDTV approaches were submitted for FCC testing. The four were proposed by General Instrument Corporation, the Advanced Television Research Consortium (composed of NBC, David SarnoffResearch Center,Philips Consumer Electronics, and Thomson Consumer Electronics, Inc.), Zenith Electronics in cooperation with AT&T Bell Labs and AT&T Microelectronics, and the American Television Alliance (General Instrument Corporation and MIT). There were manycommon aspects to the four proposals, but major differences existed in the data compression approaches. The data compression techniques can be viewed as two-dimensional extensions of techniques used in voiceencoding.   Something unprecedented happened in the spring of 1993. The various competing parties decided, with some encouragement fromanFCC  advisorycommittee, to merge to form aGrand Alliance.The Alliance consists of seven members: AT&T,General Instrument Corp., MIT,Philips, Sarnoff, Thomson, and Zenith. This permitted the selection of the best featuresofeach of the proposals. The advisorycommittee was then able to spend the fall of 1995 on completion of the proposed HDTV standard. In the following,wedescribe a generic system. The reader is referred to the references for details.Broadcasting                                                                       1 -41                           FIGURE 1.30 Block diagram of HDTV transmitter.    Figure1.30 shows ageneral block diagram of adigital HDTV transmitter.Each frame from the camera is digitized, and the system has the capabilityofstoring one entireframe. Thus, the processor works with two inputs—the current frame (A) and the previous frame (B). The current frame and the previous frame are compared in amotion detector that generates coded motion information (C). Algorithms used for motion estimation attempt to produce three-dimensional parameters fromsequential two-dimensional information. Parameters mayinclude velocity estimates for blocks of the picture.   The parameters from the motion detector are processed along with the previous frame to produce a prediction of the current frame (D). Sincethe motion detector parameters are transmitted, the receiver can perform asimilar prediction of the current frame.   We are describing acompression scheme known as MPEG-2.Ifonly asmall section of the picture changes, the MPEG-2 encoder only changes that area and leaves the rest of the picture unchanged. On the next frame in the video,only that section of the picture is changed.   MPEG-2 has some problems, but it is agood compression scheme, and it is already an industrystandard for digital video for DVD-Videos and some satellite television services. One problemisthat MPEG-2 is a lossy compression method. That means ahigher compression rate gives apoorer picture.There is some loss in picture qualitybetween the digital video camera and the ﬁnal image produced on the television. However,the qualityisstill signiﬁcantly better than an average NTSC image. Using these compression schemes, MPEG-2 can reduce the amount of bits by about 55 to 1. With this compression ratio,much of the information is lost, but the change is not easily noticeable visually.Infact, the compression is carefully designed to match the characteristics of human vision. As one example, the human eyeismore sensitivetovariation in luminance than to variation in chrominance, and this can be used in the design of the compression algorithms.   The human ear is not as forgiving as the eye. The ear is much moresensitive to subtle changes in sound. Digital TV improves the sound over traditional analog television by using advances in digital sound developed over the last two decades.   Referring back to the block diagram, the predicted current frame is compared to the actual current frame, and adifference signal (E) is generated. This difference signal generally has asmaller dynamic range than the original signal. For example, if the television image is static (i.e., is not changing with time), the difference signal will be zero.   The difference signal is compressed to form the transmitted video signal (F). This compression is performed both in the time and transform domains. Entropycoding of the type used in facsimile can be incorporated to take spatial continuityinto account (i.e., apictureusually does not change over the span of asingle picture element, so variations of run length coding can often compress the data). The compression technique incorporates the MPEG-2 syntax. The actual compression algorithms, based on the discrete cosine transform, are adaptive; therefore, avarietyofformats can be accommodated (e.g., 1080-line interlaced scanning,720-line progressive, bi-directional). The main feature is the decreased data rate caused by extracting essential parameters that describe the waveform.1 -42                                  Broadcasting and Optical Communication Technology                             FIGURE 1.31 Block diagram of HDTV receiver.    Four data streams are asynchronously multiplexed to form the information to be transmitted (G). These four signals consist of the coded differential video,the motion detector parameters, the digital audio signal (using Dolby Labs’ AC-3 digital audio), and the synchronizing signals. Other information can be multiplexed, including various control signals that maybeneeded by cable operators.   Forward errorcorrection is applied to the multiplexed digital signal to produce an encoded signal (H) that makes the transmission less susceptible to uncorrected bit errors. This is needed because of the anticipated low transmission power rates. Error control is also important because compression can amplify error effects; asingle bit error can affect manypicture elements.   The encoded data signal forms the input to the modulator.Tofurther conserve bandwidth, atypeof quadraturemodulation is employed. The actual form is 8-VSB,avariation of digital vestigial sideband that includes trellis coding.This possesses manyofthe advantages of quadratureamplitude modulation (QAM).   The corresponding receiver is shown in Figure 1.31. The receiver simply forms the inverse of each transmitter operation. The received signal is ﬁrst demodulated. The resulting data signal is decodedtoremove the redundancy and correct errors. Ademultiplexer separates the signal into the original four (or more) data signals. The audio and synchronization signals need no further processing.   The demultiplexed video signal should be the same as the transmitted signal (‘‘F’’). We use letters with quotation marks to indicate that the signals are estimates of their transmitted counterpart. This reproduced video signal is decompressed, using the inverse algorithm of that used in the transmitter to yield an estimate of the original differential picture signal (‘‘E’’). The predict block in the receiver implements the same algorithm as the transmitter.Its inputs are the reconstructed motion signal (‘‘C’’) and the previous reconstructed frame (‘‘B’’). When the predictor output (‘‘D’’) is added to the reconstructed differential picture signal (‘‘E’’), the result is areconstructed version of the current frame.  Deﬁning   Terms Aspect ratio: Ratio of frame width to height. Digital vestigial sideband: Aform of digital modulation where aportion of one of the sidebands is      partially suppressed. Discrete cosine transform: Apopular format for video compression. The spatial signal is expanded in a      cosine series, where the higher frequencies represent increased video resolution. Entropy coding: Aform  of data compression that reduces atransmission to ashorter length by reducing      signal redundancy. Eureka 95 and EU95:  European HDTV  systems. Grand Alliance: Aconsortium formed of seven of the organizations proposing HDTV systems. Interlaced scanning: Abandwidth reduction technique wherein everyother scan line is ﬁrst transmitted      followed by the in-between lines.Broadcasting                                                                       1 -43  Motion detector: Asystem that compares two adjacent frames to detect differences. MPEG-2:   Video compression standarddevised by the Moving PictureExperts Group. MUSE:   Multiple sub-Nyquist encoding, atechnique used in Japanese HDTV system. Taboo channels:  Channels that the FCC does not currently assign in ordertoavoid interference from      adjacent channels. Trellis coding: Aform of digital encoding that provides aconstraint (i.e., astructure)toastream of      digital data.  References S. Banerjee, ‘‘Brief overview of the evolution of an technologyfor highdeﬁnition television,’’ Proc. SPIE,      vol.3582, 1998. G.W.Beakley,‘‘Channel coding for digital HDTV terrestrial broadcasting,’’ IEEE Trans. Broadcast.,vol.37,      no.4,1991. A. Bock and G. Drury, ‘‘The introduction of HDTV into digital television networks,’’ SMPTE J.,August, 1998. R. Hopkins, ‘‘Digital HDTV broadcasting,’’ IEEE Trans. Broadcast.,vol. 37, no.4,1991. P.C. Jain and V. Mitra, ‘‘Digital television and video compression,’’ IETE J. Res.,vol. 46, no.5,September-      October,2000. R.K. Jurgen, Ed., ‘‘The challenges of digital HDTV,’’ IEEE Spectrum,April, 1991. Conrad Persson, Guide to HDTV Systems,New York: Delmar Learning,1999. M.S. Roden, Analog and Digital Communication Systems,5th ed., Los Angeles, CA: Discovery Press,      2003. W.Y. Zou, ‘‘Digital HDTV compression techniques,’’ IEEE Trans. Broadcast.,vol.37, no.4,1991.  Further Information The IEEE Transactions on Broadcasting and Consumer Electronics continue to haveperiodic articles relating to the HDTV standardsand implementation of these standards. Another source of information, thoughnot overly technical, is the periodical Broadcasting and Cable (available by subscription online at www.broadcastingcable.com). Of course, the Webcan be used in conjunction with anypopular search engine. The vast majority of Websites havecommercial messages, but informational articles and sometimes even universityclass notes with tutorial information are available.  1.5     Digital Audio Broadcasting Stanley Salek and Almon H. Clegg  Digital audio broadcasting (DAB) is the term applied to afew independently developed emerging technologies that promise to provide consumers with anew and better aural broadcast system. DABoffers dramatically better reception over existing terrestrial analog AM and FM broadcasts by providing improved audio qualityand by integrating technologytoprovide resistance to interference in stationaryand mobile/ portable reception environments. Additionally,the availabilityofadigital data stream direct to consumers opens the prospect of providing extra services to augment basic sound delivery.   At this time, DABtransmission and reception systems are available from at least ﬁvesystem developers. Two of the systems, from Sirius Satellite Radio and XM Satellite Radio,transmit viasatellite, using ground-based augmentation transmitters as needed, while the system from the Eureka 147/DAB consortium can be deployed as asatellite, terrestrial, or hybrid satellite/terrestrial system. The remaining systems, from iBiquityDigital Corporation and the Digital Radio Mondiale consortium, are designed for terrestrial signal delivery,operating in established broadcasting frequency bands. This chapter provides ageneral overview of the technical aspects of DABsystems, as well as asummaryofthreeexample transmission systems.1 -44                                  Broadcasting and Optical Communication Technology  The  Need  for DAB Sincethe early 1980s, the consumer marketplace has undergone agreat shift towarddigital electronic technology. The explosion of personal computer use has led to greater demands for information, including full multimedia integration. Overthe same time period, compact disc (CD) digital audio technologyovertook long-playing records and analog audio cassettes as the consumer audio playback medium of choice. Similar digital transcription methods and effects havealso been incorporated into commonly available audio and video equipment, including digital videodisc (DVD) and hard disk/ﬂash memory-based recorders and players. Additionally,the ongoing transition to highdeﬁnition television broadcasting systems has fully incorporated digital methods for video and audio transmission. Because of these market pressures, the radio broadcast industrydetermined that the existing analog methods of broadcasting needed updating to keep pacewith the expectations of the advancing audio marketplace.   Along with providing signiﬁcantly enhancedaudio quality, DABsystems havebeen developed to overcome the technical deﬁciencies of existing AM and FM analog broadcasting systems. The foremost problemof current broadcast technology, as perceived by the industry, is its susceptibilitytointerference. AM medium- wave broadcasts operating in the 530- to 1700-kHz frequency range are prone to disruption by ﬂuorescent lighting and powersystem distribution networks, as well as by numerous other unintentional radiators, including computer and telephone systems. Additionally,natural effects such as nighttime skywave propagation, interference between stations, and nearby lightning discharges cause irritating service disruption to AM reception. FM broadcast transmissions in the 88- to 108-MHz band are much more resistant to these types of interference.However,multipath propagation and abrupt signal fading,especially found in urban and mountainous areas containing alarge number of signal reﬂectors and shadowers (e.g., buildings and terrain), can seriously degrade FM reception, particularly to mobile receivers.  DAB   System  Design   Goals DABsystems are designed with several technical goals in mind. The ﬁrst goal is to create aservice that delivers compact disc qualitystereosound for broadcast to consumers. The second goal is to overcome the interference problems of current AM and FM broadcasts, especially under portable and mobile reception conditions. Third,DAB must be spectrally efﬁcient; total bandwidth should be no greater than the bandwidth currently used for corresponding analog broadcasts. Fourth, the DABsystem should provide space in its data stream to allowfor the addition of ancillaryservicessuch as textual program information display, software downloading, or subscription data services. Finally,tofoster rapid consumer acceptanceDAB receivers must not be overly cumbersome, complex, or expensive.   In addition to these goals, desired featuresinclude reduced RF transmission powerrequirements (when compared to AM and FM broadcast stations having the same signal coverage), amechanism to seamlessly ﬁll- in coverage areas that are shadowed from the principal transmitted signal, and the abilitytoeasily integrate DABreceivers into personal, home, and automotivesound systems.  Historical Background DABdevelopment work began in Europe in 1986, with the initial goal to provide high-quality audio services to consumers directly by satellite. Companion terrestrial systems were developed to evaluate the technology, as well as to provide ﬁll-in service in small areas wherethe satellite signals were shadowed. Aconsortium of European technical organizations known as Eureka-147/DAB demonstrated the ﬁrst working terrestrial DAB system in Geneva, Switzerland, in September 1988. Subsequent terrestrial system demonstrations followed in Canada during the summer of 1990 and in the United States in 1991 (http://www.worlddab.org/eureka.aspx).   VHF and UHF transmission frequenciesbetween 200 and 900 MHz were used for the demonstrations with satisfactoryresults. Because most VHF and UHF frequency bands suitable for DABare already in use (or in use by/reservedfor digital television and other services),anadditional Canadian study in 1991 evaluated frequencies near 1500 MHz (L-band) for use as apotential worldwide DABallocation. This study concludedBroadcasting                                                                       1 -45  that L-bandfrequencieswould supportaDABsystem such as Eureka-147/DAB,while continuing to meet the overall system design goals.   During the scheduled 1992 World AdministrativeRadio Conference(WARC-92), frequency allocations for manydifferent radio systems weredebated. As aresult of WARC-92, aworldwide L-band standard allocation at 1452 to 1492 MHz was designated for both satellite and terrestrial digital radio broadcasting.However, because of existing government and militaryuses in L-band, the United States was excluded from the standard. Instead, an S-band allocation at 2310 to 2360 MHz was substituted. Additionally,nations including Japan, China, and Russia opted for an extra S-band allocation in the 2535- to 2655-MHz frequency range.   During the same time period, most DABsystem development work in the United States shifted from out- band (i.e., UHF,L-band, and S-band) to in-band, because it was uncertain whether S-band frequencies were suitable for terrestrial broadcasting.In-band terrestrial systems merged DABserviceswith existing medium wave AM and VHF FM broadcasts, using novel adjacent- and co-channel modulating schemes. Between 1992 and 2000, competing system proponents demonstrated proprietarymethods of extracting acompatible digital RF signal from co-channel analog AM and FM broadcast transmissions.   Standardization activities for DABsystems werestarted in the early 1990s by the Consumer Electronics Group of the Electronic Industries Association (EIA), now known as the Consumer Electronics Association (CEA). In 1996, the EIA conducted ﬁeld-testing of some the systems in the San Francisco Bayarea, California. Shortly thereafter,standardization and ﬁeld-testing activities for IBOC AM and FM systems were startedbythe National Radio Systems Committee, ajoint committee of the National Association of Broadcasters (NAB) and the CEA. In October 2002, the FCC conditionallyapproved deployment of proprietaryanalog/digital in-band on-channel ( IBOC)AMand FM    technologydeveloped by system proponent iBiquityDigital Corporation (http:// www.ibiquity.com).Asofmid-2004, IBOC standards activities continue with apermanent Federal Communi- cations Commission (FCC) authorization of systems usage by all U.S. broadcastersanticipated in the near future.   In 1998, aworldwide consortium known as Digital Radio Mondiale (DRM) was formed, with the goal of producing near FM qualitydigital broadcasting in AM broadcast bands below 30 MHz (http//www.drm.org). Although there is some technical similaritytothe iBiquityAMIBOCsystem, the non-proprietaryDRM system primarily uses an all-digital transmission without amodulated co-channel analog AM component. In 2001, DRM received International Telecommunications Union (ITU) approval, and atechnical standardwas published. As of mid-2004, anumber of international broadcasters havebeen conducting over-air DRM system testing.   The inception of S-band satellite broadcastinginthe United States began in 1997, when the FCC auctioned two12.5 MHz-wide frequency bands to private operators. Commercial Satellite DABcommencedinlate 2001 and 2002, when systems licensees XM Satellite Radio and Sirius Satellite Radio began broadcasting to subscribers. Both systems offer numerouscommercial-free music channels and other content(http:// www.xmsatelliteradio.com and http://www.sirius.com).  Technical Overview of DAB Regardless of the actual signal delivery system used, all DABsystems share acommon overall topology. Figure1.32 presents ablock diagram of atypical DABtransmission system.   To maintain the highest possible audio quality, program material must originate from digital sourcessuch as CD players and digital audio recorders, or digital audio feeds from network sources. Analog sources such as microphones are converted to adigital audio data stream using an analog-to-digital (A/D) converter,prior to switching or summation with the other digital sources.   The linear digital audio data stream from the studio is then applied to the input of a source encoder.The purpose of this device is to reduce the required bandwidth of the audio information, helping to produce a spectrally efﬁcient RF broadcast signal. Forexample, a16-bit linear digital audio sampled at 48 kHz (the standardprofessional rate) requires adata stream of 1.536 megabits/s to transmit astereoprogram in aserial format. This output represents abandwidth of approximately 1.5 MHz, which is much greater than that used by an equivalent analog audio modulating signal [Smyth, 1992]. Sourceencoders can reduce the data rate by factors of 8:1 or more, yielding amuch moreefﬁcient modulating signal.1 -46                                  Broadcasting and Optical Communication Technology      FIGURE 1.32 An example DABtransmission system. ( Source:Hammett &Edison, Inc., Consulting Engineers.)     Following the sourceencoder,the resulting serial digital signal is applied to the input of the channel encoder,adevice that modulates the transmitted RF wave with the reduced-rate audio information. Also auxiliaryserial data such as program information and/or receiver control functions can be input to the channel encoder (or combined within the sourceencoder) for simultaneous transmission.   The channel encoder uses sophisticated modulating techniques to accomplish the goals of interference cancellation and highspectral efﬁciency.Methods of interferencecancellation include expansion of time and frequency diversityofthe transmitted information, as well as the inclusion of error correction codes in the data stream. Time diversity involves transmitting the same information multiple times by using apre- determined time interval. Frequency diversity produced by systems such as spread-spectrum, multiple-carrier, or frequency-hopping provides the means to transmit identical data on several different frequencies within the bandwidth of the system. At the receiver,real-time mathematical processes are used to locate the required data on aknown frequency at aknown time. If the initial information is found to be unusable because of signal interference,the receiver simply uses the same data found on another frequency and/or at another time, producing seamless demodulation.   Spectral efﬁciency is afunction of the modulation system used. Among the modulation formats that have been evaluated for DABtransmission are QPSK, M-aryQAM, and MSK [Springer,1992]. Using these and other formats, digital transmission systems that use no more spectrum than their analog counterparts have been designed.   The RF output signal of the channel encoder is ampliﬁed to the appropriate power levelfor transmission. Because the carrier-to-noise (C/N) ratio of the modulated waveform is not generally as critical as that required for analog communications systems, relatively low transmission powercan often be used. Depending on the sophistication of the data recoverycircuits contained in the DABreceiver,the use of C/N ratios as low as 6dB are possible without causing degradation to the received signal.   DABreception is largely the inverse of the transmission process, with the inclusion of sophisticated error correction circuits. Figure 1.33 shows atypical DABreceiver.   DABsignal reception is similar to the process used by virtually all radio receivers. Areceiving antenna feeds an appropriate stage of RF selectivityand ampliﬁcation, producing asample of the coded DABsignal. This signal drives achannel decoder,which reconstructs the audio and auxiliarydata streams. To accomplish this task, the channel decodermust demodulate and de-interleavethe data contained on the RF carrier and then apply appropriate computational and statistical error correction functions.   The source decoderconvertsthe reduced bit-rate audio stream back to pseudolinear at the original sampling rate. The decoder computationally expands the mathematically reduced data and ﬁlls the gaps left from the extraction of irrelevant audio information with averagedcode or other masking data. The output of the source decoderfeeds audio digital-to-analog (D/A) converters, and the resulting analog stereoaudio signal is ampliﬁed for the listener.Broadcasting                                                                       1 -47          FIGURE 1.33 An example DABreceiver.(Source:Hammett &Edison, Inc., Consulting Engineers.)    In addition to audio extraction, DABreceivers are generally capable of decodingauxiliarydata. This data can be used in conjunction with the user interface to control receiver functions, or for acompletely separate purpose. Atypical user interface contains adata displayscreen in addition to the usual receiver tuning and audio controls. This data screen can be used to obtain information about the programming,news reports, sports scores, advertising,orany other useful data sent by the station or an originating network. Also,external interfaces could be used to provide asoftware link to personal computer systems, or other digital information, and productivitydevices.  Audio Compression and Source Encoding The development of digital audio encoding started with research into pulse-code modulation (PCM) in the late 1930s and then evolved to include work on the principles of digital PCM coding.Linear predictivecoding (LPC) and adaptivedelta pulse-code modulation (ADPCM) algorithms had evolved in the early 1970s and later were adopted into standardssuch as C.721 (published by the CCITT) and CD-I (Compact Disc- Interactive). At the same time, algorithms werebeing invented for use with phoneme-based speech coding. Phonetic coding, aﬁrst-generation model-based speech-coding algorithm, was mainly implemented for low bit-rate speech and text-to-speech applications. These classes of algorithms for speech further evolved to include both CELP (Code Excited Linear Predictive)and VSELP (Vector Selectable Excited Linear Predictive) algorithms by the mid-1980s. In the late 1980s, these classes of algorithms were also shown to be useful for high-qualityaudio music coding. They were used commercially from the late 1970s to the latter part of the 1980s.   Sub-band coders evolved from the early work on quadraturemirror ﬁlters in the mid-1970s and continued with polyphase ﬁlter-based schemes in the mid-1980s. Hybrid algorithms employing both subband and ADPCM  coding were developed in the latter partofthe 1970s and standardized (e.g., CCITTG.722) in the mid- to late 1980s. Adaptive transform coders for audio evolved in the mid-1980s from speech coding work done in the late 1970s. By employing psychoacoustic noise-masking properties of the human ear,perceptual encoding evolved from early work of the 1970s and wherehigh-qualityspeech coders wereemployed.Music qualitybit-rate reduction schemes such as MPEG (Motion Picture ExpertGroup),PASC (Precision Adaptive Subband Coding), and ATRAC(AdaptiveTRansform  Acoustic Coding) havebeen developed. Further reﬁnements to the technologywill focus attention on novel approachessuch as wavelet-based coding and the use of entropy coding schemes.   Audio coding for digital broadcasting in the early daysemployed one of the manyperceptual encoding schemes previously mentioned or some variation thereof. Fundamentally,they depend on two basic psychoacoustic phenomena: (1) the threshold of human hearing and (2) masking of nearby frequency components. In the early days of hearing research, Harvey Fletcher,aresearcher at Bell Laboratories, measured1 -48                                  Broadcasting and Optical Communication Technology  the hearing of manyhuman beings and published the well-known Fletcher–Munson threshold-of-hearing chart. Basically,itstates that depending on the frequency,audio sounds belowcertain levelscannot be heardby the human ear.Also,the masking effect occurs when twofrequencies are very close to each other;when one is higher in acoustic level than the other,the weaker of the two is masked and will not be heard. These two principles allowfor as much as 80% of the data representing amusical signal to be discarded.   Figure1.34 shows howthe introduction of frequencycomponentsaffects the ear’sthreshold of hearing versus frequency.Figure1.35 shows how the revised envelope of audibilityresults in the elimination of components that would not be heard.   The electronic implementation of these algorithms employs adigital ﬁlter that breaks the audio spectrum into manysubbands, and various coefﬁcient elements are built into the program to decide when it is permissible to removeone or moreofthe signal components. The details of how the bands are divided and coefﬁcients are determined are usually proprietarytothe individual system developers. Standardization groups and system proponents havespent manyhours of evaluation, attempting to determine the most accurate coding system for given compression ratios or data transfer rates.   During recent years, the algorithms used for encoding music signals havesigniﬁcantly improved, even thoughthe fundamental methods remain as outlined above. Experienceinthe artofproducing both hardware and softwareencoders and the frequent use of listening test panels has brought about great improvements. Tweaking of the overall technology has improved sound qualitywhile keeping the compression and transfer rates as needed for the broadcast service to which it is applied. This optimization has beneﬁted the prospects for highqualitysound in digital transmission and broadcasting systems.   FIGURE 1.34 An example of the masking effect. Based on the hearing thresholdofthe human ear (dashed line), a500-Hz sinusoidal acoustic waveform, shown on the left graph, is easily audible at relatively lowlevels. However,itcan be masked by adding nearby higher-amplitude components, as shown on the right. ( Source:Almon H. Clegg.)   FIGURE 1.35 Sourceencoders use an empirically derived masking thresholdtodetermine which audio components can be discarded (left). As shown on the right, only the audio components with amplitudes abovethe masking thresholdare retained. ( Source:Almon H. Clegg.)Broadcasting                                                                       1 -49    Some examples of coding schemes considered for use in broadcasting channels are:   The Perceptual Audio Coder (PAC)extends the idea of perceptual coding to stereo pairs. It uses both L/R (left/right) and M/S (sum/differencematrixing to abate noise generation) coding, switched in both frequency and time in asignal dependent fashion.   The AdaptiveTransform Acoustic Coder (ATRAC) has been developed for portable digital audio,but because of its robust coding scheme and availabilityofcommercial integrated circuits, it also lends itself to broadcast use. This coder uses hybrid frequency mapping employing asignal splitting scheme that breaks the band into three sub-bands (0–5.5, 5.5–11, and 11–22 kHz) followed by asuitable dynamically windowed modiﬁed discrete cosinetransform (MDCT).   Advanced Audio Coding (AAC) is ahigh-qualityperceptual audio coding technologyappropriate for many broadcast and electronic music-distribution applications. Coding efﬁciency is superior to that of MP3, providing higher qualityaudio at lower transmitted bit rates. Developed and standardized as an ISO/IEC speciﬁcation by industryleaders, AACissupported by agrowing number of hardware and software manufacturers as the logical successor to MP3 for consumers, computer products, and broadcasting (www.aac-audio.com, http://www.iis.fraunhofer.de/amm/techinf/aac/, and http://www.mp3prozone.com).   Broadcast studios must havethe necessarytransforms available in their facilities to convertthe manydigital formats to their broadcast algorithm of choice. Hence, within the broadcast community, expertknowledge of all types and generations of audio compression formats is necessary. If an audio signal passes through several successive compression or bit reduction algorithms, there may be unexpected audible artifacts produced. It is the responsibilityofthe broadcast engineers to learn the compatibilityissues and adjust the signal processing methods to makethe necessaryconversions with minimum degradation of audio quality. Consideration must be given to the format of the sourceaudio program contentand its legacy and compatibilitywith the coder being used for the ﬁnal transmission or download. In the case of Internet streaming or downloading,the source coding used maysound best when listened to on aparticular software decoder.Hence, there are many choicesinthe marketplace, each claiming certain improvements over its competitors.   An interesting modern source coding example is CT-aacPlus audio compression. CT-aacPlus is the combination of AAC, ahighly efﬁcient global standardbased on the collaborative work of several experts on perceptual audio encoding, and additional coding from Coding Technologies’Spectral Band Replication (SBRTM)technology. SBR creates additional bitrate efﬁciency and utilizes subjective improvements workedout by the German Fraunhofer Institute, the inventor of MP3 (http://www.codingtechnolgies.comand http:// www.xmradio.com). With this combination of AACand SBR, CT-aacPlus has been tested by audio experts and found to provide certain audio improvements. In adouble-blind listening test, AACalone was 33% more efﬁcient when compared to previous generations of competing algorithms. Double-blind listening tests have established that the CT-aacPlus combination is at least 30% moreefﬁcient than AAC. As aresult of these tests, CT-aacPlus has been adopted by the DRM consortium and accepted by the MPEG standardization group as the referencemodel for the upcoming versions of MPEG-4 (a standardfor anew form of audio and video compression) (http://www.ebu.ch/trev_291-dietz.pdf).  System  Example: Eureka-147/DAB Eureka-147/DAB was the ﬁrst fully developed DABsystem that has demonstrated acapabilitytomeet virtually all the described system goals. Developed by aEuropean consortium, it is an out-band system in that its design is based on the use of afrequency spectrum outside the AM and FM radio broadcast bands. An out-band operation is required because the system packs up to 16 stereophonic broadcast channels (plus auxiliarydata) into one contiguous band of frequenciesthat can occupy atotal bandwidth of several megahertz. Thus, overall efﬁciency is maintained with 16 digital program channels occupying about the same total bandwidth as 16 equivalent analog FM broadcast channels. System developers have promoted Eureka-147/DABfor satellite transmission, as well as for terrestrial applications in locations that haveasuitable block of unused spectrum in the L-band frequency range or below.   The ISO/MPEG-2 sourceencoding/decoding system has been deﬁned for use with the Eureka-147/DAB system. Originally developed by IRT(Institut fu¨ rRundfunktechnik) in GermanyasMUSICAM (Masking1 -50                                  Broadcasting and Optical Communication Technology  pattern-adapted Universal SubbandIntegrated Coding And Multiplexing), the system works by dividing the original digital audio sourceinto 32 subbands. As with the sourceencoders described earlier,each of the bands is digitally processed to removeredundant information and sounds not perceptibletothe human ear.Using this technique, the original audio is sampled at arate of 768 kilobits/s per channel and is reduced to as little as 96 kilobits/s per channel, representing acompression ratio of 8:1.   The Eureka-147/DAB channel encoder operates by combining the transmitted program channels into a large number of adjacent narrowband RF carriers: each are modulated using QPSK and grouped to maximize the spectrum efﬁciency known as orthogonal frequency-division multiplex (OFDM). The information to be transmitted is distributed among the RF carriers and is also time-interleaved to reduce the effects of selective fading.Aguard interval is inserted between blocks of transmitted data to improvesystem resistance to intersymbol interference caused by multipath propagation. Convolutional coding is used in conjunction with aViterbi maximum-likelihood decoding algorithm at the receiver to make constructive use of echoed signals and correct random errors (Alard and Lassalle, 1988).   RF powerlevels of just afew tens of watts per program channel have been used to provide arelatively wide coverage area, depending on the height of the transmitting antenna abovesurrounding terrain. This low power levelispossible because the system can operate at aC/N ratio of less than 10 dB,incontrast to the more than 30 dB that is required for high-ﬁdelity demodulation of analog FM broadcasts.   Another demonstrated capabilityofthe system is its abilitytouse gap ﬁller transmitters to augment signal coverage in shadowed areas. Agap ﬁller is simply asystem that directly receives the DABsignal at an unobstructed location, provides RF ampliﬁcation, and retransmits the signal on the same channel into the shadowed area. Because the system can makeconstructive use of signal reﬂections (within atime window deﬁned by the guard interval and other factors), the demodulated signal is uninterrupted on amobile receiver when it travels between an area servedbythe main signal into the service area of the gap ﬁller.  System  Example:   iBiquity FM   IBOC As disclosed in recent public FCC proceedings, the iBiquityFMIBOCDAB system provides anovel proprietarymethod for the transition from analog FM broadcasting to ahybrid digital/analog technologyand ﬁnally to an all-digital system, using existing broadcast spectrum in the 88–108 MHz FM broadcast band. Source coding uses aproprietarymethod known as High Deﬁnition Coder (HDC), which likeCT-aacPlus has its roots in AAC. The present implementation operates at 96 kilobits/s for both stereochannels, representing a compression ratio of 16:1. This ratio is larger if the stream is split to allow both audio and data transmission, or transmission of twoormoreseparate but simultaneously delivered audio programs.   As shown in Figure1.36, groupings of primarylow levelOFDM carriers are added to the FM analog signal, both belowand above the host signal. Optional extended carriers also maybeadded, althoughtheir presence is not required for basic system operation. In addition to the FM analog host carrier frequency,reference subcarriers (used by the receiver/decoder for synchronization) are included with the IBOC carriers. As illustrated, the injection levelofthe individual carriers is quite low and is easily contained within the spectral emission mask speciﬁed by the U.S. FCC Rules.   Because the ultimate system goal is for atransition to an all-digital broadcast system, the methodologyand spectrum loading has been predeﬁned to be compatible with receivers marketed to operate using the hybrid system. Figure 1.37 showsthe spectral content of the deﬁned all-digital system. Note,the powerofthe primary carrier groupings, previously adjacent to the analog FM signal, have been increased,and groupings of secondarycarriers havebeen added in the location of the former FM analog signal spectrum. The higher level OFDM  carriers are maintained in an approximate 100- to 200-kHz offset from the carrier frequency so that interference to or from co-channel and adjacent-channel stations is not degraded subsequent to the all-digital broadcasting transition.   Forthe hybrid (analog compatible) system, the IBOC DABsignal can be combined with the analog FM signal by wayofanumber  of potential methods. Three of the more common combining methods are illustrated in the following block diagrams. In all cases, the FM transmitter analog audio is delayed by several seconds to achieve time alignment with the recovered digital audio signal in the receiver.Thus, the system canBroadcasting                                                                                    1 -51   FIGURE 1.36  Spectral content of the iBiquityhybrid FM analog/IBOC DABsystem (dBc ¼ decibels relative to carrier level). ( Source:Hammett &Edison, Inc., Consulting Engineers.)   FIGURE 1.37  Spectral content of the iBiquityall-digital IBOC DABsystem intended for use in the present FM broadcast band allocation. The injection level of the secondarycarriers is variable, depending on carrier loading.(Source:Hammett & Edison, Inc., Consulting Engineers.)  automatically and seamlessly return to analog audio demodulation in locations wheredigital demodulation becomes impaired.   Figure1.38 depicts the high-level combining method by which full-power analog and digital transmission systems operate in parallel; the twosignals are combined prior to the input of the transmitting antenna system. This method has the beneﬁt of maintaining highlinearityofthe IBOC    DABsignal, since it never passes1 -52                                  Broadcasting and Optical Communication Technology   FIGURE 1.38 High-level combining of IBOC DABand analog FM signals. ( Source:Hammett &Edison, Inc., Consulting Engineers.)   FIGURE 1.39 Low-level combining of IBOC DABand analog FM signals. ( Source:Hammett &Edison, Inc., Consulting Engineers.)  through the same active stages as the analog FM signal. However,the combiner must be carefully designed to minimize losses, particularly to the analog FM signal, which for manyFMstations operates at power levels of 10 kilowattsormore. In ordertoincrease the analog combining efﬁciency,the digital combining efﬁciency must be decreased. This requiresasigniﬁcantly higher DABpower than is actually transmitted, often on the order of 10 dB higher.For example, to achieve100 watts of DABpower at the input to the antenna system, 1,000 watts of DABpowermust be generated. This method is most popular withmoderate powerFMstations, sincethe amount of DABpowerwasted in the combining process remains reasonable.   One alternative to high-level combining is low-level combining,shown in Figure 1.39. This method combines the low poweroutput signals of the analog FM and IBOC exciter units and applies the composite signal to the transmitter.Low levelcombining requires exceptional linearityinthe transmitter RF ampliﬁer stages to prevent distortion of the IBOC carriers. It is generally suitable for transmitters operating at an FM analog output powerof10kilowattsorless.Broadcasting                                                                        1 -53   FIGURE 1.40 Combining of IBOC DABand analog FM signals using separate transmitting antennas. ( Source:Hammett &Edison, Inc., Consulting Engineers.)    Athird method of combining analog FM and IBOC carriers is to employ separate transmitting antennas as depicted in Figure1.40. Field testing has shown this method to be acceptable provided the transmitting antennas havesimilar radiation patterns, are located close to each other,and are installed at comparable radiation center heights. This method is of signiﬁcant interest to stations that operate with very highanalog FM transmitter powerof30kilowattsormore, where the highlevelcombining method is impractical due to efﬁciency issues, and low level combining is not possible due to the prohibitive cost of producing ahighpower linear transmitter to operate at VHF frequencies. The use of separate antennas is also of interest to manyother FM  broadcasters, sincethe method allows the existing analog FM facilitytobeleft in place.  System   Example: iBiquity AM     IBOC As with the FM IBOCsystem, the proprietaryiBiquityAMIBOCtransmission system is capable of operating in two modes, hybrid and all-digital, with only the hybrid mode exhibiting interoperability with existing analog broadcasting.Aswith the FM IBOC system, sourcecoding employsHDC and operates at 36 kilobits/s for both stereo channels, representing acompression ratio of almost 43:1. A400 bit/s data stream is reserved for receiver program identiﬁcation, but the data path can be expanded further at the expense of using ahigher audio compression ratio.According to the developers, HDC is capable of producing acceptable audio quality at bit rates as low as 20 kilobits/s.   Figure1.41 provides an illustration of the hybrid AM IBOCtransmission system spectrum occupancy. Similar to the related FM IBOC system, transmissions are carried by aseries of OFDM carriers and separated into primary, secondary, and tertiarygroupings. The primarydigital carrier groupings are offset ^ 10 to ^ 15 kHz from the carrier frequency,while the secondarydigital carrier groupings are offset ^ 5to ^ 10 kHz of the carrier frequency.The tertiarycarriers are located within ^ 5kHz of the carrier frequency,sharing same spectrum as the AM analog host sidebands. As shown by the dashed line, this arrangement of digital carriers ﬁts within the U.S. FCC Rules spectrum mask.1 -54                                  Broadcasting and Optical Communication Technology   FIGURE 1.41 Spectral content of the iBiquityhybrid AM analog/IBOC DABsystem. ( Source:Hammett &Edison, Inc., Consulting Engineers.)     In general, the system requires the analog audio bandwidth be limited to 5kHz, to protect the secondary digital carriers. An alternate mode of the system allows up to 7.5 kHz analog audio bandwidth, which generates some interference to the secondarydigital carriers. Because the digital sidebands carryduplicative information that is inverted about the carrier frequency,recoveryofthe lower portion of the lower secondarydigital subcarriers, along with the upper portion of the upper secondarydigital subcarriers, still results in full recoveryofthe secondarydigital information. The drawback is that recoveryofboth secondarysidebands is required for use of 7.5 kHz analog audio bandwidth, while recoveryofjust one secondarysideband is required for use of 5kHz analog audio bandwidth. Thus, IBOC digital service area for 7.5 kHz analog audio bandwidth maybeimpaired in the presence of adjacent-channel interfering signals.   Figure1.42 provides arepresentation of the spectrum occupancy for the deﬁned all-digital IBOC AM system. As with the FM IBOC system, an anticipated transition to this system would take placeafter most stations haveimplemented the hybrid IBOC system, and when market indicators haveshown IBOC receiver penetration exceeding adeﬁned threshold.   Note, the unmodulated carrier signal is retained in the all-digital system, but the primarydigital carrier groupings nowimmediately surround the carrier,replacing the hybrid system tertiarydigital carriers. Deﬁned secondaryand tertiarycarriers occupy the former positions of the hybrid system secondarydigital carrier groupings, and no information is transmitted beyond ^ 10 kHz of the center carrier frequency.Asaresult of this design, the anticipated migration to an all-digital transmission should enhancestation digital coverage over the hybrid system and show improved performanceontransmitting antenna systems exhibiting narrow bandwidth.   Figure1.43 provides ablock diagram of atypical AM IBOC installation. The conversion of an analog station requires the addition of an exciter,anauxiliaryservice unit (ASU), and appropriate audio processing.   The AuxiliaryService Unit (ASU) accepts AES/EBU digital audio as input, handles sampling rate conversions, and feeds the digital audio inputs of separate AM and IBOC audio processors. The ASU also contains aGPS-synchronized clock to generate the carrier reference and other synchronizing signals used by the exciter.The DABaudio processor controls and levels audio of the IBOC digital transmission, while the analog audio processor controls analog audio.The exciter contains adelaycircuit for the analogBroadcasting                                                                        1 -55   FIGURE 1.42 Spectral content of the iBiquityall-digital IBOC DABsystem intended for use in the present medium wave AM  broadcast band allocation. ( Source:Hammett &Edison, Inc., Consulting Engineers.)   FIGURE 1.43 Combining of IBOC DABand analog AM signals. ( Source:Hammett &Edison, Inc., Consulting Engineers.)   audio path so that digital-to-analog transitions in receivers are time-aligned, similar to the FM IBOC system.   The exciter DABpath encodes the source audio,introduces errorcorrection and interleaving overhead, and generates the coded OFDM carriers. The DABand analog paths are then summed and converted to magnitude/phase signals that feed the broadcast transmitter.The exciter output signals feed the transmitter analog audio input and frequency-determining stages of the transmitter.The hardwaretopologyofanall- digital conversion is the same as that shown, except the analog audio processor and related exciter circuitry are not used.1 -56                                  Broadcasting and Optical Communication Technology  Deﬁning   Terms Channel encoder:  Adevice that converts source-encoded digital information into an analog RF signal for      transmission. The type of modulation used depends on the particular digital audio broadcasting (DAB)      system, althoughmost modulation techniques employmethods by which the transmitted signal can be      made moreresistant to frequency-selective signal fading and multipath distortion effects. In-Band On-Channel  (IBOC):  Amethod  of combining digital audio broadcasting signals, intended for      reception by radio listeners, with the signals of existing analog broadcast servicesoperating in the      medium wave AM and VHF  FM broadcast bands. Gap ﬁller: Alow-power transmitter that boosts the strength of transmitted DABRFsignals in areas that nor-      mally would be shadoweddue to terrain obstruction. Gap ﬁllers can operate on the same frequency as DAB      transmissions, or on alternate channels that can be located by DABreceivers using automatic switching. Source encoder:  Adevice that substantially reduces the data rate of linearly digitized audio signals by      taking advantage of the psychoacoustic properties of human hearing,eliminating redundant and      subjectively irrelevant information from the output signal. Transform sourceencoders work entirely      within the frequency domain, while time-domain source encoders work primarily in the time domain.      Sourcedecoders reverse the process, using various masking techniques to simulate the properties of the      original linear data.  References M. Alardand R. Lassalle, ‘‘Principles of modulation and channel coding for digital broadcasting for mobile      receivers,’’ Advanced Digital Techniques for UHF Satellite Sound Broadcasting,European Broadcasting      Union (collected papers), 1988, pp.47–69. R. Bruno,‘‘Digital audio and video compression, present and future,’’presented to the Delphi Club,Tokyo,      Japan, July,1992. G. Chouinardand F. Conway, ‘‘Broadcasting systems concepts for digital sound,’’ Proc. 45th Annu. Broadcast      Eng. Conf.,National Association of Broadcasters, 1991, pp.257–266. F. Conway, R. Voyer, S. Edwards, and D. Tyrie, ‘‘Initial experimentation with DABinCanada,’’ Proc. 45th Annu.      Broadcast Eng. Conf.,National Association of Broadcasters, 1991, pp.281–290. S. Kuhand J. Wang,‘‘Communications systems engineering for digital audio broadcast,’’ Proc. 45th Annu.      Broadcast Eng. Conf.,National Association of Broadcasters, 1991, pp.267–272. P.H. Moose and J.M. Wozencraft, ‘‘Modulation and coding for DABusing multi-frequency modulation,’’ Proc.      45th Annu. Broadcast Eng. Conf.,National Association of Broadcasters, 1991, pp.405–410. S. Smyth, ‘‘Digital audio data compression,’’ Broadcast Eng. Mag.,1992, pp.52–60. K.D.Springer,Interference Between FM and Digital M-PSK Signals in the FM Band, National Association of      Broadcasters, 1992. First Report and Order to Mass Media Docket No.99-325, ‘‘Digital Audio Broadcasting Systems and their      Impact on the Terrestrial Radio Broadcast Service’’,Federal Communications Commission, Washington,      D.C., 2002.  Further  Information The National Association of Broadcasters (http://www.nab.org) publishes periodic reports on the technical, regulatory, and political status of DABinthe United States. Additionally,their Broadcast Engineering Con- ference proceedings published since 1990 contain asubstantial amount of information on emerging DAB technologies.   IEEE Transactions on Broadcasting is typically published quarterly by the Institute of Electrical and Electronics Engineers, Inc. and periodically includes papers on digital broadcasting (http://www.ieee.org).   Additionally,the periodic newspaper publication Radio World (http://www.rwonline.com)provides continuous coverage of DABtechnology, including proponent announcements, system descriptions, ﬁeld test reports, and broadcast industryreactions.                                                                                2                                                           Equalization                                 2.1 Linear Transversal Equalizers .................................................. 2 -1                                     Automatic Synthesis * AdaptiveEqualization                               2.2 Nonlinear Equalizers ............................................................. 2 -3 Richard C. Dorf                   Decision-Feedback Equalizers * Fractionally Spaced Equalizers University of California      2.3 Linear Receivers ................................................................... 2 -5                                   Matched Filter Zhen  Wan                     2.4 Nonlinear Receivers .............................................................. 2 -5  University of Texas               Decision-Feedback Equalizers * AdaptiveFilters for MLSE   In bandwidth-efﬁcient digital communication systems the effect of each symbol transmitted over atime dispersivechannel extends beyond the time interval used to represent that symbol. The distortion caused by the resulting overlap of received symbols is called intersymbol interference (ISI) [Lucky et al., 1968]. ISI arises in all pulse-modulation systems, including frequency-shift keying (FSK), phase-shift keying (PSK), and quadratureamplitude modulation (QAM) [Lucky et al., 1968]. However,its effect can be most easily described for abaseband PAMsystem.   The purpose of an equalizer, placed in the path of the received signal, is to reduce the ISI as much as possible to maximize the probabilityofcorrect decisions.  2.1    Linear   Transversal Equalizers  Among the manystructures used for equalization, the simplest is the transversal (tapped delay line or non- recursive) equalizer shown in Figure2.1. In such an equalizer the current and past values r ( t – nT)ofthe  received signal are linearly weighted by equalizer coefﬁcients (tap gains) c n and summed to producethe output. In the commonly used digital implementation, samples of the received signal at the symbol rate are  stored in adigital shift register (or memory), and the equalizer output samples (sums of products) z ( t 0 þ kT) or z k are computed digitally,once per symbol, according to                                      N X   1                                 z k ¼   c n r ð t 0 þ kT   ntÞ                                      n ¼ 0  where N is the number of equalizer coefﬁcients and t 0 denotes sample timing.   The equalizer coefﬁcients, c n , n ¼ 0, 1,... , N –1,may be chosen to forcethe samples of the combined channel and equalizer impulse response to zeroatall but one of the NT-spaced instants in the span of the equalizer.Such an equalizer is called a zero-forcing (ZF) equalizer [Lucky,1965].   If we let the number of coefﬁcients of aZFequalizer increase without bound, we would obtain an inﬁnite- length equalizer with zeroISI at its output. An inﬁnite-length zero-ISI equalizer is simply an inverse ﬁlter, which invertsthe folded frequency response of the channel. Clearly,the ZF criterion neglects the effect of noise altogether.Aﬁnite-length ZF equalizer is approximately inverse to the folded frequency response of                                                                                      2 -12 -2                                   Broadcasting and Optical Communication Technology   FIGURE 2.1 Linear transversal equalizer.(Source: K. Feher, Advanced Digital Communications,Englewood Cliffs, N.J.: Prentice-Hall, 1987, p. 648. With permission.)  the channel. Also,aﬁnite-length ZF equalizer is guaranteed to minimize the peak distortion or worst-case ISI only if the peak distortion beforeequalization is less than 100% [Lucky,1965].   The least-mean-squared (LMS) equalizer [Lucky et al., 1968] is morerobust. Here the equalizer coefﬁcients are chosen to minimize the mean squarederror(MSE)—the sum of squares of all the ISI terms plus the noise poweratthe output of the equalizer.Therefore, the LMS equalizer maximizes the signal-to-distortion ratio (S/D) at its output within the constraints of the equalizer time span and the delaythrough the equalizer.  Automatic   Synthesis Beforeregular data transmission begins, automatic synthesis of the ZF or LMS equalizers for unknown channels maybecarried out during atraining period. During the training period, aknown signal is transmitted and asynchronized version of this signal is generated in the receiver to acquireinformation about the channel characteristics. The automatic adaptiveequalizer is shown in Figure2.2. Anoisy but unbiased estimate:                                 d e 2 k                                      ¼ 2 e k r ð t 0 þ kT   nTÞ                                d c n ð k Þ  is used. Thus, the tap gains are updated according to                 c n ð k þ 1 Þ¼c n ð k Þ D e k r ð t 0 þ kT   nTÞ ; n ¼ 0 ; 1 ; ... ; N   1  where c n ( k )isthe n th tap gain at time k , e k is the error signal, and D is apositive adaptation constant or step size, error signals e k ¼ z k – q k can be computed at the equalizer output and used to adjust the equalizer coefﬁcients to reducethe sum of the squarederrors. Note q k ¼ xx^ k .   FIGURE 2.2 Automatic adaptive equalizer.(Source: K. Feher, Advanced Digital Communications,Englewood Cliffs, N.J.: Prentice-Hall, 1987, p. 651. With permission.)Equalization                                                                        2 -3    The most popular equalizer adjustment method involves updates to each tap gain during each symbol interval. The adjustment to each tap gain is in adirection opposite to an estimate of the gradient of the MSE with respect to that tap gain. The idea is to move the set of equalizer coefﬁcients closer to the unique optimum set corresponding to the minimum MSE. This symbol-by-symbol proceduredeveloped by Widrowand Hoff [Feher,1987] is commonly referred to as the stochastic gradient method.  Adaptive Equalization After the initial training period (if thereisone), the coefﬁcients of an adaptiveequalizer maybecontinually  adjusted in a decision-directed manner.Inthis mode the error signal e k ¼ z k – q k is derived from the ﬁnal (not necessarily correct) receiver estimate { q k }ofthe transmitted sequence { x k }where q k is the estimate of x k . In normal operation the receiver decisions are correct with highprobability,sothat the error estimates are correct often enoughtoallow the adaptive equalizer to maintain precise equalization. Moreover,adecision- directed adaptiveequalizer can track slow variations in the channel characteristics or linear perturbations in the receiver front end, such as slow jitter in the sampler phase.  2.2    Nonlinear Equalizers  Decision-Feedback Equalizers Adecision-feedback equalizer (DFE) is asimple nonlinear equalizer [Monsen, 1971], which is particularly useful for channels with severeamplitude distortion and uses decision feedback to cancel the interference from symbols which havealready been detected. Figure2.3 shows the diagram of the equalizer.   The equalized signal is the sum of the outputs of the forward and feedback parts of the equalizer.The forward partislike the linear transversal equalizer discussed earlier.Decisions made on the equalized signal are fed back viaasecond transversal ﬁlter.The basic idea is that if the values of the symbols already detected are   FIGURE 2.3 Decision-feedback equalizer.(Source: K. Feher, Advanced Digital Communications, Englewood Cliffs, N.J.: Prentice-Hall, 1987, p. 655. With permission.)2 -4                                   Broadcasting and Optical Communication Technology  known (past decisions are assumed to be correct), then the ISI contributed by these symbols can be canceled exactly,bysubtracting past symbol values with appropriate weighting fromthe equalizer output.   The forward and feedback coefﬁcients may be adjusted simultaneously to minimize the MSE. The update equation for the forward coefﬁcients is the same as for the linear equalizer.The feedback coefﬁcients are adjusted according to                        b m ð k þ 1 Þ¼b m ð k ÞþD e k xx^ k   m m ¼ 1 ; ... ; M  where xx^ k is the k th symbol decision, b m ( k )isthe m th feedback coefﬁcient at time k ,and thereare M feedback coefﬁcients in all. The optimum LMS settings of b m , m ¼ 1, ... , M ,are those that reduce the ISI to zero,within the span of the feedback part, in amanner similar to aZFequalizer.  Fractionally Spaced  Equalizers The optimum receive ﬁlter in alinear modulation system is the cascade of aﬁlter matched to the actual channel, with atransversal T -spaced equalizer [Forney,1972]. The fractionally spaced equalizer (FSE), by virtue of its sampling rate, can synthesize the best combination of the characteristics of an adaptivematched ﬁlter and a T -spaced equalizer,within the constraints of its length and delay. A T -spaced equalizer,with symbol-rate sampling at its input, cannot perform matched ﬁltering.Afractionally spaced equalizer can effectively compensate for moresevere delaydistortion and deal with amplitude distortion with less noise enhancement than a T -equalizer.   Afractionally spaced transversal equalizer [Monsen, 1971] is shown in Figure 2.4. The delay-line taps of such an equalizer are spaced at an interval t ,which is less than, or afraction of, the symbol interval T .The tap spacing t is typically selected such that the bandwidth occupied by the signal at the equalizer input is j f j 5 1 = 2 t :that is, t -spaced sampling satisﬁes the sampling theorem. In an analog implementation, there is no other restriction on t ,and the output of the equalizer can be sampled at the symbol rate. In adigital implementation t must be KT/ M ,where K and M are integers and M . K .(In practice, it is convenient to choose t ¼ T / M ,where M is asmall integer,e.g., 2.) The received signal is sampled and shifted into the equalizer delayline at arate M / T ,and one input is producedeach symbol interval (for every M input sample). In general, the equalizer output is given by                                                                                N X   1          nKT                               z k ¼   c n rt0 þ kT                                      n ¼ 0             M   FIGURE 2.4 Fractionally spaced equalizer.(Source: K. Feher, Advanced Digital Communications,EnglewoodCliffs, N.J.: Prentice-Hall, p. 656. With permission.)Equalization                                                                        2 -5  The coefﬁcients of a KT/ M equalizer maybeupdated once per symbol based on the error computed for that symbol accordingto                                                                                       nKT               c ð k þ 1 Þ¼c ð k Þ D e rtþ kT        ;  n ¼  0 ; 1 ; ... ; N   1                n          n       k   0         M  2.3    Linear Receivers  When the channel does not introduce anyamplitude distortion, the linear receiver is optimum with respect to the ultimate criterion of minimum probability of symbol error.The conventional linear receiver consists of a matched ﬁlter,asymbol-rate sampler,aninﬁnite-length T -spaced equalizer,and amemoryless detector.The linear receiver structureisshown in Figure2.5.   In the conventional linear receiver,amemoryless threshold detector is sufﬁcient to minimize the probability of error; the equalizer response is designed to satisfy the zero-ISI constraint, and the matched ﬁlter is designed to minimize the effect of the noise while maximizing the signal.  Matched Filter                                                      2    2 The matched ﬁlter is the linear ﬁlter that maximizes ð S = N Þ out ¼ s 0 ð t Þ = n 0 ð t Þ of Figure 2.6 and has atransfer function given by                                             S * ð f Þ                                   H ð f Þ¼K      e   j o t 0                                            P n ð f Þ  where Sf ¼ F [ s ( t )] is the Fourier transform of the known input signal s ( t )ofduration T sec. P n ( f )isthe PSD of the input noise, t 0 is the sampling time when ( S / N ) out is evaluated, and K is an arbitraryreal nonzero constant.   Ageneral representation for amatched ﬁlter is illustrated in Figure2.6. The input signal is denoted by s ( t )  and the output signal by s 0 ( t ). Similar notation is used for the noise.  2.4    Nonlinear Receivers  When amplitude distortion is present in the channel, amemoryless detector operating on the output of this receiver ﬁlter no longer minimizes symbol error probability. Recognizing this fact, several authors have                               FIGURE 2.5 Conventional linear receiver.   FIGURE 2.6 Matched ﬁlter.(Source: L.W.Couch, Digital and Analog Communication Systems,New York: Macmillan, p. 497, 1990. With permission.)2 -6                                   Broadcasting and Optical Communication Technology  investigated optimum or approximately optimum nonlinear receiver structures subject to avarietyofcriteria [Lucky,1973].  Decision-Feedback    Equalizers ADFE  takes advantage of the symbols that havealready been detected (correctly with highprobability) to cancelthe ISI due to these symbols without noise enhancement. ADFE makes memoryless decisions and cancels all trailing ISI terms. Even when the whitened matched ﬁlter (WMF) is used as the receive ﬁlter for the DFE, the DFE suffers from areduced effective signal-to-noise ratio,and error propagation, due to its inability to defer decisions.   An inﬁnite-length DFE receiver takes the general form (shown in Figure 2.7) of aforward linear receive ﬁlter,symbol-rate sampler,canceler,and memoryless detector.The symbol-rate output of the detector is then used by the feedback ﬁlter to generate futureoutputs for cancellation.  Adaptive  Filters for MLSE Forunknown  and/or slowly time-varying channels, the receive ﬁlter must be adaptive in order to obtain the ultimate performancegain from MLSE (maximum-likelihood sequence estimation). Secondly,the complexityofthe MLSE  becomes prohibitive for practical channels with alarge number of ISI terms. Therefore, in apractical receiver,anadaptive receive ﬁlter may be used prior to Viterbi detection to limit the time spread of the channel as well as to track slow time variation in the channel characteristics [Falconer and Magee, 1973].   Several adaptivereceive ﬁlters are available that minimize the MSE at the input to the Viterbi algorithm. These methods differ in the form of constraint [Falconer and Magee, 1973] on the desiredimpulse response (DIR) which is necessaryinthis optimization process to exclude the selection of the null DIR corresponding to no transmission through the channel. The general form of such areceiver is shown in Figure2.8.   One such constraint is to restrict the DIR to be causal and to restrict the ﬁrst coefﬁcient of the DIR to be unity. In this case the delay(LT)inFigure 2.8 is equal to the delay through the Viterbi algorithm and the ﬁrst  coefﬁcient of { b k }isconstrained to be unity.   FIGURE 2.7 Conventional decision-feedback receiver.(Source: K. Feher, Advanced Digital Communications,Englewood Cliffs, N.J.: Prentice-Hall, 1987, p. 675. With permission.)Equalization                                                                        2 -7   FIGURE 2.8 General form of adaptive MLSE receiver with ﬁnite-length DIR. ( Source: K. Feher, Advanced Digital Communications,Englewood Cliffs, N.J.: Prentice-Hall, 1987, p. 684. With permission.)    The least restrictive constraint on the DIR is the unit energyconstraint proposed by Falconerand Magee [1973]. This leads to yet another form of the receiver structure as shown in Figure 2.8. However,the  adaptation algorithm for updating the DIR coefﬁcients { b k }isconsiderably morecomplicated [Falconerand Magee, 1973]. Note that the ﬁxed predetermined WMF and T -spaced preﬁlter combination of Falconer and Magee [1973] has been replaced in Figure 2.8 by ageneral fractionally spaced adaptiveﬁlter.  Deﬁning   Terms Equalizer: Aﬁlter used to reduce the effect of intersymbol interference. Intersymbol interference: The distortion caused by the overlap (in time) of adjacent symbols.  References L.W.Couch, Digital and Analog Communication Systems, NewYork: Macmillan, 1990. D.D. Falconer and F.R. Magee, Jr., ‘‘Adaptivechannel memorytruncation for maximum likelihood sequence      estimation,’’ Bell Syst. Technical Journal, vol. 5, pp.1541–1562, November 1973. K. Feher, Advanced Digital Communications, Englewood Cliffs, N.J.: Prentice-Hall, 1987. G.D.Forney,Jr.,‘‘Maximum-likelihood sequenceestimation of digital sequencesinthe presence of      intersymbol interference,’’ IEEE Trans. Information Theory, vol. IT-88, pp.363–378, May1972. R.W.Lucky,‘‘Automatic equalization for digital communication,’’ Bell Syst. Tech. Journal, vol.44, pp.547–588,      April 1965. R.W.Lucky,‘‘A survey of the communication theoryliterature: 1968–1973,’’ IEEE Trans. Information Theory,      vol.52, pp.1483–1519, November 1973. R.W.Lucky,J.Salz, and E.J.Weldon, Jr., Principles of Data Communication, NewYork: McGraw-Hill, 1968. P. Monsen, ‘‘Feedback equalization for fading dispersivechannels,’’ IEEE Trans. Information Theory, vol. IT-17,      pp.56–64, January1971.This page intentionally left blank                                                                                 3                               Optical Communication                                 3.1  Lightwave Technologyfor Video Transmission ........................ 3 -1                                     Video Formats and Applications * Compressed Digital Video *                                     Intensity Modulation * Noise Limitations * Linearity Requirements *                                     Laser Linearity * Clipping * External Modulation * Miscellaneous                                     Impairments * Summary                               3.2  Long Distance Fiber Optic Communications .......................... 3 -10                                     Fiber * Modulator * Light Source * SourceCoupler * Isolator * Thomas E. Darcie                                    Connectors and Splices * Optical Ampliﬁer * Regenerator * AT& TBell Laboratories                                    Photodetector * Other Components * System Considerations * Joseph C. Palais                   Error Rates and Signal-to-Noise Ratio * System Design Arizona State University      3.3  Photonic Networks ............................................................. 3 -18                                    Introduction * Background: LightwaveTransmission Links  Alan E. Willner                    Interconnecting Nodes * ArchitectureofPhotonic Networks *  University of Southern California  Add/Drop Multiplexer * Optical Crossconnects * PassiveOptical                                     Networks * Protocols of Photonic Networks * Circuit, Packet,  Reza Khosravani                    and Burst Switching * Enabling Switching Technologies *  Sonoma State University            Wavelength Conversion * Summary   3.1     Lightwave     Technology for     Video   Transmission Thomas E. Darcie  Lightwave technology has revolutionized the transmission of analog and, in particular,video information. Because the light output intensity from asemiconductor laser is linearly proportional to the injected current, and the current generated in aphotodetector is linearly proportional to the incident optical intensity,analog information is transmitted as modulation of the optical intensity. The lightwavesystem is analogous to a linear electrical link, where current or voltage translates linearly into optical intensity. High- speed semiconductor lasers and photodetectors enable intensity-modulation bandwidths greater than 10 GHz. Hence, awide varietyofradio frequency (RF) and microwave applications havebeen developed [Darcie, 1990].   Converting microwaves into intensity-modulated (IM) light allows the use of optical ﬁber for transmission in placeofbulky inﬂexible coaxial cable or microwave waveguide. Sincethe ﬁber attenuation is 0.2–0.4 dB/km, compared withseveral decibels per meter for waveguide, entirely new applications and architectures are possible. In addition, the signal is conﬁned tightly to the coreofsingle-mode ﬁber,whereitisimmune to electromagnetic interference,cross talk, or spectral regulatorycontrol.   To achieve these advantages, several limitations must be overcome. The conversion of current to light intensity must be linear.Several nonlinear mechanisms must be avoided by proper laser design or by the use of various linearization techniques. Also,because the photon energyismuch larger than in microwave systems, the signal ﬁdelityislimited by quantum or shot noise.                                                                                      3 -13 -2                                   Broadcasting and Optical Communication Technology    This section describes the basic technology for the transmission of various video formats. We begin by describing the most common video formats and deﬁning transmission requirements for each. Sources of noise, including shot noise, relativeintensity noise (RIN), and receiver noise are then quantiﬁed. Limitations imposed by sourcenonlinearity,for both direct modulation of the laser bias current and external modulation  using an interferometric LiNbO3 modulator,are compared. Finally,several other impairments caused by ﬁber nonlinearity or ﬁber dispersion are discussed.  Video  Formats  and  Applications Each video format represents acompromise between transmission bandwidth and robustness or immunityto impairment. With the exception of emerging digital formats, each is also an entrenched standardthat often reﬂects the inefﬁciencies of outdated technology.  FM  Video Frequency-modulated (FM) video has servedfor decades as the basis for satellite video transmission [Pratt and Bostian, 1986], wherehighsignal-to-noise ratios (SNRs) are difﬁcult to achieve. Video information with  abandwidth of B v ¼ 4.2 MHz is used to FM modulate an RF carrier.The resulting channel bandwidth B is given by                                       B , D f pp þ 2 f m                            ð 3 : 1 Þ   where D f pp is the frequency deviation (22.5 MHz) and f m is the audio subcarrier frequency (6.8 MHz). As a result of this bandwidth expansion to typically 36 MHz, ahighSNR can be obtained for the baseband video  bandwidth B v even if the received carrier-to-noise ratio (CNR) over the FM bandwidth B is small. The SNR is given by                                                                                             3 B  D f                       SNR  ¼ CNR  þ 10 log         pp  þ  W  þ PE                  ð 3 : 2 Þ                                             2 B v B v  where W is aweighting factor (13.8 dB) that accounts for the waythe eyeresponds to noise in the video bandwidth, and PE is apre-emphasis factor (0–5 dB) that is gained by emphasizing the high-frequency video components to improve the performanceofthe FM modulator.High-qualityvideo (SNR ¼ 55 dB) requires a CNR  of only 16 dB.This is achievedeasily in alightwavetransmission system.   Applications for lightwaveFMvideo transmission include links to satellite transmission facilities, transport of video between cable television companyhead-ends (super-trunking), and perhaps delivery of video to subscribers over large ﬁber distribution networks [Way et al., 1988; Olshansky et al., 1988].  AM-VSB   Video The video format of choice,both for broadcast and cable television distribution, is AM-VSB. Each channel consists of an RF carrier that is amplitude modulated (AM) by video information. Single-sideband vestigial (VSB) ﬁltering is used to minimize the bandwidth of the modulated spectrum. The resultant RF spectrum is dominated by the remaining RF carrier,which is reduced by typically 5.6 dB by the AM, and contains relatively low-level signal information, including audio and color subcarriers. An AM-VSB channel requires a bandwidth of only 6MHz, but CNRs must be at least 50 dB.   Forcable distribution, manychannels are frequency-division multiplexed (FDM), separated nominally by 6MHz  (8 MHz in Europe),overthe bandwidth supported by the coaxial cable. Atypical 60-channel cable system operates between 55.25 and 439.25 MHz. Given the large dynamic range required to transmit both the remaining RF carrier and the low-level sidebands, transmission of this multichannel spectrum is achallenge for lightwavetechnology.Optical Communication                                                               3 -3    The need for such systems in cable television distribution systems has motivated the development of suitable high-performancelasers. Beforethe availabilityoflightwaveAM-VSB systems, cable systems used long (up to 20 km) trunks of coaxial cable with dozens of cascaded electronicampliﬁers to overcome cable loss. Accumulations of distortion and noise, as well as inherent reliabilityproblems with long cascades, were serious limitations.   Fiber AM-VSB trunk systems can replacethe long coaxial trunks so that head-end qualityvideo can be delivered deep within the distribution network [Chiddix et al., 1990]. Inexpensive coaxial cable extends from the optical receivers at the ends of the ﬁber trunks to each home. Architectures in which the number of electronic ampliﬁers between each receiver and anyhome is approximately threeorfewer offer agood compromise between cost and performance. The shortspans of coaxial cable supportbandwidths approaching 1GHz, twoorthreetimes the bandwidth of the outdated long coaxial cable trunks. With fewer active components, reliabilityisimproved. The cost of the lightwavecomponentscan be small compared to the overall system cost. These compelling technical and economic advantages resulted in the immediate demand for lightwaveAM-VSB systems.  Compressed Digital    Video The next generation of video formats willbethe product of compressed digital video (CDV) technology [Netravali and Haskel, 1988]. Foryears digital ‘‘NTSC-like’’video requiredabit rate of approximately 100 Mbps. CDVtechnologycan reduce the required bit rate to less than 5Mbps. This compression requires complex digital signal processing and large-scale circuit integration, but advances in chip and microprocessor design havemade inexpensive implementation of the compression algorithms feasible.   Various levelsofcompression complexitycan be used, depending on the ultimate bit rate and quality required. Each degree of complexityremoves different types of redundancy from the video image. The image is brokeninto blocks of pixels, typically 8 · 8. By comparing differentblocks and transmitting only the differences(DPCM), factors of 2reduction in bit rate can be obtained. No degradation of qualityneed result. Much of the information within each block is imperceptible to the viewer.Vector quantization (VQ) or discrete-cosine transform (DCT)techniques can be used to eliminate bits corresponding to these imperceptible details. This intraframe coding can result in afactor of 20 reduction in the bit rate, although the evaluation of image qualitybecomes subjective.Finally,stationaryimages or moving objects need not require constant retransmission of everydetail. Motion compression techniques havebeen developed to eliminate these interframe redundancies. Combinations of these techniques haveresulted in coders that convert NTSC-like video (100 Mbps uncompressed) into afew megabits per second and HDTV images (1 Gbps uncompressed) into less than 20 Mbps.   CDVcan be transmitted using time-division multiplexing (TDM) and digital lightwavesystems or by using each channel to modulate an RF carrier and transmitting using analog lightwavesystems. There are numerous applications for both alternatives. TDM systems for CDV are no different from anyother digital transmission system and will not be discussed further.   Using RF techniques offers an additional levelofRFcompression, wherein advanced multilevel modulation formats are used to maximize the number of bits per hertz of bandwidth [Feher,1987]. Quadrature-amplitude modulation (QAM) is one example of multilevel digital-to-RF conversion. Forexample, 64-QAM uses 8amplitude and 8phase levels and requires only 1Hzfor 5bits of information. As the number of levels, hence the number of bits per hertz, increases, the CNR of the channel must increase to maintain error-free transmission. A64-QAM channel requires aCNR of approximately 30 dB.   Asynopsis of the bandwidth and CNR requirements for FM, AM-VSB,and CDVisshown in Figure3.1. AM-VSB requires highCNR but low bandwidth. FM is the opposite. Digital video can occupy awide area, depending on the degree of digital and RF compression. The combination of CDVand QAMoffers the possibilityofsqueezing ahigh-qualityvideo channel into 1MHz of bandwidth, with arequired CNR of 30 dB. This drastic improvement over AM-VSB or FM could havetremendousimpact on future video transmission systems.3 -4                                   Broadcasting and Optical Communication Technology  Intensity Modulation As mentioned in the introduction, the light output from the laser should be linearly proportional to the injected current. The laser is prebiased to an average output inten-  sity L 0 .Manyvideo channels are combined electronically, and the total RF signal is added directly to the laser cur- rent. The optical modulation depth ( m )isdeﬁned as the  ratio of the peak modulation L 0 for one channel, divided by L 0 .For 60-channel AM-VSBsystems, m is typically near 4%.   The laser (optical carrier) is modulated by the sum of the video channels that are combined to form the total RF signal spectrum. The resultantoptical spectrum contains sidebands from the IM superimposed on unintentional frequency modulation, or chirp ,that generally accom- panies IM. This complex optical spectrum must by under- stood if certain subtle impairments are to be avoided.   Aphotodetector convertsthe incident optical power into current. Broadband InGaAs photodetectors with  responsivities ( R 0 )ofnearly 1.0 A/W and bandwidths greater than 10 GHz are available. The detector generates FIGURE 3.1 Bandwidth versus carrier-to-noise ratio adccurrent corresponding to the average received optical (CNR) required for AM-VSB, FM, and digital video. power L r and the completeRFmodulation spectrum that Increasingly complex digital compression techniques was applied at the transmitter.Anaccoupled electronic reducethe bit rate required for NTSC-like video from preampliﬁer is used to removethe dc component and 100 Mbps to less than 5Mbps. Bandwidth efﬁcient RF boost the signal to usable levels.               techniques like QAMminimize the bandwidth                                                  required for each bit rate but require greater CNRs. Noise  Limitations The deﬁnition of CNR deservesclariﬁcation. Depending on the video format and RF modulation technique, the RF power spectrum of the modulated RF carrier varies widely.For AM-VSB video the remaining carrier is the dominant feature in the spectrum. It is thereby convenient to deﬁne the CNR as the ratio of the power remaining in the carrier to the integrated noise powerina4-MHz bandwidth centered on the carrier frequency.For FM or digitally modulated carriers, the original carrier is not generally visible in the RF spectrum. It is then necessarytodeﬁne the CNR as the ratio of the integrated signal powerwithin the channel bandwidth to the integrated noise power. Shot Noise Shot noise is aconsequence of the statistical nature of the photodetection process. It results in anoise power spectral density,orelectrical noise powerper unit bandwidth (dBm/Hz) that is proportional to the received  photocurrent I r ( ¼ R 0 L r ). The total shot noise power in abandwidth B is given by                                         N s ¼ 2 eIr B                               ð 3 : 3 Þ where e is the electronic charge.   With small m ,the detected signal current is asmall fraction of the total received current. The root mean square (rms) signal power for one channel is                                            1                                       P ¼   ð mI Þ 2                               ð 3 : 4 Þ                                        s   2   r  The total shot noise powerthen limits the CNR ( P s / N s )toalevelreferred to as the quantum limit. Received powers near 1mWare required if CNRs greater than 50 dB are to be achievedfor 40- to 80-channel AM-VSB systems.Optical Communication                                                               3 -5  Receiver Noise Receiver noise is generated by the electronicampliﬁer used to boost the detected photocurrent to usable levels. The easiest receiver to build consists of a pin photodiode connected directly to alow-noise 50- to 75-O ampliﬁer,asshown in Figure3.2(a). The effective input current noise density, ( n ), for this simple receiver is given by                              4 kTF                       n 2 ¼                        ð 3 : 5 Þ                              R L  where k is the Boltzmann constant, T is the absolute temperature, F is the noise ﬁgure of the ampliﬁer,and R L is the inputp ﬃﬃﬃﬃ impedance. For a 50-O input impedance and F ¼ 2, n ¼ 20 pA= Hz.   Avarietyofmore complicated receiver designs can reduce the noise current appreciably [Kasper,1988]. The example shown in  Figure3.2(b) uses ahigh-speed FET. R L can be increased to maximize the voltage developed by the signal current at the FET input. Input capacitancebecomes alimitation by shunting high-frequency                                                          FIGURE 3.2  Receivers for broadband components of signal current. High-frequency signals are then analog lightwavesystems. Coupling a pin reduced with respect to the noise generated in the FET,resulting in to alow-noise ampliﬁer (a) is simple, but poor high-frequency performance. Various impedancematching improved performancecan be obtained  techniques havebeen proposed to maximize the CNR for speciﬁc using designs like the pin FET (b). C t is frequency ranges.                                        the undesirable input capacitance.  Relative Intensity Noise Relativeintensity noise (RIN) can originate fromthe laser or from reﬂections and Rayleigh backscatter in the ﬁber.Inthe laser,RIN is caused by spontaneous emission in the active layer.Spontaneous emission drives random ﬂuctuations in the number of photons in the laser which appear as arandom modulation of the output intensity, with frequency components extending to tens of gigahertz. The noise power spectral density            2 from RIN is I r RIN, whereRIN is expressed in decibels per hertz.   RIN is also caused by component reﬂections and double-Rayleighbackscatter in the ﬁber,byaprocess called multipath interference. Twice-reﬂected signals arriving at the detector can interfere coherently with the unreﬂected signal. Depending on the modulated optical spectrum of the laser,this interference results in noise that can be signiﬁcant [Darcie et al., 1991].   The CNR, including all noise sourcesdiscussed, is given by                                                 2 2                                              m  I r                              CNR   ¼     2          2                              ð 3 : 6 Þ                                      2 B ½ n þ 2 eIr þ I r RIN   All sources of intensity noise are combined into RIN. Increasing m improves the CNR but increases the impairment caused by nonlinearity,asdiscussed in the following subsection. The optimum operating value for m is then abalancebetween noise and distortion.   Figure3.3 shows the noise contributions from shot noise, receiver noise, and RIN. For FM or digital systems, the low CNR values required allow operation with small received optical powers. Receiver noise is then generally the limiting factor.Much larger received powers are required if AM-VSB noise requirements are to be met. Althoughdetecting moreoptical powerhelps to overcome shot and receiver noise, the ratio of signal to RIN remains constant. RIN can be dominant in high-CNR systems, when the received poweris large. AM-VSB systems require special care to minimize all sourcesofRIN. The dominant noise source is then shot noise, with receiver noise and RIN combining to limit CNRs to within afew decibels of the quantum limit.3 -6                                   Broadcasting and Optical Communication Technology  Linearity Requirements Source linearitylimits the depth of modulation that can be applied. Linearity,inthis case, refers to the linearityofthe current-to-light-intensity (I–L) conversion in the laser or voltage-to-light (V–L) transmission for an external modulator.Numerous nonlinear mechanisms must be considered for direct modulation, and no existing external modulator has alinear transfer function.   ATaylor-series expansion of the I–L or V–Lchar- acteristic, centered at the bias point, results in linear, quadratic, cubic, and higher-order terms. The linear term describes the efﬁciency withwhich the applied signal is converted to linear intensitymodulation. The quadratic term results in second-order distortion, the cubic produces third-order distortion, and so on.   Requirements on linearitycan be derived by consider- ing the number  and  spectral distribution of the FIGURE 3.3 Current noise densities from receivers, distortion products generated by the nonlinear mixing RIN, and shot noise as afunction of total received between carriers in the multichannel signal. Second- photocurrent. Receiver noise is dominant in FM or order nonlinearity results in sum and difference(f ^ f ) some digital systems where the total received power is                                         i   j   small. The solid line for receiver noise represents the mixing products for everycombination of the two                                                 noise current for atypical 50-O low-noise ampliﬁer. channels. This results in as manyas50second-order                                                 More sophisticated receiver designs could reduce the products within asingle channel, in a60-channel AM- noise to the levels shown approximately by the dotted VSBsystem  with the standardU.S. frequency plan. lines. RIN and shot noise are more important in AM- Similarly,for third-order distortion, products result VSB systems. from mixing among all combinations of three channels. However,sincethe number of combinations of threechannels is much larger than for two,upto1130 third- order products can interferewithone channel. The cable industrydeﬁnes the composite second-order ( CSO) distortion as the ratio of the carrier to the largest group of second-order products within each channel. For third-order distortion, the composite triple beat ( CTB)isthe ratio of the carrier to the total accumulation of third-order distortion at the carrier frequency in each channel.   The actual impairment from these distortion products depends on the spectrum of each RF channel and on the exact frequency plan used. Atypical 42-channel AM-VSBfrequency plan, with carrier frequencies shown as the vertical bars on Figure 3.4, results in the distributions of second- and third-order products shown in   FIGURE 3.4 Second-order (a) and third-order (b) distortion products for a42-channel AM-VSB system. The maximum number of second-order products occurs at the lowest frequency channel, where30products contribute to the CSO.The maximum number of third-order products occurs near the center channel, where530 products contribute to the CTB.Optical Communication                                                               3 -7  Figure3.4(a) and (b), respectively.Since the remaining carrier is the dominant featureinthe spectrum of each channel, the distortion products are dominated by the mixing between these carriers. Because high-quality video requires that the CSO is –60 dBc (dB relative to the carrier), each sum or difference product must be less than –73 dBc. Likewise, for the CTB to be less than 60 dB,each product must be less than approximately –90dB.   FM or CDVsystems havemuch less restrictivelinearityrequirements, because of the reduced sensitivityto impairment. Distortion products must be counted, as with the AM-VSB example described previously,but each product is no longer dominated by the remaining carrier.Because the carrier is suppressed entirely by the modulation, each product is distributed over more than the bandwidth of each channel. The impairment resulting from the superposition of manyuncorrelated distortion products resembles noise. Quantities analogous to the CSO and CTB can be deﬁned for these systems.  Laser Linearity Several factors limit the light-versus-current (L–I) linearity of directly modulated lasers. Early work on laser dynamics led to acomplete understanding of resonance-enhanceddistortion (RD). RD arises from the same carrier-photon interaction within the laser that is responsible for the relaxation- oscillation resonance.    The second-harmonic distortion (2f i )and two- tone third-order distortion (2f i – f j )for atypical 1.3-m mwavelength directly modulated semiconduc- tor laser are shown in Figure3.5 [Darcie et al., 1986]. Both distortions are small at low frequencies but rise to maxima at half the relaxation resonance FIGURE 3.5 Resonance distortion for directly modulated frequency.AM-VSB systems are feasible only within laser with resonance frequency of 7GHz. Both the second- the low-frequency window. FM or uncompressed harmonic 2 f i and two-tone third-order 2 f i f j distortion peak digital systems  requireenoughbandwidth      near half the resonancefrequency and are small at low per  channel that multichannel systems must  frequency.Also shown is the same third-order distortion                                              for an external modulator biased at the point of zero operate in the region of large RD.Fortunately,                                              second-order distortion. the CNR  requirements allowfor the increased distortion. The large second-order RD can be avoided entirely by operating within aone-octave frequency band (e.g., 2–4 GHz), such that all second-order products are out of band.   Within the frequency range between 50 and 500 MHz, nonlinear gain and loss, intervalence-band absorption, and, moreimportantly,spatial-hole burning (SHB) and carrier leakage can all be signiﬁcant. Carrier leakage prevents all of the current injected in the laser bond wire from entering the activelayer.This leakage must be reducedtoimmeasurable levels for AM-VSB applications.   SHB results from the nonuniform distribution of optical power along the length of the laser.InDFB lasers, because of the grating feedback, the longitudinal distribution of optical power can be highly nonuniform. This results in distortion [Takemoto et al., 1990] that can add to or cancelother distortion, making it, in some cases, adesirable effect.  Clipping Even if all nonlinear processes were eliminated, the allowable modulation would be limited by the fact that the minimum output poweriszero. Typical operating conditions with, for example, 60 channels, each with an average modulation depth ( m )near 4%, result in apeak modulation of 240%. Althoughimprobable, modulations of morethan 100% result in clipping.3 -8                                   Broadcasting and Optical Communication Technology    The effects of clipping were ﬁrst approximated by Saleh [1989], who calculated the modulation level at which the total power contained in all orders of distortion became appreciable.Evenfor perfectly linear lasers, the modulation depth is bounded to values beyond which all orders of distortion increase rapidly.Assuming that half the total powerinall orders of distortion generated by clipping is distributed evenly over each of N channels, clipping results in acarrier-to-interference ratio (CIR) given by                                                   2                                       p ﬃﬃﬃﬃ ð 1 þ 6 m Þ 2                                CIR ¼   2 p         e 1 = 2 m                       ð 3 : 7 Þ                                              m 3  wherethe rms modulation index m is                                             q ﬃﬃﬃﬃﬃﬃ                                       m ¼ m  N = 2                                 ð 3 : 8 Þ  External  Modulation Laser-diode-pumped YAGlasers with lowRIN and output powers greater than 200 mW havebeen developed  recently.Combined with linearized external LiNbO3 modulators, these lasers havebecome high-performance competitors to directly modulated lasers. YAGlasers with external modulation offer aconsiderable increase in launched power, and the low RIN of the YAGlaser translates into aslight CNR improvement. The most challenging technical hurdle is to develop alinear low-loss optical intensity modulator.    Low-loss LiNbO3 Mach–Zehnder modulators are available with insertion losses less than 3dB, modulation bandwidths greater than afew gigahertz, and switching voltages near 5V.The output intensityofthese modulators is asinusoidal function of the bias voltage. By prebiasing to 50% transmission, modulation applied to the Mach–Zehnder results in the most linear intensitymodulation. This bias point, which corresponds to the point of inﬂection in the sinusoidal transfer function, produces zero second-order distortion. Unfortunately,the corresponding third-order distortion is approximately 30 dB worse than a typical directly modulated DFB laser,atlow frequencies. This comparison is shown on Figure 3.5. Forhigh- frequency applications whereRDisimportant, external modulators can offer improved linearity. Ameans of linearizing the third-order nonlinearity is essential for AM-VSB applications.   Various linearization techniques havebeen explored. The two most popular approachesare feedforward and predistortion. Feedforward requires that aportion of the modulated output signal be detected and compared to the original applied voltage signal to provide an error signal. This error signal is then used to modulate a second laser,which is combined with the ﬁrst laser such that the total instantaneous intensity of the twolasers is areplica of the applied voltage. In principle, this technique is capable of linearizing anyorder of distortion and correcting RIN from the laser.   Predistortion requires less circuit complexitythan feedforward.Acarefully designed nonlinear circuit is placed before the nonlinear modulator,such that the combined transfer function of the predistorter- modulator is linear.Various nonlinear electronicdevices or circuits can act as second- or third-order predistorters. Difﬁculties include matching the frequency dependenceofthe predistorter with that of the modulator,hence achieving good linearityoverawide frequency range. Numerous circuit designs can provide reductions in third-order distortion by 15 dB.  MiscellaneousImpairments Laser chirp can cause problems with direct laser modulation. Chirp is modulation of the laser frequency caused by modulation of the refractive index of the laser cavityinresponse to current modulation. The interaction of chirp and chromatic dispersion in the ﬁber can cause unacceptable CSO levels for AM-VSB systems as shortasafew kilometers. Dispersion converts the FM into IM, which mixes with the signal IM to produce second-order distortion [Phillips et al., 1991]. These systems must operate at wavelengths corresponding to low ﬁber dispersion, or correctivemeasures must be taken.Optical Communication                                                               3 -9    Chirp also causes problems withany optical component that has atransmission that is afunction of optical frequency.This can occur if two optical reﬂections conspire to form aweak interferometer or in an erbium- doped ﬁber ampliﬁer (EDFA) that has afrequency-dependent gain [Kuo and Bergmann, 1991]. Onceagain, the chirp is converted to IM, which mixes withthe signal IM to form second-order distortion.   Although externally modulated systems are immune to chirp-related problems, ﬁber nonlinearity,inthe form of stimulated Brillouin scattering (SBS), places alimit on the launched power. SBS, in which light is scattered from acoustic phonons in the ﬁber,causes arapid decrease in CNR for launched powers greater than approximately 10 mW [Mao et al., 1991]. Sincethe SBS process requires highoptical powers within anarrow optical spectral width (20 MHz), it is aproblem only in low-chirp externally modulated systems. Chirp in DFB systems broadens the optical spectrum so that SBS is unimportant.  Summary Awide range of applications for transmission of video signals over optical ﬁber has been made possible by reﬁnements in lightwavetechnology. Numerous technologyoptions are available for each application, each with advantages or disadvantages that must be considered in context with speciﬁc system requirements. Evolution of these video systems continues to be driven by development of new and improved photonic devices.  Deﬁning   Terms Chirp:  Modulation of the optical frequency that occurs when alaser is intensitymodulated. Composite second order (CSO): Ratio of the power in the second-order distortion products to power in      the carrier in acable television channel. Composite triple beat (CTB): Same as CSO but for third-order distortion. Direct modulation: Modulation of the optical intensityoutput from asemiconductor diode laser by direct      modulation of the bias current. Erbium-doped ﬁber ampliﬁer:  Fiber doped with erbium that provides optical gain at wavelengths near      1.55 m mwhen pumped optically at 0.98 or 1.48 m m. External modulation:  Modulation of the optical intensity using an optical intensity modulator to      modulate aconstant power(cw) laser. Fiber dispersion: Characteristic of optical ﬁber by which the propagation velocitydepends on the optical      wavelength. Fiber nonlinearity: Properties of optical ﬁbers by which the propagation velocity,orother characteristic,      depends on the optical intensity. Lightwave technology:   Technologybased on the use of optical signals and optical ﬁber for the      transmission of information. Linear: Said of anydevicefor which the output is linearly proportional to the input. Noise ﬁgure:  Ratio of the output signal-to-noise ratio (SNR) to the input SNR in an ampliﬁer. Rayleigh backscatter: Optical powerthat is scattered in the backwards direction by microscopic inhomo-      geneities in the composition of optical ﬁbers. Relative intensity noise: Noise resulting from undesirable ﬂuctuations of the optical power detected in      an optical communication system. Shot noise:  Noise generated by the statistical nature of current ﬂowing throughasemiconductor p - n      junction or photodetector.  References J.A. Chiddix, H. Laor,D.M. Pangrac, L.D.Williamson, and R.W.Wolfe, ‘‘AM video on ﬁber in CATV systems,      need and implementation,’’ IEEE J. Selected Areas in Communications, vol. 8, p. 1229, 1990.3 -10                                  Broadcasting and Optical Communication Technology  T.E. Darcie, ‘‘Subcarrier multiplexing for lightwavenetworks and video distribution systems,’’ IEEE J. Selected      Areas in Communications, vol.8,p.1240, 1990. T.E. Darcie, G.E. Bodeep,and A.A.M. Saleh, ‘‘Fiber-reﬂection-induced impairments in lightwaveAM-VSB      CATV  systems,’’ IEEE J. Lightwave Technol., vol. 9, no.8,pp. 991–995, Aug. 1991. T.E. Darcie, R.S. Tucker, and G.J.Sullivan, ‘‘Intermodulation and harmonic distortion in IaGaAsP lasers,’’      Electron. Lett., vol. 21, 665–666, erratum; vol 22, p. 619, 1986. K. Feher,Ed., Advanced Digital Communications,Englewood Cliffs, N.J.: Prentice-Hall, 1987. B.L. Kasper,‘‘Receiver design,’’ in Optical Fiber Telecommunications II, S.E. Miller and I.P.Kaminow, Eds.,      San Diego: Academic Press, 1988. C.Y.Kuo and E.E. Bergmann, ‘‘Erbium-doped ﬁber ampliﬁer second-order distortion in analog links and      electroniccompensation,’’ IEEE Photonics Technol. Lett., vol. 3, p. 829, 1991. X.P.Mao,G.E. Bodeep,R.W.Tkach, A.R. Chraplyvy,T.E. Darcie, and R.M. Derosier,‘‘Brillouin scattering      in lightwaveAM-VSB CATV  transmission systems,’’ IEEE Photonics Technol. Lett., vol. 4, no.3,      pp.287–289, 1991. A.N. Netravali and B.G. Haskel, Digital Pictures, NewYork: Plenum Press, 1988. R. Olshansky,V.Lanzisera, and P. Hill,‘‘Design and performanceofwideband subcarrier multiplexed      lightwavesystems,’’ in Proc. ECOC ’88, Brighton, U.K., Sept. 1988, pp.143–146. M.R. Phillips, T.E. Darcie, D. Marcuse, G.E. Bodeep,and N.J.Frigo,‘‘Nonlinear distortion generated by      dispersivetransmission of chirped intensity-modulated signals,’’ IEEE Photonics Technol. Lett., vol. 3,      no.5,pp. 481–483, 1991. T. Pratt and C.W.Bostian, Satellite Communications, NewYork: Wiley,1986. A.A.M. Saleh, ‘‘Fundamental limit on number of channels in subcarrier mulitplexed lightwaveCATVsystems,’’      Electron. Lett., vol. 25, no.12, pp.776–777, 1989. A. Takemoto,H.Watanabe, Y. Nakajima, Y. Sakakibara, S. Kakimoto,U.Yamashita, T. Hatta, and Y. Miyake,      ‘‘Distributed feedback laser diode and module for CATV systems,’’ IEEE J. Selected Areas in      Communications, vol. 8, 1359, 1990. W. Way, C. Zah. C. Caneau, S. Menmocal, F. Favire,F.Shokoochi, N. Cheung,and T.P. Lee, ‘‘Multichannel      FM  video transmission using traveling wave ampliﬁers for subscriber distribution,’’ Electron. Lett.,      vol. 24, p. 1370, 1988.  Further  Information National Cable Television Association (NCTA), Proceedings from Technical Sessions, annual meetings, 1724      Massachusetts Ave. NW,Washington D.C., 20036, 1969. SocietyofCable Television Engineers (SCTE), Proceeding from Technical Sessions, biennial meetings, Exton      Commons, Exton, Penn. T.E. Darcie, ‘‘Subcarrier multiplexing for lightwavemultiple-access lightwavenetworks,’’ J. Lightwave      Technol., vol.LT-5, pp.1103–1110, Aug. 1987. T.E. Darcie and G.E. Bodeep,‘‘Lightwavesubcarrier CATV transmission systems,’’ IEEE Trans. Microwave      Theoryand Technol., vol. 38, no.5,pp. 534–533, May1990. IEEE J. Lightwave Technol.,Special Issue on ‘‘Broadband Analog Video Transmission Over Fibers,’’ to be      published Jan./Feb.1993.  3.2     Long   Distance    Fiber  Optic   Communications Joseph  C.  Palais  When the ﬁrst laser was demonstrated in 1960, numerousapplications wereanticipated. Some predicted laser beams would transmit messages through the air at highdata rates between distant stations. Althoughlaser beams can travelthrough the atmosphere,too manyproblems prevent this scheme from becoming practical. Included in the objections are the need for line-of-sight paths and the unpredictabilityoftransmissionOptical Communication                                                              3 -11                                         Transmitter           Electrical            Input                       Modulator          Laser           Coupler                            Optical          Regenerator        Optical                          Amplifier                           Amplifier                         Photodiode        Amplifier        Processing                                                                       Electrical                                         Receiver                      Output                        FIGURE 3.6 Long-distance ﬁber communications system.  throughanatmosphere, whereweather variations randomly change path losses. Guided paths using optical ﬁbers offer the only practical means of optical transmission over long distances.   Long-distanceﬁber systems typically havethe following operational characteristics: they are morethan 10 km long,transmit digital signals (rather than analog), and operate at data rates aboveafew tens of megabits per second. This section describes systems in this category.   Figure3.6 illustrates the basic structureofageneralized long-distanceﬁber optic link. Each of the components are described in the following paragraphs.   Auseful ﬁgure of merit for these systems is the product of the system data rate and its length. This ﬁgure of merit is the well-known rate-length product. The bandwidth of the transmitting and receiving circuits, including the light source and photodetector,limits the achievable system data rate. The bandwidth of the ﬁber decreases with its length, so that the ﬁber itself limits the rate-length product. The losses in the system, including those in the ﬁber,also limit the path length. Systems are bandwidth limited if the rate-length ﬁgure is determined by bandwidth restraints and loss limited if determined by attenuation.   The ﬁrst efﬁcient ﬁber appeared in 1970, having aloss of 20 dB/km. Seven years later,the ﬁrst large-scale application was constructed between two telephone exchanges in Chicago.Bythis time, the loss had been reduced to around 3dB/km. The digital technologyused could accommodatearate of 45 Mbps over an unrepeatered length of 10 km and atotal length of over 60 km with repeaters. The unrepeatered rate-length product for this initial system was amodest 450 Mb/s · km. As ﬁber technologyadvanced, this ﬁgure steadily increased. Unrepeatered rate-length products haveimproved to beyond 1000 Gb/s · km (e.g., 10 Gb/s over a path of 100 km). Including regenerators and optical ampliﬁers in the links increase the net rate-length product considerably.Values beyond 70 Tb/s · km (70,000 Gb/s · km) are achievable with optical ampliﬁers. This latter ﬁgure allows construction of atransmission system operating at 5Gb/s over a14,000-km path. The longest terrestrial paths are across the Atlantic and Paciﬁc oceans, distances of about 6,000 and 9,000 km, respectively. Fibers are capable of spanning these distances with high-capacitylinks.  Fiber All ﬁbers used for long-distancecommunications are made of silica glass and only allow asingle mode of propagation. The silica is doped with other materials to producethe required refractiveindex variations for the ﬁber coreand cladding.The important ﬁber characteristics that limit system performanceare its loss and bandwidth. The loss limits the length of the link, and the bandwidth limits the data rate.   Figure3.7 shows the loss characteristics of single-mode silica ﬁbers. The loss in the 800–900 nm region is dominated by Rayleigh scattering.Total attenuations of approximately 2.5 dB/km makethis wavelength region3 -12                                  Broadcasting and Optical Communication Technology                         3.0                                   TOTAL      OH ABSORPTION  PEAK                       2.5         LOSS                       2.0                        1.5                       1.0                            RAYLEIGH                       0.5                            SCATTERING                      ATTENUATION (dB/km)                       0.0                         800 900  1000 1100 1200 1300 1400 1500 1600 1700                                       WAVELENGTH  (nm)  FIGURE 3.7 Spectral attenuation of asilica glass ﬁber showing contributions from Rayleigh scattering and water vapor absorption.                 TABLE 3.1 Spectral Band Classiﬁcation Scheme                 Band                   Descriptor               Range (nm)                 O-band              Original                     1260–1360                E-band              Extended                     1360–1460                S-band              Short wavelength             1460–1530                C-band              Conventional                 1530–1565                L-band              Long wavelength              1565–1625                U-band              Ultra-long wavelength        1625–1675  unsuitable for long distancelinks. Losses are approximately 0.5 dB/km around 1300 nm and 0.25 dB/km around 1550 nm. Manufacturing ﬁbers with low OH ion content can reduce the water vapor absorption peak at 1390 nm. As aresult, the entirewavelength range from 1260 to 1675 nm can be utilized for ﬁber systems. The appropriate wavelength designations appear in Table 3.1. The minimum loss near 1550 nm makes this the optimum choice for the very longest links.   Dispersion refers to the spreading of apulse as it travels along asingle-mode ﬁber as aresult of material and waveguide effects. This spreading creates intersymbol interference if allowed to exceed roughly 70% of the original pulse width, causing receiver errors. The dispersion factor Misusually given in units of picoseconds of pulse spread per nanometer of spectral width of the light source and per kilometer of length of ﬁber.   In the range from 1200 to 1600 nm, the an approximation of the dispersion curve for silica is                                              !                                         M        l 4                                    M  ¼   0  l    0                                ð 3 : 9 Þ                                          4       l 3   where l is the operating wavelength, l 0 is the zerodispersion wavelength,and M 0 is the slope at the zero                                               2 dispersion wavelength. M 0 is approximately 0.095 ps/(nm · km). The pulse spread for apath length L ,using alight source whose spectral width is D l ,isthen                                        D t ¼ M L D l                               ð 3 : 10Þ  The zero dispersion wavelength, which is close to 1300 nm for silica ﬁbers, makes this wavelength attractive for high-capacitylinks. The dispersion at 1550 nm is typically close to 20 ps/(nm · km). This is amoderate amount of dispersion. If aproposed 1550-nm system is bandwidth limited because of this spread, several alternativesare available. One solution is to use dispersion-shifted ﬁber,which is aspecial ﬁber with aOptical Communication                                                              3 -13  refractiveindex proﬁle designed to shift the zerodispersion wavelength from 1300 nm to 1550 nm. Another solution is to transmit soliton pulses, which use the nonlinearityofthe ﬁber to maintain pulse shape during transmission.   Because of highloss, the 800–900 nm partofthe spectrum can only be used for moderate lengths (around 10 km). Because of highdispersion, the data rates are also limited in this region. In the 1300 nm region, the nearly zerodispersion allows high-rate transmission, but the losses limit the distance that can be covered (typically around 50 km). Around 1550 nm, the loss is roughly half the 1300-nm attenuation and twice as much distance can be covered. Dispersion-shifted ﬁber allows the same highrates as the 1300-nm operation. Regenerators and ampliﬁers extend the useful distanceofﬁber links well beyond the distances listed here.  Modulator Adigital electrical signal current modulates the light source. The driver circuit must be fast enoughtooperate at the system bit rate. As bit rates increase into the multigigabit per second range, this becomes increasingly difﬁcult. Modulation can be completed in the optical domain at very highspeeds. In this case, the modulator follows the laser diode rather than preceding it. External modulation is usually accomplished using integrated- optic structures.  Light Source Laser diodes or light-emitting diodes (LEDs) supply the optical carrier wavesfor most ﬁber links. LEDs cannot operate at speeds in the gigabit range, but laser diodes can. Forthis reason, laser diodes are normally required for high-rate, long-distance links. Laser diodes can be modulated at frequenciesbeyond 40 GHz.   Laser diodes emitting in the 1200–1700 nm region are semiconductor heterojunctions made of InGaAsP. The exact emission wavelength is primarily determined by the proportions of the constituent atoms. Output powers are commonly in the order of afew milliwatts.   Typical laser diode spectral widths are between 1and 5nmwhen operating in more than one longitudinal mode. Single-mode laser diodes can havespectral widths of just afew tenths of ananometer.Aspredicted by Equation (3.10), narrow-spectral-width emitters minimize pulse spreading.Minimizing pulse spreading increases the ﬁber bandwidth and its data capacity.   Solid-state lasers, rather than semiconductor laser diodes, may be useful in speciﬁc applications. Examples are the Nd:YAG laser and the erbium-doped ﬁber laser.  Source Coupler The light emitted from the diode must be coupled as efﬁciently as possible to the ﬁber.Because the beam pattern emitted by alaser diode does not perfectly match the pattern of light propagating in the ﬁber,thereis an inevitable mismatch loss. Good coupler designs, sometimes using miniature lenses, reducethis loss to about 3dBwhen feeding asingle-mode ﬁber.  Isolator An optical isolator is aone-way transmission path. It allows power to ﬂow from the transmitter toward the receiver,but blocks the power ﬂow in the opposite direction. The optical isolator protects the laser diode from back reﬂections, which tend to increase the laser noise.  Connectors and Splices Connections between ﬁbers and between the ﬁber and other components occur at numerouspoints in along- distancelink. Because there maybemanysplices in along system, the loss in each splicemust be small. Fusion splices with an average loss of no morethan 0.05 dB are often speciﬁed. Mechanical splices are also suitable.3 -14                                  Broadcasting and Optical Communication Technology  They often involve epoxy for ﬁxing the connection. Connectors are used whereremateable connections are required. Good ﬁber connectors introduce losses of only afew tenths of adecibel.   In addition to having low loss, good connectors and splices also minimize back reﬂections. This is especially important to reduce laser noise in connectionsnear the transmitter.Fusion splices producelittle reﬂection, but mechanical splices and all connectors must be carefully designed to keep reﬂected power levels low.Reﬂections occur because of small gaps at the interface between the mated ﬁbers. Successful techniques for reducing reﬂections include aphysical contact connection,where the ﬁber end faces are polished into hemispheres (rather than ﬂat surfaces), so that the cores of the two mated ﬁbers are in contact with each other.Performance is increased by angling the end faces afew degrees, causing reﬂected light to be ﬁltered out of the single propagating mode.  Optical  Ampliﬁer Manyﬁber links are loss limited. One cause is the limited power available from the typical laser diode, which (with the losses in the ﬁber and the other system components) restricts the length of ﬁber that can be used. The ﬁber optic ampliﬁer increases the powerlevelofthe signal beam without conversion to the electrical domain. Forexample, gains of 30 dB are attainable at 1550 nm using the erbium-doped ﬁber ampliﬁer (EDFA). Note, the EDFAhas abandwidth of over 20 nm, allowing several WDM or numerousOFDM channels (both described later in this section) to be ampliﬁed simultaneously.   As indicated in Figure3.6, thereare anumber of possible locations for optical ampliﬁers in asystem. An optical ampliﬁer that is only following the transmitter increases the optical powertraveling down the ﬁber. Ampliﬁers along the ﬁber path continually keep the power levels above the system noise. An ampliﬁer located at the ﬁber end acts as areceiver preampliﬁer,enhancing its sensitivity. Manyampliﬁers can be placed in a ﬁber network, extending the total path length to thousands of kilometers.  Regenerator The regenerator detects the optical signal by converting it into electrical form. It then determines the content of the pulse stream,uses this information to generate anew optical signal, and launches this improved pulse train into the ﬁber.The new optical pulse stream is identical to the one originally transmitted. The regenerated pulses are restored to their original shape and powerlevel by the repeater.   Manyregenerators maybeplaced in aﬁber network, extending the total path length to thousands of kilometers. The advantage of the optical ampliﬁer over the regenerator is its lowercost and improved efﬁciency.The greater cost of the regenerator arises from the complexityofthe conversion between the optical and electrical domains. The regenerator does havethe advantage of restoring the signal pulse shape, which increases the system bandwidth. This advantage is negated by asystem propagating soliton pulses, which do not degrade with propagation.  Photodetector The photodetector convertsanincomingoptical beam into an electrical current. In ﬁber receivers, the most commonly  used photodetectors are semiconductor pin photodiodes and avalanche photodiodes (APD). Important detector characteristics are speed of response, spectral response, internal gain, and noise. Because avalanche photodiodes have internal gain, they are preferred for highly sensitive receivers. Both germanium (Ge) and InGaAs photodiodes respond in the low-loss 1200- to 1700-nm wavelength regions. InGaAs performs better at low signal levelsbecause it has smaller values of dark current (less noisy).   The current produced by aphotodetector in response to incident optical powerPis                                        i ¼ G Z eP= hf                              ð 3 : 11Þ  where G is the detector’sgain, Z is its quantum efﬁciency (close to 0.9 for good photodiodes), h is Planck’s constant (6.63 · 10–34 Js), e is the magnitude of the charge on an electron (1.6 · 10  19coulomb),Optical Communication                                                              3 -15  and f is the optical frequency.For pin photodiodes (where G ¼ 1), typical responsivities are in the order of 0.5 m A/m W. Receiver Because of the lowpowerlevels expected at the input to the receiver,anelectronic ampliﬁer is normally required following the photodetector.The remainder of the receiver includes such electronicelements as band-limiting ﬁlters, equalizers, decision-making circuitry, other ampliﬁcation stages, switching networks, digital-to-analog converters, and output devices(e.g., telephones, video monitors, and computers).  Other Components There are anumber of ﬁber components, not shown in Figure3.6, that can be found in some systems. These include passive couplers for tapping offaportion of the beam from the single ﬁber and wavelength-division multiplexers for couplingdifferent optical carriers onto the transmission ﬁber.  System  Considerations Long-distanceﬁber links carryvoice, video,and data information. Messages that are not already in adigital format are converted. Asingle voicechannel is normally transmitted at arate of 64,000 bits per second. Video requires amuch higher rate. The rate could be as highas90Mbps, but video compression techniques can lowerthis rate signiﬁcantly.Fiber systems for the telephone network operate at such highrates that manyvoice channels can be time-division multiplexed (TDM) onto the same ﬁber for simultaneous transmission. Forexample, aﬁber operating at arate of 2.5 Gb/s can carrymorethan 30,000 digitized voicechannels, while one operating at 10 Gb/s can accommodatealmost 130,000 voicechannels.   Several optical carriers can simultaneously propagate along the same ﬁber.Suchwavelength-division multiplexed (WDM) links further increase the capacityofthe system. Systems using up to 32 optical carriers are common. Adding 16 or morechannels puts constraints on the multiplexers and light sources. Nonetheless, systems with over 100 channels are feasible. The carrier wavelengthsare spaced by afew tenths of anmorless. In long systems, wideband optical ampliﬁers are preferred over regenerators for WDM systems because a single ampliﬁer can boost all the individual carriers simultaneously;however, separate regenerators are needed for each carrier wavelength.   Placing numerous ﬁbers inside asingle cable increases total cable capacity. This is acost-effectivestrategy when installing long ﬁber cables. The added cost of the extra ﬁbers is small compared to the costs of actually deploying the cable itself. Fiber counts of several hundred are practical. Multiﬁber cables can haveenormous total data capacities.   Still further capacityispossible using optical frequency-division multiplexing (OFDM). In this scheme, manyoptical carriers very closely spaced in wavelength (maybe afew hundredths of ananometer) operate as independent channels. Many hundreds of channels can be visualized in 1200–1700 nm wavelength regions. Systems of this type require coherent detection receivers to separate the closely spaced carriers.  Error Rates and Signal-to-Noise Ratio The signal-to-noise ratio is ameasure of signal quality. It determines the error rate in adigital network. At the receiver,itisgiven by                                                 2                              S            ð G r P Þ R L                                ¼   n                                              ð 3 : 12Þ                              N    G 2 eRL B ð I D þ r P Þþ4 kTB  where P is the received optical power, r is the detector’sunampliﬁed responsivity, G is the detector gain if an APD is used, n accounts for the excess noise of the APD (usually between 2and 3), B is the receiver’s bandwidth, k is Boltzmann’s constant ( k ¼ 1.38 · 10–23 J/K), e is the magnitude of the charge on an electron3 -16                                  Broadcasting and Optical Communication Technology          –19 (1.6 · 10 coulomb), T is the receiver’stemperatureindegrees Kelvin, I D is the detector’sdark current, and R L is the resistanceofthe load resistor that follows the photodetector.   The ﬁrst term in the denominator of Equation (3.12) is caused by shot noise and the second term is attributed to thermal noise in the receiver.Ifthe shot-noise term dominates (and the APD excess loss and dark current are negligible), the system is shot-noise limited. Therefore,the probabilityoferrorhas an upper bound given by                                                 n s                                        P e ¼ e                                    ð 3 : 13Þ  where n s is the average number of photoelectrons generated by the signal during asingle bit interval when a binary1is received. An error rate of 10–9 or better requires about 21 photoelectronsper bit. Shot noise depends on the optical signal level.Because the power levelisnormally low at the end of along-distance system, the shot noise is small compared to the thermal noise. Avalanche photodiodes increase the signal level so that shot noise dominates over thermal noise. With APD receivers, ideal shot-noise limited operation can be approached but, not reached because of the APD excess noise and limited gain.   If the thermal noise dominates, the error probabilityisgiven by                                  P e ¼ 0 : 5   0 : 5erfð 0 : 354Þð3                  : 14Þ  whereerf is the error function. An error rate of 10–9 requires asignal-to-noise ratio of nearly 22 dB.  System  Design Amajor  partofﬁber system design involves the powerbudget and the bandwidth budget. The next few paragraphs describe these calculations.   In aﬁber system, component losses (or gains) are normally given in decibels. The decibel is deﬁned by                                     dB ¼  10 log P 2 = P 1                         ð 3 : 15Þ  where P 2 and P 1 are the output and input powers of the component. The decibel describes relative power levels. Similarly, dBm and dBm describe absolute power levels. They are given by                                     dBm  ¼ 10 log P                               ð 3 : 16Þ  where P is in milliwatts and                                     dBm  ¼ 10 log P                               ð 3 : 17Þ  where P is in microwatts.   Powerbudget calculations are illustrated in Table 3.2 for asystem that includes an ampliﬁer.Aspeciﬁc numerical example is found in the last two columns. The receiver sensitivityin dBm is subtracted fromthe poweravailable from the light sourcein dBm.This difference is the loss budget (in decibels) for the system. All the system losses and gains are added together (keeping in mind that the losses are negative and the ampliﬁer  gains are positive). If the losses are morethan the gains (as is usual), the system loss dBSL will be anegative number.The loss margin is the sum of the loss budget and the system loss. It must be positive for the system to meet the receiver sensitivityrequirements. The system loss margin must be speciﬁed to account for component aging and other possible system degradations. Table 3.2 illustrates a6-dB margin for the system discussed above.The ﬁber in the table has atotal loss of 24 dB.Ifits attenuation is 0.25 dB/km, the total length of ﬁber allowed would be 24/0.25 ¼ 96 km.   In addition to providing sufﬁcient power to the receiver,the system must also satisfy the bandwidth requirements imposed by the rate at which data are transmitted. Aconvenient method of accountingfor the bandwidth is to combine the rise times of the various system components and comparethe result with the rise time needed for the given data rate and pulse coding scheme.Optical Communication                                                              3 -17    The system rise time is given in terms of the data rate by the expression                                       t ¼ 0 : 7 = R NRZ                            ð 3 : 18Þ  for non-return-to-zero (NRZ) pulse codes and                                       t ¼ 0 : 35= R RZ                             ð 3 : 19Þ  for return-to-zero (RZ) codes.   An example of bandwidth budget calculations appears in Table 3.3. The calculations are based on the accumulated rise times of the various system components.   The system in Table 3.3 runs at 500 Mb/s with NRZ coding for a100-km length of ﬁber.Equation (3.18) yields arequired system rise time no morethan 1.4 ns. The transmitter is assumed to havearise time of 0.8 ns. The receiver rise time, given as 1nsinthe table, is acombination of the photodetector’srise time and that of the receiver’selectronics.   The ﬁber’srise time was calculated for asingle-mode ﬁber operating at awavelength of 1550 nm. Equation (3.9) shows that M ¼ 18 ps/(nm · km) at 1550 nm. The light sourcewas assumed to haveaspectral width of 0.2 nm. Then, the pulse dispersion calculated from Equation (3.10) yields apulse spread of 0.36 ns. Because the ﬁber’srise time is close to its pulse spread, this value is placed in the table.   The total system rise time is the square root of the sum of the squares of the transmitter,ﬁber,and receiver rise times. That is,                                         q ﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃ                                           2    2   2                                    t s ¼ t t þ t f þ t r                          ð 3 : 20Þ  In this example, the system meets the bandwidth requirements by providing arise time of only 1.33 ns, where as much as 1.4 ns would have been sufﬁcient.              TABLE 3.2 PowerBudget Calculations              Sourcepower:dBms                                   3             Receiver sensitivity:           dBmr                 30             Loss budget: dBms-dBmr                   dBLB               33             Component efﬁciencies:                Connectors                    dBc                  5               Splices                       dBs                  2               Source coupling loss          dBcl                 5               Fiber loss                    dBf                  24               Isolator insertion loss       dBi                  1               Ampliﬁer gain                 dBa                10             Total system loss:                dBc þ dBs þ dBcl þ dBf þ dBi þ dBa     dBSL                 27             Loss Margin: dBLB þ dBSL                 dBLM               6                   TABLE 3.3 Bandwidth Budget Calculations*                  Transmitter:                t t        0.8                 Fiber:                      t f        0.36                 Receiver: q ﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃ t r        1                            2  2   2                 System total: t t þ t f þ t r t s                   1.33                 System required:            t                       1.4                    *All quantities in the table are rise time values in nanoseconds.3 -18                                  Broadcasting and Optical Communication Technology  Deﬁning   Terms Coherent detection: The signal beam is mixed with alocally generated laser beam at the receiver.This      results in improved receiver sensitivityand receiver discrimination between closely spaced carriers. Material dispersion: Wavelength dependenceofthe pulse velocity.Itiscaused by the refractive index      variation with wavelength of glass. Quantum   efﬁciency:  Aphotodiode’s conversion efﬁciency from incident photons to generated free      charges. Single-mode ﬁber (SMF):  Aﬁber that can supportonly asingle mode of propagation. Spectral width: The range of wavelengths emitted by alight source.  References G.P.Agrawal, Fiber-Optic Communication Systems, 3rd ed., NewYork: JohnWiley and Sons, 2002. E.E. Basch, Ed., Optical-Fiber Transmission,Indianapolis: Howard W. Sams &Co.,1987. M. Bass, ed., Fiber Optics Handbook,New York: McGraw-Hill, 2002. C.C. Chaffee, The Rewiring of America,San Diego: Academic Press, 1988. E. Desurvire,Erbium-Doped Fiber Ampliﬁers,New York: John Wiley &Sons, 2002. M.J.F.Digonnet, Rare Earth Doped Fiber Lasers and Ampliﬁers,2nd ed., NewYork: MarcelDekker,2001. R.J.Hoss, Fiber Optic Communications Design Handbook,Englewood Cliffs, N.J.: Prentice-Hall, 1990. L.B.Jeunhomme, Single-Mode Fiber Optics,2nd ed., NewYork: MarcelDekker,1990. N. Kashima, Passive Optical Components for Optical Fiber Transmission,Norwood, Mass.: Artech House, 1995. S.V.Kartalopoulos, DWDM: Networks, Devices and Technology ,New York: JohnWiley &Sons, 2002. G. Keiser,Optical Fiber Communications,3rd ed., NewYork: McGraw-Hill, 2000. R.H. Kingston, Optical Sources, Detectors and Systems,New York: Academic Press, 1995. J.C. Palais, Fiber Optic Communications,5th ed., Upper Saddle River,N.J.: Prentice-Hall, 2005. S. Shimada, Coherent Lightwave Communications Technology ,New York: Chapman and Hall, 1994. A. Yariv, Optical Electronics in Modern Communications,5th ed., NewYork: Oxford UniversityPress, 1997.  Further  Information Continuing information on the latest advances in long-distance ﬁber communications is available in several professional societyjournals and trade magazines including: IEEE Journal of Lightwave Technology , IEEE Photonics TechnologyLetters, Lightwave, and Laser Focus World.  3.3     Photonic    Networks Alan   E. Willner  and  Reza   Khosravani Introduction Due to the highsignal attenuation in copper transmission lines for data rates exceeding hundreds of Mbit/s, it has long been recognized that lightwavetechnologyhas signiﬁcant advantages over copper for long-distance data transmission and for cable TV (CATV)distribution networks. Over the past 20 years, the capacitythat can be transmitted over an optical ﬁber has increased and the cost-per-transmitted-bit decreased by several orders of magnitude. Optical transmission is the only viable solution for the ever-increasing bandwidth requirements, even within manylocal-area networks (LANs). Recently,asingle ﬁber was used for the transmission of 3.7 Tbit/s (3700 Gbit/s) over transoceanic distances [1]. Optical ﬁbers possess an enormous bandwidth that is the base technologyfor high-capacitynetworks of present and future.   However,overthe past decade, the need for high-capacitymulti-user networks has intensiﬁed because of signiﬁcant Internet growth. The World Wide Web, which is founded on wide-area networks (WAN), has become one of the main avenues of demand for information transfer.Asdemand grows, the copper-basedOptical Communication                                                              3 -19  routing and switching nodes might fail to provide the required throughput. Alaudable goal has been to make use of photonic powertobring greater throughput and cost-effectiveness to multi-user networking.   Akey technology, which allows straightforward routing and optical data path switching in aphotonic network, is wavelength-division-multiplexing (WDM), in which manyindependent data channels are transmitted in parallel down an optical ﬁber.Byutilizing wavelength-selectivecomponent technologies, each data channel’swavelength can be used to determine the routing through the network. Therefore, data is actually transmitted through wavelength-speciﬁc ‘‘light-paths’’ on aroute arranged by anetwork controller to optimize throughput.   Signiﬁcant changes in photonic networks have occurred in the last few years. The widespread deployment of optical ﬁbers and technological breakthroughs in photonic devices (i.e., optical ampliﬁers, wavelength multiplexers, and optical switches) has signiﬁcantly changed the achievable architecture of photonic networks. Furthermore,the gradual shift in the nature of the network trafﬁc from voice to data has called for new thinking in protocols, control, and management. Note, the circuit-switch-likeSONET ring photonic networks havebecome areality, wherethe wavelength lightpath determines the routing,and the networks are deployed throughout the world in metro-networks and beyond. We hope that some of the moreadvanced technologies discussed in this chapter will develop during the next decade.   Here, we ﬁrst review lightwavetransmission links that connect network nodes. Then, we discuss architecturesand protocols of photonic networks as well as switching techniques. Finally,wediscuss some of the enabling technologies and challenges in realizing highthroughput photonic networks.  Background:Lightwave      Transmission Links Interconnecting Nodes This section provides the basic background for asimple lightwavetransmission link that forms the basis for interconnecting nodes in aphotonic network [2].   Alightwavetransmission link consists of: atransmitter (Tx) that converts an electrical signal to an optical signal (E/O), an optical ﬁber as the connecting medium, and areceiver (Rx) that convertsthe optical signal back to the electrical domain (O/E). The transmitter typically uses alaser diode (LD) as the optical source. The receiver may employaPIN (positive-intrinsic-negative)orAPD (avalanche photodiode) photodetector.   The standardsilica ﬁber has aminimum attenuation of , 0.2 dB/km around 1550 nm. Depending on the core diameter and the optical wavelength, aﬁber may be multimode or single mode. Multimode ﬁbers (with a typical corediameter of 62.5 m m) allowseveral modes to propagate throughthe ﬁber,and may be used for shortdistancecommunications (e.g., LANs inside abuilding). The pulse energyisdivided among the different modes, and each mode arrives at the receiver with adifferent delay. This intermodal dispersion effect causes pulse spreading.Asingle mode ﬁber has amuch smaller corediameter ( , 10 m m) and allows only one mode to propagate. Althoughthe intermodal dispersion is eliminated in asingle mode ﬁber,the much lower intramodal chromatic dispersion is generated because of the index of refraction’s wavelength dependency.This dispersion is signiﬁcant for long-distanceand high-data-rate systems. The chromatic dispersion value for a conventional single mode ﬁber is approximately 17 ps/nm/km.   As an optical signal propagates through the ﬁber,the ﬁber loss causes signal power attenuation. At some point, the optical signal poweristoo weak for the receiver to detect. Prior to the invention of optical ampliﬁers, an electronicregenerator was required everyseveral tens of kilometers to detect and re-transmit each data channel. The invention of Erbium-doped ﬁber ampliﬁers (EDFAs) [3,4] improved optical communications in twomajor ways. First, the maximum reachofoptical links extended signiﬁcantly from tens to thousands of kilometers. Second, EDFAs are broadband and can amplify all the multi-wavelength optical signals simultaneously and cost-effectively.For example, agroup of four (or 40) optical channels co-propagating on different wavelengths inside the same ﬁber maybeampliﬁed using asingle EDFA(see Figure3.8). As aresult, EDFAs enable the practical deployment of wavelength-division-multiplexing that allows the parallel transmission of multiple optical signals on the same ﬁber.WDM dramatically enhances ﬁber capacitysince‘‘ N ’’ wavelength channels will increase the capacitybyafactor of N .EDFAampliﬁcation is independent of the signal wavelength,bit rate, and modulation format.3 -20                                  Broadcasting and Optical Communication Technology                                                E                   FIGURE 3.8 EDFAampliﬁes several optical channels at the same time.    Another optical ampliﬁcation technique, called Raman ampliﬁcation, has recently gained popularity. Compared to EDFAs, Raman ampliﬁers [5] can reducethe system degrading effects of ﬁber nonlinearities. Additionally,the pump wavelength determines the gain wavelength range of Raman ampliﬁers, allowing other signal wavelengths bands to be ampliﬁed. However,the pump efﬁciency of aRaman ampliﬁer is often signiﬁcantly lowerthan an EDFA.   In addition to transmitting on multiple parallel wavelengths, there is also atrend towards higher-bit-rates per channel. Today,commercial optical data channels can operate at 10 Gbit/s, with40-Gbit/s links about to be deployed. Astandard technique to generate ahighbit-rate signal is to combine manylow speed signals in the time domain. This technique is called time-division multiplexing (TDM). Since chromatic dispersion is a keylimitation, high-data-rate long-distance transmission should employdispersion compensating elements, for which the positive dispersion is compensated periodically along the ﬁber link with anegativedispersion value. These developments made it possible to build deployed optical networks that operate at 10 or even 40 Gbit/s. Forexample, single-mode ﬁber transmission links can supportbit-rate-distanceproduct of 500 (Gb/s) · km and beyond.   Multiple optical signals are transmitted on the same ﬁber simultaneously to utilize the wide bandwidth of an optical ﬁber moreefﬁciently.This is accomplished by assigning different wavelengths to the optical channels based on the International Communications Union (ITU) recommendation. In this standard, the available wavelengths are equally spaced (i.e., 50 or 100 GHz frequency spacing) on agrid stabilized to 193.1 THz ( , 1552.52 nm) [6]. Each digital data stream is modulated on adifferentoptical carrier wavelength. These optical signals are combined using awavelength multiplexer and transmitted through the ﬁber.Atthe receiving end, the WDM signal is demultiplexed, and each wavelength is detected using aseparate receiver. Figure3.9 shows atypical optical link comprised of multiple transmitters, awavelength multiplexer,optical ﬁbers and ampliﬁers, awavelength demultiplexer,and multiple receivers.  ArchitectureofPhotonic     Networks In the early stages of photonic systems, optics was used only for point-to-point connections. The ﬁrst generation of photonic networks used the traditional communication networks approach in which data was detected at each node and electronics provided the processing, routing, and switching tasks. With the introduction of WDM and the development of advanced optical components, manyfunctionalities that were not possible in traditional networks are nowavailable.Consequently,the new architectures composedofthese optical network elements were introduced [7].                     FIGURE 3.9 Atypical block diagram of alightwavetransmission link.Optical Communication                                                              3 -21                            FIGURE 3.10 Different topologies of the networks.    The general topologyofanetwork (the way that end terminals are interconnected) falls into one of the following categories: bus, ring,star,ormesh. In abus topology, all terminals are connected to the transmission medium thoughtaps (see Figure3.10a). Data transmitted by each terminal propagates in both directions until it is received by the destination terminal [8]. Aring network is made by connectingseveral nodes in aclosed loop fashion (Figure3.10b). The real topologymay be agood deal moreirregular than acircle, depending on the accessibilityofstations. Commercial token rings use wire interconnections or optical links to join stations. The transit-time delayincreases linearly with the number of stations. In astar topology(Figure 3.10c), all terminals are connectedtoacentral node [8]. Data that is transmitted from aterminal to the central node, will then be retransmitted to all other terminals, including the destination terminal.   Reliabilityisaprobleminboth ﬁber and wire rings. If astation is disabled, or if aﬁber breaks, the whole network goes down.Toaddress this problem, adouble-ring optical network, also called a‘‘self-healing’’ring,is used to bypass the defective stations and loop back around aﬁber break (Figure 3.11). Each station has two inputs and twooutputs connected to tworings that operate in opposite directions; therefore, cost increases.   As the size of anetwork increases, amore irregular topologymay be needed. Large metropolitan area networks (MANs) and WANs may consist of several ring networks that are interconnected throughone or morenodes. In amesh topology, each node is principally interconnectedtoits neighboring nodes. Data may be transmitted between anytwo terminals throughintermediate nodes. Sinceseveral independent routes may connect the terminals, the mesh networks are less vulnerable to the network failures.   WDM   technologyhas provided anew domain for routing in photonic networks by allowing multiple signals to be transmitted through the same ﬁber.The new generation of the photonic networks utilizes this extra functionalityand makes them superior to the traditional networks. Add/Drop multiplexers and optical crossconnects are the two key network elements for the new generation of the photonic networks. These elements are discussed below.  Add/Drop Multiplexer In WDM photonic networks, each ﬁber carries multiple optical signals on different wavelengths. The ﬁber that connects two nodes carries the data that might be intended for the receiving node as well as the data for subsequent nodes. If special provisions are not made, each node must detect and retransmit all of the wavelengths to receivethe data that was intended for the local terminals. Clearly,this scenario decreases                               FIGURE 3.11 Aself-healing ring network.3 -22                                  Broadcasting and Optical Communication Technology  network efﬁciency and increases cost. Amore intelligent design would be to demultiplex all the wavelengths, but only detect/transmit the wavelength intended for that node. An optical add/drop multiplexer (OADM) is a network element that provides this functionality[9]. Figure3.12 shows asimple architecture of an OADM. The wavelengths that are not dropped experienceatransparent node and are directed to the output port.    The data on l i that is intended for the local terminals is dropped. Newdata is transmitted on l i and is added to the ﬁber.   The problemwith aﬁxedOADM  is that it does not allow reconﬁguration after deployment. This imposes a limitation on the network design. It is possible to avoid this limitation by using 2 · 2optical switches on each wavelength path between the MUX and DEMUX (reconﬁgurable OADM). This way, it is possible to choose the desiredwavelengths to drop.Tunable transmitters and receivers can be used to reduce the number of transmitters and receivers at each node.  Optical  Crossconnects In large and complex mesh networks, morethan one ﬁber mayarriveateach node. Each ﬁber carries multiple wavelengths that mightbeintended for different destinations. Optical crossconnects (OXC)are network elements that allowany input ﬁber wavelength to be switched to anyoutput ﬁber [10]. Figure3.13 shows the architecture of an OXC.   The optical switch can connect anywavelength of the input ﬁbers to the desired output.                           FIGURE 3.12 Aﬁxedoptical add/drop multiplexer.                                 FIGURE 3.13 Optical crossconnect.Optical Communication                                                              3 -23    The issue of contention is anoticeable problem with the OXC. Forexample, if two similar wavelengths from twodifferent input ﬁbers contain two packets that are destined to the same output ﬁber,itisimpossible to transmit two optical signals with the same wavelengths on the same ﬁber.Note, the WDM allows the transmission of multiple optical signals on the same ﬁber only if they havedifferent wavelengths. This issue causes output-portcontention in photonic networks, which willbediscussed later.  Passive Optical Networks The signiﬁcantly large bandwidth of optical ﬁbers makes them agood transmission media for highcapacity MANs and WANs. As the demand for bandwidth increases, it may be necessarytobring the ﬁber closer to the end users. Fiber-To-The-Curb(FTTC)and Fiber-To-The-Home (FTTH) are network architectures using optical ﬁbers in access networks. The idea is to transmit digital data from the Central Ofﬁce(CO) to the Optical Network Units (ONUs) using optical ﬁbers and passiveoptical componentssuch as star couplers and WDM multiplexers. The passive N · N star coupler has N single-mode ﬁber inputs and N outputs [2]. In an ideal passive star,asignal incident on anyinput is divided equally among all the outputs, i.e., the star broadcasts everyinput to every output. Sinceonly passive optical componentsare used in these networkarchitectures, they are called Passive Optical Networks(PONs). PONs are generally reliable, simple, and cost-effective.The most common PON architecture broadcasts the data from CO to the ONUsusing apassivestar coupler.Sincethe network is transparent to the optical signals, data from all ONUscan be transmitted to the CO using the same network. There are provisions to use WDM technologies in PONs to increase the capacityofthe network [7,11]. As an example, WDM multiplexers may be used to divide or combine the wavelengths of different ONUs.  Protocols of Photonic Networks The synchronousoptical network (SONET)and acompatible European version, SDH (SynchronousDigital Hierarchy), are standards that are designed to simplify time division multiplexing in highbit-rate networks and to take advantage of highcapacity of optical links [8,12]. The lowest bit-rate for SONET is 51.84 Mbit/s and is called Synchronous TransportSignal level-1 (STS-1) or Optical Carrier level-1 (OC-1). Higher bit-rates at STS-n are generated by interleaving n synchronized STS-1 signals. Forexample, STS-192 or OC-192 corresponds to 9.95328 Gbit/s.   The gradual shift of the network trafﬁc from voice to data heralded new standards to moreefﬁciently control the ‘‘bursty’’nature of the trafﬁc. Asynchronous Transfer Mode (ATM) is an ITU-T standard that was developed for packet transfers [8]. The data is broken into ﬁxed size packets (53 bytes) or cells. ATMwas designed with the capabilitytotransfer both voiceand data trafﬁc. Therefore, the size of the packet is relatively shorttoreducethe delayinvoice networks. ATMalso provides different quality-of-service (QoS) for different types of trafﬁc. ATMcells maybetransmitted asynchronously or synchronously on top of SONET frames (Figure 3.14).   Internet Protocol(IP) is the most widely used protocol for the Internet [8]. IP convertsthe data into variable-size packets and adds aheader,which includes the source and destination addresses, and error control information. IP packets maybebroken into several ATMcells that could be transferred on aSONET network. In general, IP offers no guaranteed QoS. Instead, it uses abest-effortapproach to transmit the packets to their destinations. In 1998, Multi- protocol Label Switching (MPLS) technologywas introduced to improve the switching efﬁciency in WDM networks and provide some QoS guarantees [13]. MPLS can provide a label-switched path (LSP) in the network that can be used to FIGURE 3.14 (a) An ATMcell comprised of ﬂag, establish an end-to-end path between twostations. It also has header,and data ﬁelds; (b) an ATMcell placed on some measures for protection and failure detection. aSONET frame.3 -24                                  Broadcasting and Optical Communication Technology    Ethernet networks are based on carrier sense multiple access with collision detection (CSMA/CD) and operate at 10 Mbit/s to connect users on acopper bus. Fast Ethernets, operating at 100 Mbit/s and higher,use optical ﬁbers. 100BASE-FX is aﬁber-based Fast Ethernet standardthat supports anetwork span of up to 400 meters. Several standards havealso been introduced to implement Gigabit Ethernet (e.g., 1000BASE-SX and 1000BASE-LX) and  10-Gbit/s Ethernet (10GBASE-S, 10GBASE-L, 10GBASE-E, and 10GBASE-LX4), depending on the maximum reachand the operation wavelength (850, 1310, or 1550 nm) utilizing the large capacityofoptical ﬁber [8].  Circuit, Packet, and  Burst Switching Acommunication  network provides communication routes between manystations through intermediate nodes. These routes are made by accurately setting the switching devices at each node. There are two general approaches for setting the switches. In acircuit switching network, the status of the intermediate switches remains unchanged during the connection.Asaresult, adedicated path with aspeciﬁc bandwidth is established between the source and destination. Circuit switching networks set up the path before each data transmission and disconnect it afterwards. Onceaconnection is established, some partofthe network resources is allocated to the connection irrespective of the actual data transmission [8]. Public telephone networks use circuit switching to establish acall between twoclients.   In apacket switching network, the data transmitted by the sourceisbroken into several packets. Each packet contains aportion of the original data as well as the destination address. Apacket switching network transmits the packets throughdifferent nodes depending on the availability of resources (bandwidth) at each node. Packet switching networks do not requirethe establishment and disconnection of circuits, but each packet must carryadditional header information that includes the destination address. The header information may be sent as additional bits at the beginning of the packet, on asubcarrier placed outside the spectrum of the transmitted data, or on adifferent wavelength reservedfor this purpose. Figure3.15 shows the schematic diagram of circuit and packet switching networks.   In the past several years, data trafﬁc has been growing at amuch faster rate than voicetrafﬁc. Data trafﬁc is burstyinnature,and reserving bandwidth for burstytrafﬁc could be very inefﬁcient. Therefore,circuit switching networks are not optimized for this type of trafﬁc. However,packet switching networks can handle burstytrafﬁc moreefﬁciently throughstatistical time multiplexing of the packets.   In anypacket switching network, thereisapossibilitythat twoincoming packets, coming from different sources and intended for the same output port, will arrive at anode at the same time, causing contention. If contention is not resolved, some packets may be dropped and neverarrive at their destinations. Packet-loss- probabilityisthe ﬁgure of merit in evaluating contention resolution techniques. Conventional routers resolve contention by buffering the incomingpackets until the associated output portisavailable.   To facilitate the increased performanceofpacket switching networks, O/E and E/O conversions must be avoided and the signals retained in the optical domain at all times. An all-optical photonic network avoids the bottleneck of electronic components and the cost of O/E and E/O conversions. In principle, all-optical channels would offer connectivityindependent of data-rate and format. Currently, there is signiﬁcant research   FIGURE 3.15 In acircuit switching network, apath is reserved between two end terminals during the data transmission. In apacket switching network, packets coming from different sources may share the same path.Optical Communication                                                              3 -25  and efforttodesign and implement all-optical packet switching (OPS) networks. OPS networks use optical switches that are transparent to the optical signals. One of the most challenging issues in building an OPS network is the issue of contention. Unfortunately,there is no existing optical equivalent to electronicmemory that can buffer packets if necessary. In addition, OPS networks require acomplex processing unit and fast optical switches [14].   Optical burst switching (OBS) was introduced with avision to reduce the processingrequired for switching at each node and to avoidoptical buffering [15]. Conceptually,OBS is different fromOPS in the following ways:       * Burst size granularity, which is somewherebetween packet switching and circuit switching      * Switching period, which is typically on the orderofseveral packets in OBS      * Separation of header information and payload      * Reservation scheme to transmit the bursts      * Variable burst length   Similarly to OPS, OBS takesadvantage of time-domain statistical multiplexing to use asingle channel’s bandwidth to transmit several lower-bandwidth burstychannels. At the network edge, packets are gathered to generate bursts that are sent over the network core. In almost all OBS schemes, the header is sent separately from the payload with an offset time that is dependent on the scheme [16,17]. The header is sent to the switches, and the path is then reserved for the payload that follows. The loose coupling between the control packet and the burst alleviates the need for optical buffering and relaxes the requirement for switching speed.  Enabling Switching   Technologies The conventional networks withoptical links replacing copper will havehigher throughputs because of the increased bandwidth of the transmission medium. However,aninnovative improvement in throughput to terabit-per-second levels with gigabit-per-second access requires anew approach to the physical connectivity, architecture,and access protocols. Most of the photonic technologies can be used in lightwavetransmission systems to provide physical connectivity, but devices with new functionalities are needed to implement the proposed architectures. At the same time, withnew components functionalities, we can create new architectures. We havealready discussed OADM and OXCasnetwork elements that provide new functionalities. In this section, we will discuss other important technologies that enable higher and faster photonic networks.  Optical Switches One of the most important elements of anetwork is the switch that controls routing.Inthe ﬁrst generation of photonic networks, switching is performed by electronics. The optical signals that are approachinganode ﬁrst go through O/E conversion. The detected signal is buffered and switched in the electrical domain. After switching,the digital data goes through E/O conversion. In these architectures, high-speed electronics is required at each node, resulting in abottleneck.   Optical switches would circumvent the electronics bottleneck and allow much higher transmission bit-rates. In addition, optical switches are transparent to the wavelength, modulation format, and bit-rate. Unfortunately,optical switches are moreexpensive, slower,bulkier,and morelossy than their electrical counterparts, making the transition from electrical to optical switching challenging.Several technologies have been used to produceoptical switches. Mechanical switches are usually less expensive with arelatively low insertion loss, but they are slow (milliseconds), bulky,and not scalable. There are also additional issues related to the long term repeatabilityand reliability. Micro-Electro-Mechanical-System (MEMS) based switches are smaller and more scalable [18]. Thermo-optic switches operate based on changing the index of refraction in a Mach-Zehnder interferometer.The switching speed is approximately afew milliseconds, and the power consumption is comparatively high.   Electro-Optic switches can operate at sub-nanosecond speed [19]. Switching is performed by an electrical control signal that changes the index of refraction of Lithium Niobate (LiNbO3) in aMach–Zehnder3 -26                                  Broadcasting and Optical Communication Technology  conﬁguration. Electro-Optic switches usually have ahighinsertion loss and polarization sensitivity. Semiconductor space switches combine optical ampliﬁcation in semiconductors and interferometric effects. However,partofthe switch insertion loss can be compensated by the optical ampliﬁer gain. These switches can operate at nanosecond speed, but the cost of fabrication is relatively high.  SmartAmpliﬁers EDFAs are one of the crucial componentsofphotonic networks. They provide optical ampliﬁcation to compensate for the ﬁber loss, components’ insertion loss, and splitting loss in couplers. As the input optical powerofanEDFAincreases,  its gain reduces and the output power saturates. In reconﬁgurable photonic networks, whereoptical channels are added or dropped dynamically,the ampliﬁer gain changes in the sub-ms range, causing signiﬁcant channel power ﬂuctuation in the subsequent nodes [20]. The powerﬂuctuation may further degrade the system’s performancebyincreasing ﬁber nonlinearities [21] or signal-to-noise ratio degradations.   Several approaches for automatic gain control of EDFAs havebeen demonstrated. In one approach, the EDFAgain  is adjusted by using avoltage-controlled attenuator,orbycontrolling the pump power[22]. Afeedback signal from the output of the EDFAcontrols the pump power or attenuation.  Contention Resolution One of the biggest challenges facing OPS networks is the lack of random access optical memories required for resolving contention. In the optical regime, contention problems may be addressed in time, wavelength, or spacedomains. Figure3.16 summarizes the three contention resolution schemes.   In the time domain, an optical signal is delayedbygoing through aFiber DelayLine (FDL) [23]. It is also possible to construct aprogrammable delayline using acascade of optical switches and FDLs. There are several drawbacks in using FDLs as optical buffers. First, generating even asmall time delayrequires along length of ﬁber.For example, 200 mofﬁber will generate only 1 m sdelay. Second, FDLs are ﬁrst-in ﬁrst-out (FIFO) memories with apredetermined delaytime. Once an optical signal is sent into aﬁber delayline, it will not be accessible until it comes out of the other end.   In the spacedomain or deﬂection technique, acontended packet is redirected to an emptyoutput port rather than to its destination port[24]. This gives the network achancetosend the packet to its destination at alater time and/or viaadifferent node. The deﬂected packet usually ends up traveling alonger route to its destination. As aresult, packets mayarrive out of order at the destination, and may need to be rearranged.   FIGURE 3.16 Contention and contention resolution techniques. Assuming all packets are destined to output port‘‘C’’, P ¼ packet.Optical Communication                                                              3 -27  Deﬂection routing requires acomplicated control algo- rithm to make sure packets are not lost inside the network. Deﬂection also adds overhead to the network by artiﬁcially increasing the network load.   The third technique for contention resolution is based on wavelength conversion [25]. An ideal wavelength converter would change the wavelength of an optical signal independent of its bit-rate, data format, polariza- tion, wavelength, or powerasseen in Figure3.17. However,practical wavelength converters are far from  FIGURE 3.17 Wavelength conversion. ideal, and depending on the technique, are sensitiveto some or all of the input signal parameters. Other importantperformancemetrics for wavelength converters are alow noise ﬁgure and ahighoutput extinction ratio.  Wavelength Conversion Popular wavelength conversion devicesare based on one of three technologies [26]: gain saturation in semiconductor optical ampliﬁers, interferometric effects, and nonlinear wave-mixing.   Perhaps the simplest technique for wavelength conversion is to use gain saturation in semiconductor optical ampliﬁers (SOAs). As depicted in Figure3.18, if the input powerofasignal (pump) is large enough, it reduces the gain of the SOAbydepleting the carriers. As aresult, anyother signal (probe) simultaneously propagating throughthe SOAsees asigniﬁcantly reduced gain. This effect is called cross-gain-modulation (XGM). As the modulated pump signal drives the SOAinto saturation, the inversedata pattern of the pump is imprinted on the continuous wave (CW)probe signal. At the output, aﬁlter is used to select the wavelength-converted, bit- pattern-inverted copy of the original signal (i.e., on the probe wavelength). Sincethe phase information is lost in this process, this scheme can only be used for on-off-keying (OOK) data formats. The other drawbacks of this technique are signal-to-noise-ratio (SNR) degradation caused by the noise added by the SOA, the low extinction ratio,and the chirp induced on the output signal [27]. Slow gain recoveryalso limits the maximum bit-rate in an XGMbased wavelength converter.   Interferometric wavelength converters take advantage of the index of refraction’s dependency on the carrier density in semiconductors. As the index of refraction changes, the phase of the optical signal propagating throughthe medium changes as well. This phase modulation can easily be converted to amplitude modulation in an optical interferometer.   Nonlinear wave-mixing is the only wavelength conversion technique that makes an exact wavelength-shifted copy of the original signal (amplitude, frequency,conjugated phase). Wave-mixing can be generated in a   FIGURE 3.18 (a) Gain saturation in SOAcaused by highpower pump; (b) acontinuous waveprobe signal in inversely modulated by the pump.3 -28                                  Broadcasting and Optical Communication Technology  cascaded second-order nonlinear structure, which is referred to as difference frequency generation (DFG). LiNbO3 (Lithium Niobate) is agood example of amaterial with ahighsecond-order nonlinearity,ahigh optical bandwidth, and ahighdynamic range [28]. When the third-order nonlinearity is used to generate a new wavelength, the process is called four-wave-mixing (FWM).   Cross absorption modulation (XAM) in an electro-absorption modulator has also been used for wavelength conversion [29]. The principle is similar to XGM. However in XAM, the pump signal modulates the absorption instead of the gain of the device, and acopy of the input data is imprinted on the probe wavelength.  Summary Photonic networks can use light-paths to enable high-throughput and cost-effective data trafﬁc routing. Today’snetworks are mostly static entities, but morerapid reconﬁgurabilityisinthe prospective future. The future mayeveninvolve full optical packet switching in the optical domain, and the next several years of research will help deﬁne its futuredirections.  References  1.  J. Cai, D. Foursa, C. Davidson, Y. Cai, G. Domagala, H. Li, L. Liu, W. Patterson, A. Pilipetskii, M. Nissov,      N. Bergano,‘‘A DWDM demonstration of 3.73Tb/s over 11,000 km using 373 RZ-DPSK channels at      10Gb/s,’’ in Proc. Optical Fiber Communications Conf.,PD22, 2003.  2.  P. Ivan, Kaminow, and Tingye Li, Optical Fiber Telecommunications IV Aand B ,4th ed., Academic Press,      2002.  3.  E. Desurvire, Erbium-Doped Fiber Ampliﬁers: Principles and Applications,Wiley,New York, 1994.  4.  E. Desurvire, Erbium-Doped Fiber Ampliﬁers, Device and System Developments,Wiley,New York, 2002.  5.  J. Bromage, ‘‘Raman ampliﬁcation for ﬁber communications systems,’’ IEEE J. Lightwave Technology ,22,      79, 2004.  6.  ITU-T recommendation G.694.1, ‘‘DWDM frequency grid,’’ 2002  7.  R. Ramaswami and Kumar K. Sivarajan, Optical Networks: APractical Perspective,2nd ed., Morgan      Kaufmann, San Francisco,2002  8.  W. Stallings, Data and Computer Communications,7th ed., Pearson Prentice Hall,Upper Saddle River,      2004.  9.  M. Maier and M. Reisslein, ‘‘AWG-based metro WDM networking,’’ IEEE Communications Magazine,      42, S19, 2004. 10.  J. Lacey,‘‘Tutorial: optical cross-connect and add-drop multiplexers: technologies and applications,’’ in      Proc. Optical Fiber Communication Conference,327, 2002. 11.  S.-J.Park et al., ‘‘Fiber-to-the-home services based on wavelength-division-multiplexing passive optical      network,’’ IEEE J. Lightwave Technology ,22, 2582, 2004. 12.  ‘‘SONET Telecommunications StandardPrimer,’’Tektronix, application note, 2001. 13.  U. Black, MPLS and Label Switching Networks,2nd ed., Prentice Hall,Upper Saddle River,2002. 14.  M.J.O’Mahony, D. Simeonidou, D. Hunter,A.Tzanakaki, ‘‘The application of optical packet switching      in futurecommunication networks,’’ IEEE Communications Magazine,128, 2001. 15.  C. Qiao,‘‘Labeled optical burst switching for IP-over-WDM integration,’’ IEEE Communications      Magazine,104, 2000. 16.  M. Jeong,H.C. Cankaya, and C. Qiao,‘‘On anew multicasting approach in optical burst switched      networks,’’ IEEE Communications Magazine,96, 2002. 17.  I. Baldine et al., ‘‘JumpStart: Ajust-in-time signaling architecturefor WDM burst-switched networks,’’      IEEE Communications Magazine,82, 2002. 18.  T-W. Yeow,K.L.E. Law, and A. Goldenberg, ‘‘MEMS optical switches,’’ IEEE Communications Magazine,      39, 158, 2001.Optical Communication                                                              3 -29  19.  R. Krahenbuhl et al., ‘‘Performanceand modeling of advanced Ti:LiNbO3 digital optical switches,’’ IEEE      J. Lightwave Technology ,20, 92, 2002. 20.  A.K. Srivastava et al., ‘‘Signal powertransients in optically ampliﬁed WDM ring networks,’’ in Proc.      Optical Fiber Communication Conf.,164, 1998. 21.  M.I. Hayee and A.E. Willner,‘‘Transmission penalties due to EDFAgain transients in add-drop      multiplexed WDM networks,’’ IEEE Photonics TechnologyLetters,11, 889, 1999. 22.  C. Tian and S. Kinoshita, ‘‘Analysis and control of transient dynamics of EDFApumped by 1480- and      980-nm lasers,’’ J. Lightwave Technology ,21, 1728, 2003. 23.  I. Chlamtac, A. Fumagalli, and S. Chang-Jin, ‘‘Multibuffer delayline architectures for efﬁcient      contention resolution in optical switching nodes,’’ IEEE Transactions on Communications,48, 2089,      2000. 24.  M. Baresi et al., ‘‘Deﬂection routing effectiveness in full-optical IP packet switching networks,’’ in Proc.      IEEE International Conference on Communications,2,1360, 2003. 25.  S. Rangarajan et al., ‘‘All-optical contention resolution with wavelength conversion for asynchronous      variable-length 40 Gb/s optical packets,’’ IEEE Photonics TechnologyLetters,16, 689, 2004. 26.  J. Elmirghani and H. Mouftah, ‘‘All-optical wavelength conversion: Techniques and applications in      DWDM   networks,’’ IEEE Communications Magazine,86, 2000. 27.  T. Durhuus, B. Mikkelsen, C. Joergensen, S. LykkeDanielsen, and K.E. Stubkjaer,‘‘All-optical      wavelength conversion by semiconductor optical ampliﬁers,’’ IEEE J. Lightwave Technology ,14, 942,      1996. 28.  I. Brener,M.H. Chou, and M.M. Fejer,‘‘Efﬁcient wideband wavelength conversion using cascaded      second-order nonlinearities in LiNbo3 waveguides,’’ in Proc. Optical Fiber Communications Conf.,39,      1999. 29.  A. Hsu and S.L. Chuang,‘‘Wavelength conversion by cross-absorption modulation using an integrated      electroabsorption modulator/laser,’’in Proc. the Conference on Lasers and Electro-Optics,488, 1999.This page intentionally left blank                                                                                 4                                        Computer Networks   John N. Daigle University of Mississippi                               4.1  Computer Communication Networks...................................... 4 -1  Sarhan M. Musa                     Introduction * General Networking Concepts * Computer  Prairie View A & MUniversity       Communication Network Architecture * Local Area Networks                                     and Internets * ATMand Frame Relay * Recent Developments MatthewN.O.Sadiku                               4.2  Local Area Networks........................................................... 4 -14 Prairie View A & MUniversity                                    Introduction * Ethernet * Ring * Star * Wireless LAN Richard B. RobrockII          4.3  The Intelligent Network ...................................................... 4 -23 Bell Laboratories Inc.             AHistoryofIntelligenceinthe Network * The Intelligent                                     Network * IntelligentNetworkSystems * The CCS7 Network *  Apostolis K. Salkintzis            The Service Control Point * Data Base 800 Service *  Motorola                           Alternate Billing Services * Other Services * The Advanced                                     Intelligent Network * Back to the Future Nikos Passas                               4.4  Mobile Internet .................................................................. 4 -32 University of Athens                                    Introduction * KeyAspects of the Evolutiontoward the Stan McClellan                     Mobile Internet * Ubiquitous Mobile Internet: Combining                                    Cellular and Wireless LAN Networks * Mobility Management Hewlett Packard Company                                    Schemes for Mobile Internet * Concluding Remarks Remzi Seker                   4.5  Quality of Service in Packet-Switched Networks ..................... 4 -50  University of Arkansas at          Introduction and Background * StructureofDiscussion * Transport  Little Rock                        Mechanisms * Routing * Access Mechanisms * Conclusion   4.1    Computer Communication Networks John N. Daigle Introduction Overthe last decade computer communication networks haveevolved from the domain of research and business tools for the few,into the mainstream of public life. We have seen continued explosivegrowthofthe Internet, aworld-wide interconnection of computer communication networks that allows low-latency person- to-person communications on aglobal basis. Advancements in ﬁrewall technologyhavemade it possible to conduct business using the Internet with signiﬁcantly-reduced fear of compromise of important private information, and broad deployment of the World Wide Web ,orsimply the Web .Technologyhas facilitated the creation of new network-based businesses that span the globe. Signiﬁcant steps havebeen taken to extend networking services to the mobile consumer.Perhaps more signiﬁcantly,the introduction of highspeed Internet access to homes and the introduction of the point-to-point protocol (PPP) have made it possible to extend the Internet to everyhome that has at least abasic twisted-pair telephone line.   The potential for using networking technologyasavehicle for delivering multimedia voice, data, image, video presentations, multiparty,and multimedia conferencing serviceshas been demonstrated, and the                                                                                      4 -14 -2                                   Broadcasting and Optical Communication Technology  important problems that must be solved in ordertorealize this potential are rapidly being deﬁned and focused upon. User-friendly applications that facilitate navigation within the Web havebeen developed and made available to networking users on anon-fee basis vianetwork servers, thus facilitating virtually instantaneous search and retrieval of information on aglobal basis. Forexample, it is now common for users to access broadcasts of exciting events, such as Le Tour de France,inreal time over the Internet using only adial-up telephone connection.Access to details concerning events of all kinds to all people is unprecedentedinhistory.   By deﬁnition, a computer communication network is acollection of applications hosted on different machines and interconnected by an infrastructure that provides communications among the communicating entities. While the applications are generally understood to be computer programs, the generic model includes the human being as an application, an example being the people involved in atelephone call.   This article summarizes the major characteristics of computer communication networks. Our objective is to provide aconcise introduction that will allowthe reader to gain an understanding of the key distinguishing characteristics of the major classes of networks that exist today and some of the issues involved in the introduction of emerging technologies.   There are asigniﬁcant number of well-recognized books in this area. Among these are the excellent texts by Schwartz [1987] and Spragins [1991], and morerecently Kurose and Ross [2001], which haveenjoyed wide acceptance both by students and practicing engineers and cover most of the general aspects of computer communication networks. Other books that havebeen found to be especially useful by manypractitioners are those by Rose [1990], Black [1991], and Bertsekas and Gallager [1994.]   The latest developments are, of course, covered in the current literature, conferenceproceedings, formal standards documents, and the notes of standards meetings. Apedagogically oriented magazine that specializes in computer communications networks is IEEE Network,but in addition, IEEE Communications and IEEE Computer often contain interesting articles in this area. ACMCommunications Review,inaddition to presenting pedagogically oriented articles, often presents very useful summaries of the latest standards activities. Major conferences that specialize in computer communications include the IEEE INFOCOM and ACMSIGCOMM     series, which are held annually.Itisbecoming common at this time to havemore and more discussion about personal communication systems, and the mobilityissues involved in communication networks are often discussed in IEEE Network and IEEE Personal Communication Systems.There are also numerous journals, magazines, and conferences specializing in subsets of computer communications technologies.   We begin our discussion with abrief statement of howcomputer networking came about and acapsule description of the networks that resulted from the early efforts. Networks of this generic class, called wide area networks (WANs) are broadly deployed today and thereare still alarge number of unanswered questions with respect to their design. The issues involved in the design of those networks are basic to the design of most networks, whether wide areaorotherwise. In the process of introducing these early systems, we describe and contrast three basic types of communication switching: circuit, message, and packet.   We next turn to adiscussion of computer communication architecture,which describes the structureof communication oriented processing software within acommunication processing system. We introduce the International Standards Organization/Open Systems Interconnection (ISO/OSI) reference model (ISORM). Initially,the ISORM model was intended to guide the development of protocols, but its most important contribution seems to havebeen the provision of aframework for discussion of issues and developments across the communications ﬁeld in general and communication networking in particular.Wethen present the Internet reference model, which parallels the Internet protocol stack that has evolved, and we base our discussion of protocols on protocols selected from the Internet protocol suite. This discussion is necessarily simpliﬁed in the extreme, thorough coverage requiring on the order of hundreds of pages, but we hope our brief description will enable the reader to appreciate some of the issues.   Having introduced the basic architectural structure of communication networks, we next turn to a discussion of an important variation on this architectural scheme: the local area network (LAN). Discussion of this topic is important because it helps to illustrate what areference model is and what it is not. Speciﬁcally,early network architectures anticipate networks in which individual node pairs areComputer Networks                                                                   4 -3  interconnected viaasingle link, and connectionsthrough the network are formed by concatenating node- to-node connections.   LAN architectures, on the other hand, anticipate all nodes being interconnected in some fashion over the same communication link (or medium). This, then, introduces the concept of adaption layers in anatural way. It also illustrates that if the servicesprovided by an architectural layer are carefully deﬁned, then the services can be used to implement virtually anyservice desiredbythe user,possibly at the price of some inefﬁciency.   Next we discuss ATMand Frame relay,two importantlink layer technologies. We conclude with abrief discussion of recent developments.  General Networking Concepts Data communication networks haveexisted since about 1950. The early networks existed primarily for the purpose of connecting users of alarge computer to the computer itself, with the additional capabilityto provide communications between computers of the same varietyand having the same operating software.The lessons learned during the ﬁrst 20 or so years of operation of these types of networks havebeen valuable in preparing the way for modern networks. For the purposes of our current discussion, however,wethink of communication networks as being networks whose purpose is to interconnect aset of applications that are implemented on hosts manufactured by possibly different vendorsand managed by avarietyofoperating systems. Networking capabilityisprovided by software systems that implement standardized interfaces speciﬁcally designed for the exchange of information among heterogeneouscomputers.   The earliest efforttodevelop large-scale, general purpose networking capabilitybased on packet switching was lead by the Advanced Research Projects Agency (ARPA) of the Department of the Army in the late 1960s; this effortresulted in the computer communication network called the ARPANET.The end results of the ARPAnetworking effort, its derivatives, and the early initiatives of manycompanies such as AT&T, DATAPOINT,DEC, IBM, and NCR havebeen far-reachinginthe extreme. We will concentrate on the most visible product of these efforts, which is acollection of programs that allows applications running in different computers to intercommunicate. Beforeturning to our discussion of the software,however,weshall provide a brief description of ageneric computer communication network.   Figure4.1 shows adiagram of ageneric computer communication network. The most visible components of the network are the terminals,the access lines,the trunks,and the switching nodes.Work is accomplished when the users of the network, the terminals, exchange messages over the network.   The terminals, which are usually referred to as end systems, represent the set of communication terminating equipment communicating over the network. Equipment in this class includes, but is not limited to,user terminals, general purpose computers, and database systems. This equipment, either through software or throughhuman interaction, provides the functions required for information exchange between pairs of application programs or between application programs and people. The functions include, but are not limited to,call setup,session management, and message transmission control. Examples of applications include electronic mail transfer,www browsing,and playbackofaudio streams. An extensivediscussion of applications is provided in Kurose and Ross [2000].   Access lines provide for data transmission between the terminals and the network switching nodes. These connections may be set up on apermanent basis or they may be switched connections, and there are numerous transmission schemes and protocols available to manage these connections. The essence of these connections, from our point of view,isachannel that provides data transmission at some number of bits per second (bps), called the channel capacity, ( C ). The access line capacities may range from afew hundred bps to in excess of millions of bps, and they are usually not the same for all terminating equipment of agiven network. The actual information carrying capacity of the link depends upon the protocols employed to effect the transfer;the interested reader is referred to Bertsekas and Gallagher [1987], especially Chapter 2, for a general discussion of the issues involved in transmission of data over communication links.   Trunks, or internodal trunks, are the transmission facilities that provide for transmission of data between pairs of communication switches. These are analogous to access lines, and from our point of view,they simply provide acommunication path at some capacity,speciﬁed in bps.4 -4                                   Broadcasting and Optical Communication Technology                       Access Lines                      Trunks                      Terminals                      Nodes                         FIGURE 4.1 Generic computer communication network.    There are threebasic switching paradigms: circuit, message, and packet switching. Circuit switching and packet switching are transmission technologies while message switching is aservice technology. In circuit switching,acall connection between two terminating pieces of equipment corresponds to the allocation of a prescribed set of physical facilities that provide atransmission path of acertain bandwidth or transmission capacity. These facilities are dedicated to the users for the duration of the call. The primaryperformance issues, other than those related to qualityoftransmission, are related to whether or not atransmission path is available at call setup time and how calls are handled if facilities are not available.   Message switching is similar in concept to the postal system. When auser wants to send amessage to one or more recipients, the user forms the message and addresses it. The message switching system readsthe address and forwards the complete message to the next switch in the path. The message moves asynchronously through the network on amessage switch to message switch basis until it reaches its destination. Message switching systems offer servicessuch as mail boxes, multiple destination delivery,automatic veriﬁcation of message delivery, and bulletin boards. Communication links between the message switches maybeestablished using circuit or packet switching networks as is the case withmost other networking applications. An example of amessage switching protocol that has been used to build message switching systems is the Simple Mail Transfer Protocol (SMTP).   In the circuit switching case, there is aone-to-one correspondence between the number of trunks between nodes and the number of simultaneous calls that can be carried. That is, atrunk is afacility between twoswitches that can service exactly one call, and it does not matter how this transmission facility is derived. Major design issues include the speciﬁcation of the number of trunks between node pairs and the routingstrategyused to determine the path through anetwork in ordertoachieve agiven call blocking probability.When blocked calls are queued, the number of calls that maybequeued is also a design question.   Apacket switched communication system exchanges messages among users by transmitting sequences of packets comprising the messages. That is, the sending terminal equipment partitions amessage into a sequence of packets, the packets are transmitted across the network, and the receiving terminal equipment reassembles the packets into messages. The transmission facilityinterconnecting agiven node pair is viewed as asingle trunk, and the transmission capacityofthis trunk is shared among all users whose packets traverseComputer Networks                                                                   4 -5  both nodes. While the trunk capacityisspeciﬁed in bps, the packet handling capacity of anode pair depends both upon the trunk capacityand the nodal processing power.   In some packet switched networks, the path traversed by apacket through the network is established during acall set-up procedure, and the network is referred to as avirtual circuit packet switching network. Other networks provide datagram service, aservice that allows users to transmit individually addressed packets without the need for call set-up.Datagram networks havethe advantage of not having to establish connections beforecommunications takes place, but havethe disadvantage that everypacket must contain complete addressing information. Virtual circuit networks havethe advantage that addressing information is not required in each packet, but havethe disadvantage that acall set-up must take place beforecommunications can occur.Datagram is an example of connectionless service while virtual circuit is an example of connection-oriented service.   Prior to the late 1970s, signaling for circuit establishment was in-band. That is, in order to set up acall throughthe network, the call set-up information was sent sequentially from switch to switch using the actual circuit that would eventuallybecome the circuit used to connect the end users. In an extreme case, this amounted to trying to ﬁnd apath throughamaze, sometimes having to retrace one’ssteps beforeﬁnally emerging at the destination or just simply giving up when no path could be found. This had two negative characteristics: ﬁrst, the rate of signaling information transfer was limited to the circuit speed, and second, the circuits that could havebeen used for accomplishing the end objective werebeing consumed simply to ﬁnd a path between the end-points. This resulted in tremendous bottlenecks on major holidays, which were solved by virtually disallowing alternate routes through the toll switching network.   An alternate out-of-band signaling system, usually called common channel interofﬁce signaling (CCIS), was developed primarily to solvethis problem. Signaling now takes placeoverasignaling network that is partitioned from the network that carries the user trafﬁc. This principle is incorporated into the concept of integrated servicesdigital networks (ISDNs), which is described thoroughly in Helgert [1991]. The basic idea of ISDN is to offer to the user some number of 64 Kbps access lines plus a16Kbps access line throughwhich the user can describe to an ISDN how the user wishes to use each of the 64 Kbps circuitsatany given time. The channels formed by concatenating the access lines with the network inter-switch trunks having the requested characteristics are established using an out-of-band signaling system, the most modern of which is Signaling System #7 (SS#7).   In either virtual circuit or datagram networks ,packets from alarge number of users maysimultaneously need transmission services between nodes. Packetsarrivetoagiven node at random times. The switching node determines the next node in the transmission path, and then places the packet in aqueue for transmission over atrunk facilitytothe next node. Packet arrival processes tend to be bursty, that is, the number of packet arrivals over ﬁxed-length intervals of time has alarge variance.Because of the ‘‘burstiness’’ of the arrival process, packets mayexperience signiﬁcant delays at the trunks. Queues may also build due to the differencein transmission capacities of the various trunks and access lines and delays result. Processing is also asource of delay, and the essence of packet switching technologyistotrade delayfor efﬁciency in resource utilization.   Protocoldesign efforts, which seek to improve network efﬁciencies and application performance, are frequent topics of discussion at both general conferences in communications and those specialized to networking.The reader is encouraged to consult the proceedings of the conferences mentioned earlier for a better appreciation of the range of issues and the diversity of the proposed solutions to the issues.  Computer Communication Network Architecture In this section, we begin with abrief, highlevel, deﬁnition of the ISORM, which is discussed in signiﬁcant detail in Black [1991]. Then we turn to amoredetailed discussion of apractical Internet reference model (PIRM). The ISORM has seven layers, none of which can be bypassed conceptually.Ingeneral, alayer is deﬁned by the types of servicesitprovides to its users and the qualityofthose services. Foreach layer in the ISO/OSI architecture,the user of alayer is the next layer up in the hierarchy, except for the highest layer for which the user is an application. Clearly,when alayered architecture is implemented under this philosophythe4 -6                                   Broadcasting and Optical Communication Technology               Application A                                     Application B                                  Peer-to-Peer Communications          7. Application Layer                                7. Application Layer           6. Presentation Layer                              6. Presentation Layer            5. Session Layer                                    5. Session Layer            4. Transport Layer                                 4. Transport Layer                                        Node C            3. Network Layer           3. Network Layer         3. Network Layer            2. Data Link Layer        2. Data Link Layer       2. Data Link Layer            1. Physical Layer          1. Physical Layer        1. Physical Layer                                 End-to-End Communications Path                               FIGURE 4.2 The ISO reference model.  qualityofservice obtained by the end user,the application, is afunction of the qualityofservice provided by all of the layers.   Figure4.2, adapted from Spragins [1991], shows the basic structureofthe OSI architecture and howthis architecture is envisaged to provide for exchange of information between applications. As shown in the ﬁgure, thereare seven layers: application, presentation, session, transport, network, data link, and physical. Brief deﬁnitions of the layers are nowgiven, but the reader should bear in mind that substantial further study will be required to develop an understanding of the practical implications of the deﬁnitions.   Physical layer: Provides electrical, functional, and procedural characteristics to activate, maintain, and deactivate physical data links that transparently pass the bit stream for communication between data link entities.   Data link layer: Provides functional and procedural means to transfer data between networkentities; provides for activation, maintenance, and deactivation of data link connections, character and frame synchronization, grouping of bits into characters and frames, error control, media access control, and ﬂowcontrol.   Network layer:Provides switching and routingfunctions to establish, maintain, and terminate networklayer connections and transfer data between transportlayers.   Transport layer:Provides host-to-host, cost effective, transparent transfer of data, end-to-end ﬂow control, and end-to-end qualityofservice as required by applications.   Session layer:Provides mechanisms for organizing and structuring dialogues between application processes.   Presentation layer:Provides for independent data representation and syntax selection by each communicating application and conversion between selected contextsand the internal architecture standard.   Application layer:Provides applications with access to the ISO/OSI communication stack and certain distributed information services.   As we havementioned previously,alayer is deﬁned by the types of servicesitprovides to its users. In the case of arequest or aresponse, these servicesare provided viainvocation of service primitives of the layer in question by the layer that wants the service performed. In the case of an indication or aconﬁrm, these services are provided viainvocation of service primitives of the layer in question by the same layer that wants the service performed.Computer Networks                                                                   4 -7    The process of invoking aservice primitive is not unlikeauser of aprogramming system calling a subroutine from ascientiﬁc subroutine package in ordertoobtain aservice, say, matrix inversion or memory allocation. For example, arequest is analogous to aCALL statement in aFORTRAN program, and aresponse is analogous to the RETURN statement in the subroutine that has been called. The requests for services are generated asynchronously by all of the users of all of the servicesand these join (typically prioritized) queues along with other requests and responses while awaiting servicing by the processor or other resource such as a transmission line.   The service primitives fall into four basic types, which are as follows: request, indication, response, and conﬁrm. These types are deﬁned as follows:   Request: Aprimitivesent by layer ( N 1 1) to layer N to request aservice.   Indication: Aprimitivesent by layer N to layer ( N 1 1) to indicate that aservice has been requested of layer N by adifferent layer ( N 1 1) entity.   Response:Aprimitivesent by ( N 1 1) to layer N in response to an indication primitive.   Conﬁrm: Aprimitivesent by layer N to layer ( N 1 1) to indicate that aresponse to an earlier request primitivehas been received.   To understand the basic ideas of anetwork, it is useful to ﬁrst ask the question ‘‘What functionality would be needed in apoint-to-point connection between two computers that are hard-wired together?’’Wewould ﬁnd that we need everything except the functions provided by layer 3; thereisnoneed for switching and routing functions. Thus, it is natural to think of the networking aspect of acomputer communication network as the interconnection of layer 3entities. In fact, we can think of the network as the collection of layer 3 entities. Everything abovelayer 3facilitates activities that are taking placeoverthe network and everything belowlayer 3isthere simply for the purpose of connectingthe layer 3entities together.   As apractical matter,networking has evolved such that the morerealistic reference model has ﬁve,rather than seven, layers. In essence, the session and presentation layers are absorbed into the application layer,with the resulting referencemodel as shown in Figure4.3.   In ordertobemorespeciﬁc about how communications takes place, we now turn to abrief discussion of Internet Protocol (IP), which is the network layer protocol of the Internet. The IP protocol provides a datagram delivery service to the transport layer.Inordertodothis, IP provides exactly one primitivetoits upper layer protocol (ULP): SEND.   As mentioned earlier,anupper layer protocol issuing aprimitive is equivalent to acomputer program calling asubroutine or procedure. A primitive has an associated set of formal parameters that are analogous to               Application AApplication                                   B                                   Peer-to-Peer Communications           5. Application Layer                               5. Application Layer             4. Transport Layer                                 4. Transport Layer                                        Node C             3. Network Layer          3. Network Layer         3. Network Layer             2. Data Link Layer        2. Data Link Layer       2. Data Link Layer             1. Physical Layer         1. Physical Layer        1. Physical Layer                                 End-to-End Communications Path                            FIGURE 4.3 Practical internet reference model.4 -8                                   Broadcasting and Optical Communication Technology  the formal parameters of aprocedureinaprogramming language. In this case, the parameters for the SEND.request primitiveare the source address,the destination address, the type-of-service parameter set, the data length, the options parameters, and the data. These parameters provide all the information needed by IP to deliverdata to the required destination. The data itself is the entitythat the ULP wants sent. The source address is the Internet address of the ULP requesting the SEND service and the destination address is the Internet address of the ULP to which the data are to be sent. The type of service and options parameters give IP information about howthe delivery is to be handled; for example, what is the priorityofthe send action? The data length parameter tells IP how much data it has to send.   In general, communications takes placebetween peer layer protocols by the exchange of protocoldata units (PDUs), which contain all of the information required for the receiving protocol entity to provide the required service.For the PIRM, we saythat applications exchange messages,layer-4 entities exchange segments, layer-3 entities exchange datagrams,layer-2 entities exchange frames,and layer-1 protocols exchange 1-PDUs . It should be mentioned that datagrams are packets that traverse networks using connectionless layer-3 protocols. Equivalently,wecan saythat the packet-switched Internet is adatagram network because IP is the only layer-3 protocol, and it provides connectionless service.   In ordertoexchange PDUs, entities of agiven layer use the servicesofthe next lowerlayer.Tomake things concrete, assume the ULP sending the information is aTransmission Control Protocol (TCP) entity in Host Aand the ULP to which the information is being sent is the TCPentityinHost B. In the case of the SEND  discussed previously,the TCPentityinHost Aisusing the SEND service to send the information in the data ﬁeld to its peer destination TCPentityinHost B; the content of this data ﬁeld is the PDU and it is called asegment sinceTCP is alayer-4 protocol. At the destination, Host B, IP uses its DELIVER primitive to deliver the packet to the TCPentityinHost B. Thus, the TCP’spacket is the data contained in the data ﬁeld of its SEND request. The destination TCPentitymust be able to determine what to do with anypacket it receives. Therefore, the PDUs, or packets, must be structured in astandardway so that acomputer program can examine the data received and take the appropriate actions. The structurethat must be adhered to is speciﬁed in the protocol standardfor the protocol. Forexample, TCPisspeciﬁed in Request for Comments  RFC 793.   Similarly,the IP entityinHost Aforms aPDU that is sent to its peer IP entityinanother mode. The IP-layer PDU,which is called a datagram sinceIPisalayer-3 protocol, is formed by using information provided in the service request issued by the TCPentityand other information available in the IP layer entityitself. Instructive information is packed into the header of the IP datagram while the data part, or sometimes asubset of the data part, of the TCPrequest forms the data partofthe IP datagram.   Some protocols are connection-oriented and some are connectionless, meaning that the service they provide is connection-oriented or connectionless. For example, IP provides aconnectionless service while TCPis connection-oriented. For connection mode communications, aconnection must be established between two peer entities beforethey can exchange PDUs. Therefore, connection-oriented protocols must provide primitives that facilitate establishment of aconnection. For example, TCPhas several forms of OPEN primitives that an application can use to initiate connectionsand TCPalso has CLOSE and ABORTprimitives that an application can use to terminate connections.   Onceaconnection is established, data exchange between the two application layer entities can take place; that is, the entities can exchange PDUs. For example, if an application layer entityinHostAwishes to send a PDU  to an application layer entityinHost B, the application layer entityinHost Awould issue a T_SEND.request to the appropriate transportlink layer entityinHost A. This entitywould package the PDU together with appropriate control information into atransportservice data unit (TSDU) and send it to its peer transport layer entityinHost B. In ordertodothis, the transportlayer would invoke the services of the network layer.Inturn, the network layer would invoke the servicesofthe data link layer and so on. Speciﬁcally,the network layer entityinHost Awould issue aDL_DATA.request to the appropriate data link layer entityinHost A. This entitywould package the PDU together with appropriate control information into adata link service data unit (DLSDU) and send it to its peer at C. The peer data link entityat Cwould extract the layer-3 PDU,deliverittothe network entityatC,which would forward it to the data link entityinC,providing the connection to Host B. This entitywould then send the DLSDU to its peer in Host B,Computer Networks                                                                   4 -9  and this data link entitywould extract the layer-3 PDU and pass it to the Host Bnetwork entityvia aDL_DATA.indication. The receiving network layer entitywould extract the SDU and forward it to the transport layer in Host Cusing the N_Data.indication.   Now, network layer PDUsare called packets and DL layer PDUsare called frames. But the data link layer does not know that the information it is transmitting is apacket; to the DL layer entity, the packet is simply user information. From the perspective of adata link entity, it is not necessarytohaveanetwork layer. The networklayer exists to add value for the user of the network layer to the servicesprovided by the DL layer. In the example above,value was added by the network layer by providing arelaying capabilitysinceHosts A and Cwere not directly connected. Similarly,the DL layer functions on ahop-by-hop basis, each hop being completely unaware that thereare anyother hops involved in the communication. We will see later that the data link need not be limited to asingle physical connection.   We  nowturn to adiscussion of LANs, which haveinherent properties that makeuse of sublayers particularly attractive.  Local Area Networks and Internets In this section, we discuss the organization of communications software for LANs. In addition, we introduce the idea of internets, which werebrought about to alarge extent by the advent of LANs. We discuss the types of networks only brieﬂy and refer the reader to the manyexcellent texts on the subject. Layers 4and 5for local areacommunications networks are identical to those of wide area networks. However,because the hosts communicating over aLAN shareasingle physical transmission facility, the routing functions provided by the network layer,layer 3, are not necessary. Thus, the functionality of alayer 3within asingle LAN can be substantially simpliﬁed without loss of utility. On the other hand, aDLlayer entitymust nowmanage many simultaneous DL layer connectionsbecause all connections entering and leaving ahost on asingle LAN do so over asingle physical link. Thus, in the case of connection oriented communications, the softwaremust manage several virtual connectionsoverasingle physical link.   There wereseveral basic types of transmission schemes in use in early local area networks. Three of these received serious consideration for standardization: the token ring, token bus,and carrier sense multiple access (CSMA). All three of those access methods became IEEE standards, the IEEE 802 series, and eventually became ISO standards (ISO 8802 series) because all merited standardization. On the other hand, all existed for the express purposes of exchanging information among peers, and it was recognized at the outset that the upper end of the data link layer could be shared by all three access techniques. The decision to use acommon logical link control (LLC) sublayer for all of the LAN protocols and develop aseparate sublayer for each of the different media apparently ushered in the idea of adaption sublayers, in this case, the media access control (MAC) sublayer.The most dominant today is the CDMA-based version, which is known as Ethernet or IEEE 802.3.   The idea of sublayering has proventobevaluable as new types of technologies havebecome available. For example, the new ﬁber distributed digital interface (FDDI) uses the LLCofall other LAN protocols, but its MACiscompletely different from the token ring MACeventhoughFDDI is atoken ring protocol. Athorough discussion of FDDI and related technologies is given in Jain [1994].   Metropolitan areanetworks (MANs) havebeen deployed for the interconnection of LANs within a metropolitan area. The primarymedia conﬁguration for MANs is adual bus conﬁguration and it is implemented viathe distributed queue, dual bus (DQDB) protocol, also known as IEEE 802.6. The net effect of this protocol is to use the dual bus conﬁguration to provide service approaching the FCFS service discipline to the trafﬁc entering the FDDI network, which is remarkable considering that the LANs being interconnected are geographically dispersed. Interestingly,DQDB concepts haverecently also been adapted to provide wide areacommunications. Speciﬁcally,structureshavebeen deﬁned for transmitting DQDB frames over standard DS-1 (1.544 megabits per second) and DS-3 (6.312 megabits per second) facilities, and these havebeen used as the basis for aservice offering called Switched Multi-megabit Data Services (SMDS).   One of the moreinteresting consequences of the advent of local area networking is that manytraditional computer communication networks became internets overnight. LAN technologywas used to connect stations4 -10                                  Broadcasting and Optical Communication Technology  to ahost computer,and these host computers were already on aWAN. It was then asimple matter to provide a relaying,orbridging,service at the host in ordertoprovide wide area interconnection of stations on LANs to each other.Inshort, the previously established WANs became networks for interconnection of LANs; that is, they wereinterconnectingnetworks rather than stations. Internet performancesuddenly became aprimary concern in the design of networks. Anew business developed to provide specialized equipment, routers, to provide interconnection among networks.   Overthe last decade, wireless LANs, which are local area networks in which radio or photonic links serve as cable replacements, havebecome common. Indeed, wireless LAN technologyisnow widely deployed in homes as well as in ofﬁces. Most of this technologyfeaturesspread-spectrum wireless transmission with the media access technologybeing in the IEEE 802.11 family.   We nowdescribe twobroadly deployed technologies that provide link-layer connections in communication networks: frame relay (FR) technology, which is described in Braun [1994] and asynchronous transfer mode (ATM) technology, which is described in McDysan and Spohn [1994] and Pildush [2000].   As we havementioned previously,there is really no requirement that the physical media between two adjacent data link layers be composed of asingle link. In fact, if apath through the network is initially established between two data link entities, there is no reason that DLCprotocols need to be executed at intermediate nodes. Throughthe introduction of adaption layers and an elementaryrouting layer at the top of the DL layer,DLC frames can be relayed across the physical links of the connection without performing the error checking, ﬂow control, and retransmission functions of the DLClayer on alink-by-link basis. The motivation is that sincelink transmission is becomingmorereliable, extensiveerror checking and ﬂowcontrol is not needed across individual links; an end-to-end check should be sufﬁcient. Meanwhile, the savings in processing due to not processing at the network layer can be applied to frame processing,which allows interconnection of the switches at higher line speeds. Sincebit-per-second costs decrease with increased line speed, service providers can offer savings to their customers through FRNs. Signiﬁcant issues are frame loss probabilityand retransmission delay. Such factors will determine the retransmission strategydeployed in the network. The extensive deployment of FR technologyatthis time suggests that this technologyprovides improvements over standard packet technology.   Another recent innovation is the ATM. The idea of ATMistopartition auser’sdata into manysmall segments, called cells, for transmission over the network. Independent of the data’s origin, the cell size is 53 octets, of which 5octets are for use by the network itself for routing and error control. Users of the ATMare responsible for segmentation and reassembly of their data. Anycontrol information required for this purpose must be included in the 48 octets of user information in each cell. In the usual case, these cells would be transmitted over networks that would provide users with 135 Mb/s and abovedata transmission capacity(with user overhead included in the capacity).   The segmentation of units of data into cells introduces tremendousﬂexibilityfor handling different types of information, such as voice, data, image, and video,overasingle transmission facility. As aresult, there has been atremendous investment in developing implementation agreements that will enable alarge number of vendors to independently develop interoperable equipment. This effortisfocused primarily in the ATM Forum, aprivate, not-for-proﬁt consortium of over 500 companies of which morethan 150 are principal members and activecontributors.   LANs, WANs, and MANs based on the ATMparadigm are being deployed as described in Pildush [2000], for example. ATMisanexcellent technologyfor providing worldwide Ethernet LAN interconnection at afull rate of 10 or 100 megabits per second. Originally,numerous vendors wereplanning to haveATM capabilities at the backplane in much the same waythat Ethernet has been provided, but the bulk of deployment of ATM today is at the link layer between IP routers.  ATMand     Frame  Relay The asynchronoustransfer mode technologywas initially developed in hopes of extending ISDN technology towards the concept of abroadband integrated servicesdata network that would provide bandwidth on demand to end users. The ATMarchitecture consists of three sublayers: the ATMadaption layer (AAL), theComputer Networks                                                                  4 -11  ATMlayer,and the Physical Media Dependent (PMD) layer.Itisconvenient for the purposes of this discussion to think of services as falling into twocategories: circuit mode and packet mode, whereacircuit mode service, such as voice, is aservice that is naturally implemented over acircuit switched facilityand apacket-mode service, such as e-mail, is aservice that is more naturally implemented over apacket switched connection. From manyperspectives, it is natural to implement circuit-mode services directly over ATMwhile it is more natural to implement packet-mode servicesatthe Internet (or packet) layer.   The implication of this partitioning of service types is that anyservice that has been developed for deployment over an IP network would naturally be deployed over an ATMnetwork by simply using the ATM network as apacket deliverynetwork. Each packet would traverse the network as asequenceofcells over an end-to-end virtual connection. If the ﬂowcontrol and resource management procedures can be workedout, the net effect of this deployment strategywould be, for example, that an application designed to be deployed over an Ethernet segment could be deployed on anationwide(or even global) network without anoticeable performancedegradation. The implications of this type of capabilityare obvious as well as mind-boggling.  Recent Developments Broad deployment of networking technologies and wireless communications devices are the big storyin recent times.   Atypical home deployment of networking technologytoday includes arouter,which is at the interfaceofthe home network and the outside world. Typically,the router has an IEEE 802.3 (Ethernet) portofthe 100 Mb/s varietyaswell as an IEEE 802.11 wireless LAN portfacing the in-home side of the network. The connection from the home may be one of manytechnologies, ranging from atraditional relatively low-speed dialed connection over aphone line to relatively highspeed connection based on some form of digital subscriber loop (DSL) technology, such as asymmetric DSL (ADSL), which deliversservice to the home in the hundreds of kb/s to low Mb/s range. Connected together to form anetwork within ahome maybeanumber of computers and a print serverhaving an attached printer.Anumber of additional computers maybeattached to the same network viawireless IEEE 802.11 connections. The router itself is usually connected to an Internet service provider (ISP) over the point-to-point protocol (PPP). All Internet applications running on all endsystems (computers) in the home share the same PPP link-layer connection to the ISP porttothe Internet.   Many of the servicesthat network users enjoy today are the result of enhancements to the development of multimedia applications and protocols that are integrated into Webbrowsers. The Webbrowsers allowusers to access searchengines through which they can ﬁnd interesting Webcontent. Oncethe contentisfound, the user can generally access the content quickly by simply clicking on aURL. When the object arrives at the user’s system, the Webbrowser automatically invokes the services of ahelper application, which renders the object to the user.For example, in case the object is aJPEG ﬁle, the Webbrowser willinvokeanapplication that can displayapicture that is stored in JPEG format.   At the time of this writing,networking technologies are being extended to the wireless domain as discussed in Lin and Chlamtac [2001] and Garg [2002]. The intention is to makeall networking servicesavailable to mobile end systems using essentially the same infrastructure as the cellular telephone system. Alternatively stated, the intention is to evolve the wireless cellular phone system to apoint whereitisjust partofthe Internet. Meanwhile the intention is to evolvethe Internet to the point where it handles all types of communications services. Thus, over the long term it is expected that communications types be available over the Internet and that access to the Internet willbeavailable to users at anytime and place, whether the user is mobile or stationary. The end objective is Universal Personal Communications Services(UPCS) to which we all look forward.  Deﬁning   Terms Introduction Architecture: The set of protocols deﬁning acomputer communication network. Computer communication network:     Collection of applications hosted on differentmachines and      interconnected by an infrastructure that provides intercommunications.4 -12                                  Broadcasting and Optical Communication Technology  Firewall: Computer communication network hardwareand software introduced into an Internet at the      boundaryofpublic network and aprivate network for the purpose of protecting the conﬁdential      information and network reliabilityofthe private network. Local area network: Acomputer communication network spanning alimited geographic area, such as a      building or college campus. Metropolitan area network: Acomputer  communication network spanning alimited geographic area,      such as acity; sometimes features interconnection of LANs. Wide area network:  Acomputer communication network spanning abroad geographic area, such as a      state or country. World Wide Web:   Acollection of hypertext-styleservers interconnectedvia Internet services.  General Networking Concepts Access line: Acommunication line that connect auser’sterminal equipment to aswitching node. Circuit switching: Amethod  of communication in which aphysical circuit is established between two      terminating equipments beforecommunication begins to take place. This is analogous to an ordinary      phone call. Connectionless service: Amode   of packet switching in which packets are exchanged without ﬁrst      establishing aconnection. Conceptually,this is very close to message switching,except that if the      destination node is not active,then the packet is lost. Connection-oriented service: Amode  of packet switching in which acall is established prior to any      information exchange taking place. This is analogous to an ordinaryphone call, except that no physical      resources need be allocated. Common   channel interofﬁce signalling: Useofaspecial network, dedicated to signalling,toestablish a      path throughacommunication network, which is dedicated to the transfer of user information. Message switching:  Aservice-oriented class of communication in which messages are exchanged among      terminating equipments by traversing aset of switching nodes in astoreand forward manner.This is      analogous to an ordinarypostal system. The destination terminal need not be active at the same time as      the originator in orderthat the message exchange take place. Packet switching:  Amethod of communication in which messages are exchanged between terminating      equipments viathe exchange of asequence of fragments of the message called packets. Switching node:  Acomputer or computing equipment that provides access to networking services. Trunk:  Acommunication  line between twoswitching nodes.  Computer  Communication   Network Architecture Entity: Asoftware process that implements apartofaprotocol in acomputer communication. Formal parameters:  The parameters passed during the invocation of aservice primitive; similar to the      arguments passed in asubroutine call in acomputer program network. International Standards Organization Reference Model: Amodel, established by ISO,that organizes      the functions required by acompletecommunication network into seven layers. Protocol data unit: The unit of exchange of protocol information between entities. Typically,aprotocol      data unit (PDU) is analogous to astructure in CorarecordinPascal; the protocol is executed by      processing asequenceofPDUs. Service primitive: The name of aprocedurethat provides aservice; similar to the name of asubroutine or      procedureinascientiﬁc subroutine library.  Local Area Networks and Internets Adaption sublayer: Software that is added between two protocol layers to allow the upper layer to take      advantage of the servicesoffered by the lower layer in situations where the upper layer is not speciﬁcally      designed to interfacedirectly to the lowerlayer.Computer Networks                                                                  4 -13  Token bus:  Amethod of sharing abus-typecommunications medium that uses atoken to schedule access      to the medium. When aparticular station has completed its use of the token, it broadcasts the token on      the bus, and the station to which it is addressed takes control of the medium. Token ring: Amethod of sharing aring-type communications medium that uses atoken to schedule access      to the medium. When aparticular station has completed its use of the token, it transmits the tokenon      the bus, and the station that is physically next on the ring takescontrol. Carrier sense multiple access: Arandom access method of sharing aBus-typecommunication medium      in which apotential user of the medium listens beforebeginning to transmit. Media access control: Asublayer of the link layer protocol whose implementation is speciﬁc to the type of      physical medium over which communication takes placeand controls access to that medium. Internet: Anetwork formed by the interconnection of networks.  ATMand Frame Relay Asynchronous transfer mode (ATM):  Amode of communication in which communication takes place      through the exchange of tinyunits of information called cells. Broadband Integrated Services Digital Network (B-ISDN): Ageneric term that generally refers to the      future network infrastructure that will provide ubiquitous availabilityofintegrated voice, data, imagery,      and video services. Fast packet network: Networks in which packets are transferredbyswitching at the frame layer rather      than the packet layer.Suchnetworks are sometimes called frame relaynetworks. At this time, it is      becoming voguetothink of frame relay as aservice, rather than transmission, technology.  References D. Bertsekas and R. Gallagher, Data Networks,2nd ed., EnglewoodCliffs, NJ: Prentice Hall, 1987. U.D. Black, OSI: AModel for Computer Communication Standards ,Englewood Cliffs, NJ: Prentice Hall, 1991. E. Braun, The Internet Directory ,New York: Fawcett Columbine, 1994. V.K. Garg, Wireless Network Evolution: 2Gto3G ,Upper Saddle River,NJ: Prentice Hall, 2002. J.L. Hammond and P.J.P. O’Reilly. Performance Analysis of Local Computer Networks,Reading,MA: Addison-      Wesley,1986. H.J.Helgert, Integrated Services Digital Networks,Reading,MA: Addison-Wesley,1991. R. Jain, Handbook: High-Speed Networking Using Fiber and Other Media,Reading,MA: Addison-Wesley,1994. J.F. Kurose and K.W.Ross, Computer Networking: ATop–DownApproach Featuring the Internet,Reading,      MA: Addison-Wesley,2000. Y.-B. Lin and I. Chlamtac, Wireless and Mobile Network Architectures,New York: Wiley. D.E. McDysan and D.E. Spohn, ATM: Theoryand Application,New York: McGraw-Hill,1994. G.D.Pildush, Cisco ATMSolutions: Master ATMImplementation of Cisco Networks,Indianapolis, IN: Cisco      Systems Press, 2000. M. Rose, The Open Book,Englewood Cliffs, NJ: Prentice Hall,1990. M. Schwartz, Telecommunications Networks: Protocols, Modeling and Analysis,Reading,MA: Addison-Wesley,      1987. J.D. Spragins, Telecommunications: Protocols and Design,Reading,MA: Addison-Wesley,1991. W. Stallings, Handbook of Computer-Communications Standards: The Open Systems Interconnection (OSI)      Model and OSI-Related Standards,New York: Macmillan Publishing Company, 1990.  Further Information There are manyconferences and  workshops that provide up-to-date coverage in the computer communications area. Among these are the IEEE INFOCOM and ACMSIGCOMM   conferences and the IEEE Computer Communications Workshop,which are specialized to computer communications and are held annually.Inaddition, IEEE GLOBCOM (annual), IEEE ICC (annual), IFIPS ICCC (bi-annual), and the4 -14                                  Broadcasting and Optical Communication Technology  International Telecommunications Congress(bi-annual) regularly featureasubstantial number of paper and panel sessions in networking.   The ACM Communications Review,aquarterly,specializes in computer communications and often presents summaries of the latest standards activities. IEEE Network,abi-monthly,specializes in tutorially oriented articles across the entirebreadth of computer communications and includes aregular columnonbooks related to the discipline. Additionally, IEEE Communications and IEEE Computer,monthly magazines, frequently have articles on speciﬁc aspects of networking.Also,see IEEE Personal Communication Systems,aquarterly magazine, for information on wireless networking technology.   Forthose who wish to be involved in the most up-to-date activities, there are manyinterest groups on the Internet that specialize in some aspect of networking.Searching for information on the Internet has become greatly simpliﬁed with the advent of the World Wide Weband the deployment of anumber of searchengines on the Web. It is simply amatter of accessing one of these searchengines, entering the topic of interest, and waiting for results. For example, one can go to www.google.com and then enter mobile IP as the search keyword, and Google will return ten pages of references on the topic, including the set of standards relating to the topic.  4.2    Local   Area   Networks Sarhan   M.  Musaand     Matthew     N.O.  Sadiku  Introduction Alocal area network (LAN) is ahigh-speed data network that interconnects workstations, personal computers, mainframe computers, printers and other peripheral deviceswithin asmall geographical area. ALAN can be as small as one room,orcan extend over multiple rooms, multiple ﬂoors within abuilding,and even multiple buildings within acampus or organization. LANs offer computer users manyadvantages, including shared access to devices(such as printers), ﬁle exchange between connected users, and communication between users viaelectronicmail.   There are different kinds of LANs depending on topology: Ethernet (or CSMA/CD), token ring,tokenbus, and star.Here, we will consider each of these as well as wireless LAN.  Ethernet The Ethernet, also known as the carrier sense multiple access with collision detection (CSMA/CD) system, is the most widely used form of LAN. Ethernet was so-named to describe the waythat cabling could carry data everywhere throughout the network. Ethernet or CSMA/CD is the most widely installed local area network LAN technologybecause of its simplicity.Itisspeciﬁed and used as the basis of LAN standards by the IEEE 802.3 standards committee. Sincethen, it has evolved from10Mbps, as traditional Ethernet, to fast Ethernet that operates at 100 Mbps, and then to Gigabit Ethernet at 1or10Gbps.   With CSMA/CD,astation   wishing to transmit ﬁrst listens to the medium to see whether another transmission is in progress (carrier sense). If the medium is idle, the station may transmit. If twoormore stations attempt to transmit at the same time, therewillbeacollision; the data from both transmissions will be garbled and not receivesuccessfully.The station will normally wait 9.6m sor96bit periods before starting the transmission. The CSMA/CD Ethernet technologyiscollision based; that is to say, collision is normal in an Ethernet network. Collisions can happen early or late during transmission, and are thereforetermed as early collision and late collision. Figure 4.4 explains the scenario for an early collision.   The physical layer characteristics include:      . Data rate: 10 Mbps to 10 Gbps     . Maximum  station separation: 2.8 kmComputer Networks                                                                  4 -15       . Maximum number of stations: 1024     . Medium: twisted pair,coaxial cable, and optical ﬁber     . Logical topology: bus     . Physical topology: bus, star,hierarchical star     . Maximum frame size: 1518 bytes     . Frames on LAN: single     . Message protocols: variable frame size, ‘‘best effortdelivery’’ Topology The topologyofaLAN usually refers to the structure or geometric layout of the cable used to interconnect stations on the network. Unlikeconventional data communications networks that can be conﬁgured in a varietyofwaysbythe addition of hardware and software, most LANs are designed to operate based upon the interconnection of stations that followaspeciﬁc topology. The most commonused in LANs include bus, ring, and star as shown in Figure4.5. Bus In abus topologystructure,acable is laid out as one long branch on which each station is connected. The bus topologywas the ﬁrst topologyused when local areanetworks became commercially available in the late                 FIGURE 4.4 Explanation of early collision. (Adopted from Chowdhury[2000].)4 -16                                  Broadcasting and Optical Communication Technology                                FIGURE 4.5 Local networktopologies.   1970s. Sincethen, its use has diminished signiﬁcantly.Connecting the cable requires asimple device called a tap.The tap is apassive device,because it does not alter the signal and does not requireelectricitytooperate. On the workstation end of the cable is anetwork interfacecard(NIC), which is an electronic device that performs the necessarysignal conversions and protocol operations.  LAN  Addressing Schemes Aunique 48-bit identiﬁer or address in each unit is installed by the manufacturers of networkinterface hardware. This is often called the MAC(Media Access Control) number.Only Ethernet chip or board manufacturers are usually involved in obtaining new numbers. Althoughthe Ethernet address is often ﬁxed at the factory, alternativeschemes are employed on some systems, and the workstation IP number is used to generate partofthe Ethernet address. The Ethernet packet header has three ﬁelds, with atrailer ﬁeld containing aCRC error check number.Ethernet receivers read the 48-bit MACdestination address ﬁelds as the packets pass by.Any packet holding the reader’saddress will then continue to be read into the input buffer.A scheme of this type avoids having to read all the passing packets and also does not involvethe main CPU in anyactivity. If apacket is seen to havea‘‘foreign’’ destination address it would either be ignored or redirected to the gatewaymachine that would pass it onto the wider network. This solution becomes impossible to maintain due to daily changes and hardware failures.   As shown in Figure4.6, IP version 4numbers have four parts—the top few bits specify the Class of IP number,then the site ID,then the subnet identiﬁer,and ﬁnally,the lowerpartidentiﬁes the actual machine. This manner of separating the IP number into ﬁelds helps in understanding when networks are being conﬁgured and ranges of addresses need to be allocated to each LAN segment. Only the lower subnet +host values can be revised by the local administrator; the upper network parthas been assigned by the Internet Assigned Number Authority(NIC).Computer Networks                                                                  4 -17                         FIGURE 4.6 Fiveforms of IPv4 numbers and their ranges.  Ring In aring topology, asingle cable that forms the data highway is shaped into aring, as shown in Figure4.7. Such topologyallows unicast, multicast, and broadcast addressing.Each station checks the destination address on the received frame.   When the destination address is unicast, it must match the physical address of the station. If it does, the packet is copied and regenerated and passed to the next station in the ring. Otherwise, the packet is only regenerated and passed to the next station without being copied. When the destination address is multicast, the transmission is from one sourcetomanystations that register to receive the trafﬁc. Forbroadcast addressing,transmission is fromone sourcetoevery station on the network. Astation and the medium are interfaced through amedium interface,which can be an external pieceofhardwareinstalled on the medium or as partofthe network interfacecard installed inside the station. In the case of ring topology, both internal and external interfacesare active devices, which means that they act as repeaters.   Foraring topology, signals are propagated in only one direction from one interfacetoanother.This means that each station has apredecessor and successor.Any twomedium interfacesare connected point-to-point.  Token Ring The token ring (or token-passingring) is aprotocol deﬁned in IEEE Project 802.5 standard. It uses atoken- passing access method, which implied that stations take turns in sending data. Each station is allowedto transmit only during its turn and may send only one frame during each turn. The mechanism responsible for this rotation is called token passing.Atokenisaspecial placeholder frame that goes from station to station around the ring. Everystation is allowed to send data only when it has possession of the token. The token is passed from station to station until it encounters astation with data to send. The station keeps the tokenand sends adata frame.   The data frame is passed around the ring,being regenerated by each station. Each intermediate station examines the destination address, ﬁnds that the frame is addressed to another station, and relays it to its neighbor.The destination station recognizes it own address, copies the message, and changes four bits in the last byte of the frame to indicate the address was recognized and the frame was copied. The full packet then4 -18                                  Broadcasting and Optical Communication Technology                      FIGURE 4.7 Atypical ring LAN. (Adopted from Forouzan [2003].)  continues around the ring until it returns to the sourcestation that sent it. The sourcestation receives the frame. It releases the token back to the ring.The sender then discards the used data frame.  Star In star LAN topology, each station is directly connected to acommon central node, referred to as the star coupler,via twopoint-to-point links, one for transmission and one for reception. Endpoints on anetworkare connected to acommon central hub,orswitch, by dedicated links. Logical bus and ring topologies are often implemented physically in astar topology. Atypical star LAN is shown in Figure4.8.  VLANs Avirtual local area network (VLAN) is asubnetwork or asegment of alocal area network conﬁgured by software,not by physical wiring.Segmentation makes broadcasting possible at the data link layer.ALAN can be divided into several logical LANs called VLANS with each VLAN acting as aworkgroupinthe organization. If aperson moves from one group to another,the physical conﬁguration does not need to change since the group membership is deﬁned by software, not hardware. Some VLAN vendors use the 32-bit IP address as a membership characteristic. For example, the administrator can deﬁne that stations having IP addresses 181.34.23.67, 181.34.23.72 belong to VLAN.  Wireless  LAN The wireless local area network (WLAN) is anew form of communication system. It is basically alocal area network, conﬁned to ageographically small areasuch as asingle building,ofﬁce,store, or campus, thatComputer Networks                                                                  4 -19                      FIGURE 4.8 Atypical star LAN. (Adopted from Forouzan [2003].)  provides highdata connectivitytomobile stations. Using electromagnetic airwaves (radio frequency or infrared), WLANs transmit and receivedata over the air.AWLAN suggests less expensive, fast, and simple network installation and reconﬁguration.   The proliferation of portable computers coupled with the mobile user’sneed for communication is the major driving force behind WLAN technology. WLAN creates amobile environment for the PC and LAN user. It may lower LAN maintenanceand expansion costs sincethere are no wiresthat requirereconﬁguration. Thus, WLANs offer the following advantages over the conventional wiredLANs:      . Installation ﬂexibility: allows the network to go wherewire cannot go     . Mobility: can provide LAN users with access anywhere     . Scalability: can be conﬁgured in avarietyoftopologies to meet speciﬁc needs   However,WLAN does not perform as well as wiredLAN because of the bandwidth limitations and may be susceptible to electromagnetic interferenceand distance. While the initial investment on WLAN hardware can be higher than the cost of wiredLAN hardware, overall installation expenses and life-cycle costs can be signiﬁcantly lower.  Physical Layer and Topology WLAN   does not compete with wiredLAN. Rather,WLANs are used    to extend wired LANs for convenienceand mobility. Wireless links essentially ﬁll in for wiredlinks using electromagnetic radiation at radio or light frequencies between transceivers. Atypical WLAN consists of an access point and the WLAN adapter installed on the portable notebook. The access point is atransmitter/receiver (transceiver) device; it is essentially the wireless equivalent of aregular LAN hub.Anaccess point is typically connected with the wired backbone network at aﬁxedlocation through astandardEthernet cable and communicates with wireless devicesbymeans of an antenna. WLANs operate within the prescribed 900 MHz, 2.4 GHz, and 5.8 GHz frequency bands. Most LANs use 2.4 GHz frequency bands because it is most widely accepted.   Awireless link can provide services in several ways, the following among them:      . Replaceapoint-to-point connection between two nodes or segments on aLAN. Apoint-to-point link       is aconnection between twodevices for transferring data. Awireless link can be used to bridge twoLAN       segments, as shown in Figure4.9. Like apoint-to-point link, the link connects two wireless bridges4 -20                                  Broadcasting and Optical Communication Technology                      FIGURE 4.9 Awireless link replacing apoint-to-point connection.        attached to the twoLANs. Such an arrangement is useful for linking LANs in twobuildings wherea       highway or river makes direct connection difﬁcult.     . Provide aconnection between awired LAN and one or moreWLAN nodes. In this case, adevice is       attached to the wiredLAN to act as apoint of contact (called access point) between the wired LAN and       the wireless nodes. The device can be arepeater,bridge, or router.     . Actasastand-alone WLAN for agroup of wireless nodes. This can be achieved using topologies similar       to awired LAN, namely,astar topologycan be formed with central hub controlling the wireless nodes,       aring topologywitheach wireless node receiving or passing information sent to it or abus topology       with each wireless capable of hearing everything said by all the other nodes. The three popular WLAN       topologies are star,ring,and bus.  Technologies When designing WLANs, manufacturers havetochoose from twomain technologies that are used for wireless communications today:radio frequency (RF) and infrared (IR). Each technologyhas its own merits and demerits.   RF is used for applications wherecommunications are over long distances and are not line-of-sight. In order to operate in the license-free portion of the frequency spectrum known as the ISM band (industrial, scientiﬁc, and medical), the RF system must use amodulation technique called spread spectrum (SS). Spread spectrum is wideband radio frequency technologydeveloped by the militaryduring World WarIIfor use in reliable, secure, mission-critical communications systems. The SS system is one in which the transmitted signal is spread over afrequency much wider than the minimum bandwidth required to send the signal. Using spread spectrum, aradio is supposed to distribute the signal across the entirespectrum. This way, no single user canComputer Networks                                                                  4 -21  dominate the band and collectively all users look likenoise. The fact that such signals appear like noise in the band makes them difﬁcult to ﬁnd and jam, thereby increasing securityagainst unauthorized listeners. There are two types of spread spectrum technology: frequency hopping and direct sequence.   Frequency hopping spread spectrum (FHSS) offers acurrent maximum data rate of 3Mbps. It uses a narrowband carrier that changes frequency in apattern known to both transmitter and receiver.Itisbased on the use of asignal at agiven frequency that is constant for asmall amount of time and then movestoa new frequency.The sequence of different channels for the hopping pattern is determined in pseudorandom fashion. This means that averylong sequence code is used beforeitisrepeated, over 65,000 hops, making it appear random. Thus it is very difﬁcult to predict the next frequency at which such asystem willstop and transmit/receive data as the system appears to be anoise source to an unauthorized listener.This makes the FHSS system very secure against interference and interception. FHSS is characterized by low-cost, low-power consumption, and less range than DSSS but greater range than infrared. Most WLAN systems use FHSS.   Direct sequence spread spectrum takesasignal at agiven frequency and spreads it across aband of frequencies wherethe center frequency is the original signal. The spreading algorithm, which is the key to the relationship of the spread range of frequencies, changes with time in apseudorandom sequence.When the ratio between the original signal bandwidth and the spread signal bandwidth is very large, the system offers great immunitytointerference. Forexample, if a10kbps signal is spread across 1GHz of spectrum, the spreading ratio is 100,000 times or 50 dB.However,inthe ISM band used in WLAN, the available bandwidth critically limits the ratio of spreading and so the advantages of aDSSS scheme against interference is greatly limited. It has been shown that for the WLAN system using DSSS, the spreading ratio is at best 10 times. DSSS is characterized by high-cost, high-power consumption, and morerange than FHSS and infrared physical layers.   The second technologyused in WLAN is infrared (IR), wherethe communication is carried by light in the invisible partofthe spectrum. It is primarily used for very shortdistancecommunications (less than 1m), wherethere is aline-of-sight connection. Since IR light does not penetrate solid materials (it is even attenuated greatly by windowglass), it is not really useful in comparison to RF in WLAN system. However,IRisused in applications wherethe power is extremely limited, such as apager.  Standards Although anumber of proprietary, nonstandardwireless LANs exist, standards havenow been developed. Two international organizations havecontributed to the development of standardsfor WLANs: the Institute of Electronics and Electrical Engineers (IEEE) and the European Telecommunications Standards Institute (ETSI).   In 1997, the IEEE 802.11 committee (http://ieee802.org/11) issued astandardfor wireless LANs. The standardaddresses the physical and MAClayers of the OSI model and includes the following:      . Atransmission rate of up to 2Mbps     . Twodifferent media for transmission over wireless LAN: infrared (IR) and radio frequency (RF)     . The media access control (MAC) protocol as carrier sense multiple access with collision avoidance       (CSMA/CA), i.e. devices can interoperate with wiredLANs viaabridge     . MACprotocol provides two service types: asynchronous and synchronous (or contention-free). The       asynchronous type of service is mandatorywhile the synchronous type is optional     . MAClayer protocol is tied to the IEEE 802.2 logical link control (LLC) layer making it easier to       integrate with other LANs     . Three different physical layers: an optical-based physical-layerimplementation that uses IR light to       transmit and twoRF-based physical-layer choices—direct sequence spread spectrum (DSSS) and       frequency hopping spread spectrum (FHSS) both operating at 2.4 GHz industrial, scientiﬁc, and       medical (ISM) frequency bands. (The ISM bands 902–928 MHz, 2400–2483.5 MHz, and 5725–       5850 MHz do not require alicense to operate.) Added featurestothe MACthat can maximize battery       life in portable clients viapower-management schemes     . Data securitythrough which the wireless LANs can achieve wiredequivalent privacy4 -22                                  Broadcasting and Optical Communication Technology    The standardbasically deﬁnes the media and conﬁguration issues, transmission procedures, throughput requirements, and range characteristics for WLAN technology. It avoids rigid requirements and gives room for vendors in the following areas: multiple physical media, common MAClayer irrespective of the physical layer, common  frame format, powerlimit, and multiple on-air data rates.   There are three major problems encountered by an RF LAN. First, frequency allocation is limited for LANs, but since LANs operate with low power,frequency reuse is possible. Second, interference from other wireless LANs controlled by different organizations and other wireless sources is aproblem. This problem can be controlled by using spread spectrum techniques. Third,securityisatstakebecause an RF signal can penetrate through walls and hostile operators can intercept RF LAN communications. Encryption can be used to lessen this problem. IR LAN uses both laser diodes and light-emitting diodes as emitters. It is useful in highelectromagnetic interference (EMI) environments. It is also secure sinceIRsignals cannot penetrate walls.   CSMA/CA  is slightly different from carrier sense multiple access with collision detection (CSMA/CD), which is the MACprotocol used in an Ethernet-wired LAN. In CSMA/CA, when anode has something to transmit, it waits for silence on the network. When no other nodes are heard, it transmits and waits to receive an acknowledgment fromthe recipient node. If it fails to receiveanacknowledgment within atime period, it assumes that collision has occurred and follows aprocess similar to CSMA/CD.Each node then waits for silence and only transmits after arandom waiting time. While CSMA/CA protocol is slower than CSMA/CD due to the need for waiting for acknowledgment, it works well for wireless LANs. Also,WLANs operate in strong multipath fading channels, wherechannel characteristics can change resulting in unreliable communication.   The ETSI devoted its attention to RF wireless LANs. The ETSI is close to ﬁnalizing its standard, which is based on the 2.4 GHz range used for spread-spectrum LANs in several European countries. The European standardWLAN, called HiperLAN, will allowspeeds of 24 Mbps [5].   Besides IEEE and ETSI, thereare organizations that are moreinterested in the implementation and interoperabilityofWLAN products. Such organizations include the Wireless LAN Alliance(WLANAat www.wlana.com)and WirelessEthernet CompatibilityAlliance (WECA at www.wi-ﬁ.org or www.wireless- ethernet.com). WLANAwas formed in 1996 with12members as atrade association for wireless LAN vendors. WECA  is anonproﬁt manufacturing consortium with over 60 companies as members; it was formed in 1999 to certify interoperability of IEEE 802.11 products.  Applications Offering the obvious advantage of no wire installation costs, wireless LANs can be deployed in adynamic environment where thereisfrequent reconﬁguration of computer networks. Also,without the cables, excavation and long installation waiting times, it is simpler to connect difﬁcult-to-reach customers.   Althoughseveral products for RF and IR LANs are already available in the marketplace, the introduction of their applications is just beginning.Typical mobile users are assumed to be laptop or notebook computers and portable base stations. Servicesprovided by WLANs include data applications such as over TCP/IP and multimedia applications.   The most prominent users of WLANs are those whose projects promise quick payoffs for adding mobility. Industries such as securityservices, banks, retail, manufacturing,and healthcareare morenotable for deploying wireless LANs that allow workers to roam while gathering information.   Mobile terminals—personal digital assistants (PDAs), specialized handheld terminals, and barcode scanners—connected to WLANs are increasingly being used to enhance business operations. It has become commonplace for WLANs to be used in applications such as:      . Printersharing: linking to adistant printer within adepartment     . Electronic mail: sending and receiving e-mails fromanywhere     . Healthcare: access to patient records from practically anywhere and location-independent claims       processingComputer Networks                                                                  4 -23       . Financial servicessuch Stock or CommunityExchange: implementing hand-held communicators in the       trading room to increase the speed, accuracy,and reliabilityofits pricereporting system     . Factorycontrol: data acquisition, inventorycontrol, scoreboards, and robotics   Other applications include trading,banking,restaurants, retail industry, warehousing, manufacturing, education, ofﬁce environments, petroleum industry, agriculture, and food services. Today,WLAN technology is becoming fairly mature. WLANs are becoming morewidely recognized as ageneral-purpose connectivity alternativefor abroad range of customers.   Still, the WLAN market remains small because the technologyisnew and so componentsare expensive and the data rates are low. On the one hand, it costs less than $100 to buy the network card needed to connect aPCtoawiredEthernet LAN    with the data rate of 10 Mbps. On the other hand, the card needed to interfacethe same PC to wireless radio LAN costs $500, while the wireless hubs (access points) that connect the portable units to the wirednetwork cost as much as $3000 each for adata rate of 1to2 Mbps. However,research groups are working hard to shrink radios into achip that can be mass produced cheaply.Ifthey succeed, the demand for radio LANs may followthe same trend as cellular phones in recent years.  References D.D. Chowdhury, HighSpeed LAN TechnologyHandbook,Berlin: Springer Verlag,2000. B.A. Forouzan, Data Communications and Networking,3rd ed., NewYork: McGrawHill, 2004. B.A. Forouzan, Local Area Networks,New York: McGrawHill, 2003. M. Sadiku and M. Ilyas, Simulation of Local Area Networks,Boca Raton, FL: CRCPress, 1995. W. Stallings, High-Speed Networks and Internets: Performance and Quality of Service,Upper Saddle River,      NJ: Prentice Hall,2002. C.M. White, Data Communications and Computer Networks, ABusiness User’sApproach, Course Technology ,      Boston, MA: Thomson Learning,Inc., 2002. R. Williams, Computer Systems Architecture, ANetworking Approach,Upper Saddle River,NJ: Pearson      Education, 2001. G.M. Zobrist, ‘‘Local area networks,’’ IEEE Potentials,December/January, pp.6–10, 1995.   4.3    The Intelligent Network RichardB.Robrock II  The term intelligent network refers to the concept of deploying centralized databases in the telecommunica- tions networkand querying those databases to provide awide varietyofnetwork servicessuch as 800 service (toll-free service)and credit cardcalling.The ﬁrst use of these centralized databases was in AT&T’s network in 1981 wherethey were used to facilitate the setup of telephone calls charged to acalling card. Today such databases are widely deployed throughout NorthAmerica and supportthe handling of well over 100 billion telephone calls per year.   The words intelligent network, when ﬁrst used, had arelatively narrowdeﬁnition, but that deﬁnition has broadened considerably withthe introduction of the advanced intelligent network, the wireless intelligent network, and soon, the broadband intelligent network. The advanced intelligent networkhas introduced powerful service creation tools that haveempowerednetwork providers to create their own network services. The network providers, in turn, are beginning to broaden the participation in service creation by allowing their customers or third parties to use these tools to create services. The result has been arapid growth in new network services.4 -24                                  Broadcasting and Optical Communication Technology  AHistoryofIntelligence     in the Network The ﬁrst ‘‘intelligence’’ in the telephone networktook the form of rows of human telephone operators, sitting side by side, plugging cords into jacks to facilitate the handling of calls. These operators established calls to far- away points, selected the best routes and provided billing information. They werealso an information source—providing time or weather or perhaps disseminating the local news. Moreover,they had the opportunity to demonstrate akind of heroism—gathering volunteers to saveahouse from ﬁre, helping to catch aprowler,locating alost child, and on and on. In the early yearsoftelephony, the feats of the telephone operator were indeed legendary.   In the 1920s, however, technologybecame available that allowed automatic switching of telephone calls through the use of sophisticated electromechanical switching systems. Initially,these switches servedasanaid to operators; ultimately,they led to the replacement of operators. The combination of the rotary telephone dial and the electromechanical switch allowed customers to directly dial calls without the assistance of operators. This led to areduction of human intelligenceinthe network.   Another dramatic change took placeinthe telephone networkin1965; it was called software.Itcame with the marriage of the computer and the telephone switching system in the ﬁrst stored-program control switch. With the introduction of switching software came afamily of custom calling services(speed calling,call waiting,call forwarding, and three-way calling) for residential customers, and arobust set of Centrexfeatures (station attendant, call transfer,abbreviated dialing,etc.) for business customers. The ﬁrst software programs for these stored-program control switches contained approximately 100,000 lines of code; by 1990 some of these switching systems became enormously complex, containing 10 million lines of code and offering hundreds of different servicestotelephone users.   During the 1980s, anew architectural concept was introduced; it came to be called the intelligent network. It allowednew telecommunications servicestobeintroduced rapidly and in aubiquitous and uniform fashion. Featureand service availabilityinthe network ceased to be solely dependent upon the hardware and software in stored-program control switches. Rather some new intelligencewas centralized in databases that were accessed using packet switching techniques. Most signiﬁcantly,the intelligent network started to provide some of the capabilities that operators had made available in the early years of telephony. The remaining sections describe the intelligent network, its characteristics, and its services. They also provide adescription of the advanced intelligent network, which dramatically broadens the participation in the creation of new services.  The  Intelligent Network The intelligent network architecture is illustrated in Figure4.10; its primaryelements are aswitching system, asignaling network, acentralized database, and an operations supportsystem that supports the database. The architectural concept is asimple one. When acustomer places atelephone call that requires special   FIGURE 4.10 Intelligent network architecture—telephone calls that requirespecial handling are intercepted in a switching system that launches queries through asignaling network to acentralized database. ( Source: R.B.Robrock II, ‘‘The intelligent network—Changing the faceoftelecommunications,’’ Proc. IEEE, vol. 79, no.1,pp. 7–20, January1991. # 1991 IEEE.)Computer Networks                                                                  4 -25  handling,such as atoll-free call (800 service)orcredit cardcall, that call is intercepted by the switching system that suspends call processing while it launches aquerythrough asignaling network to acentralized database. The database, in turn, retrieves the necessaryinformation to handle the call and returns that information throughthe signaling network to the switch so that the call can be completed. The role of the operations supportsystem is to administer the appropriate network and customer information that residesinthe database.   It is conceivable that the database in this architecture could reside in the switching system, and the signaling network in this instance would not be required. However,that would magnify the task of administering the customer information, sincethat information would be contained in thousands of switches instead of dozens of centralized databases. In addition, even moreimportantly,thereare two shortcomings associated with basing manyofthe potential new services in switches, rather than utilizing centralized databases to provide information for the switches. The ﬁrst is adeployment problem. As of 1990 there weremorethan 15,000 switches in the United States, and asingle switch can cost millions of dollars. To introduceanew service in local switches and to makeitwidely available generally requires some not-so-simple changes in those switches or,insome cases, replacement of certain switch types altogether.These switch modiﬁcations typically take years to implement and require atremendous capital investment. As aresult, ten yearsafter introduction, custom calling serviceswere available to fewer than 1% of the residential customers in the United States.   Asecond problemwith switch-based services has been that asingle service sometimes behaves differently in different switch types. Forexample, the speed calling access patterns are different in various stored-program control switches. The public is not particularly sensitive to this fact, because speed calling is not associated with an individual but rather an individual’sstation set. People live in amobile society, however,and they want to havetheir services available from anystation set and havethem behave the same from anystation set.   The intelligent network architecture has been the key to solving both the deployment problem and service uniformity problemassociated with switch-based services. Servicesdeployed using an intelligent network centralized database are immediately ubiquitous and uniform throughout acompany’sserving area.  Intelligent Network Systems In 1981, AT&T introduced into the Bell System aset of centralized databases called network control points; they supported twoapplications—the billing validation application for calling card service (credit card calling) and the INWATSdatabase used to support800 service. Queries were launched to these databases throughAT&T’scommon-channel interofﬁce signaling (CCIS) network.   In 1984, following the divestiture of the Regional Bell Operating Companies fromAT&T,the regional companies began planning to deploy their own common-channel signaling (CCS) networks and their own centralized databases. They selected the signaling system 7 protocol for use in their signaling networks, called CCS7 networks, and they named their databases service control points (SCPs).  The CCS7 Network Ageneral architecturefor aregional signaling network is shown in Figure4.11. The network is made up of signal transfer points (STPs), which are very reliable, high-capacitypacket switches that route signaling messages between networkaccess nodes such as switches and SCPs. To perform these routing functions, the STPseach possess alarge routing database containing translation data.   The CCS7 network in Figure4.11 contains both local STPsand regional STPs. The STPsare typically deployed in geographically separated pairs so that in the event of anatural disaster at one site, such as an earthquake, ﬂood, or ﬁre, the total trafﬁc volume can be handled by the second site. Indeed, redundancy is provided at all keypoints so that no single failure can isolate anode.   As illustrated in Figure4.11, the following link types havebeen designated:      . A-links connect an access node, such as aswitching system or SCP,toboth members of an STP pair.     . B-links interconnect twoSTP pairs forming a‘‘quad’’offour signaling links whereeach STP indepen-       dently connects to each member of the other pair.4 -26                                  Broadcasting and Optical Communication Technology   FIGURE 4.11 Link arrangements in aCCS7 signaling network. ( Source: R.B. Robrock II, ‘‘The intelligent network— Changing the face of telecommunications,’’ Proc. IEEE, vol. 79, no.1,pp. 7–20, January1991. # 1991 IEEE.)       . C-links are the high-capacityconnectionsbetween the geographically separated members of an STP       pair.     . D-links connect one STP pair to asecond STP pair at another levelinthe signaling hierarchyorto       another carrier.     . E-links connect an access node to aremote STP pair in the signaling network and are rarelyused.     . F-links directly interconnect twoaccess nodes without the use of an STP; they are nonredundant.   The CCS7 links normally function at 56 kb/s in NorthAmerica, while links operating at 64 kb/s are common  in Europe.   The CCS7 signaling network provides the underlying foundation for the intelligent network, and the regional telephone companies in the United States began wide-scale deployment of these networks in 1986; several large independent telephone companies and interexchange carriers (ICs) soon followed. They used these networks for both trunk signaling between switches as well as for direct signaling from aswitch to adatabase.  The  Service Control  Point The ‘‘brains’’ofthe intelligent network is the SCP.Itisanon-line, fault-tolerant, transaction-processing database that provides call handling information in response to network queries. The SCP deployed for 800 service is ahigh-capacitysystem capable of handling morethan 900 queries per second or 3million per hour. It is areal-time system with aresponse time of less than one half second, and it is ahigh-availabilitysystem with adowntime of less than 1minute per year for amated SCP pair.The SCP is also designed to accommodate growth, which means that processing power or memorycan be added to an in-service system without interrupting service.Inaddition, it is designed to accommodate graceful retroﬁt, which means that a new software program can be loaded into an in-service SCP without disrupting service.  Data  Base 800  Service SCPshavebeen deployed throughout the United States in supportofthe Data Base 800 Service mandated by the Federal Communications Commission. This service provides its subscribers with number portabilityso that asingle 800 number can be used with differentcarriers. The Data Base 800 Service architecture is shown in Figure4.12. With this architecture, 800-number calls are routed fromanend ofﬁce to aservice switching point (SSP) that launches queries throughaCCS7 signaling networktothe SCP.The SCP identiﬁes the appropriate carrier,asspeciﬁed by the 800 service subscriber,and then, if appropriate, translates the 800 number to aplain old telephone (POTS) number.This information is subsequently returned to the SSP so that the call can be routed through the network by handing the call offtothe appropriate carrier.This technology allows subscribers to select the carrier and the POTS number as afunction of criteria such as time of day,dayComputer Networks                                                                  4 -27   FIGURE 4.12 Data Base 800 Service—800-number calls are routed to an SSP that launches queries throughaCCS7 networktoanSCP containing the 800 database. In this example, the SCP translates the 800 service number of 800–555– 5463 into the POTS number of 404–555–1000. ( Source: R.B.Robrock II, ‘‘The intelligent network—Changing the face of telecommunications,’’ Proc. IEEE, vol. 79, no.1.pp. 7–20, January1991. # 1991 IEEE.)  of week, day of year,percent allocation, and the location of the calling station. Thus the SCP provides two customer-speciﬁed routing information functions: acarrier identiﬁcation function and an address translation function.   The SCP 800 Service database is administeredbyasingle national service management system (SMS). The SMS is an interactive operations supportsystem that is used to process and update customer records. It is the interfacebetween the customer and the SCP.Ittranslates alanguage that is friendly to acustomer into a language that is friendly to online, real-time databases. Along the way, it validates the customer input.   Demand for toll-free servicesinthe United States became so great that, in 1996, the industrybegan to introduce additional numbering plan areas (NPAs) to supportthe service. Indeed, the Intelligent Network systems wereenhancedtosupportarange of toll-free NPAs: 800, 888, 877, 866, 855, 844, 833, and 822.  Alternate Billing Services Alternate billing services (ABS) have also been implemented using the intelligent network architecture. Alternate billing is an umbrella title that includes calling cardservice,collect calling,and bill-to-third-number calling. The network conﬁguration supporting ABS is shown in Figure4.13.   FIGURE 4.13 Alternate billing services—calls are routed to an OSS that launches queries through the CCS7 network to SCPscontaining the LIDB application. ( Source: R.B. Robrock II, ‘‘The intelligent network—Changing the face of telecommunications,’’ Proc. IEEE, vol. 79, no.1.pp. 7–20, January1991. # 1991 IEEE.)4 -28                                  Broadcasting and Optical Communication Technology    With this architecture, when acustomer places acalling card call, the call is routed to an operator services system (OSS) that suspends call processing and launches aquerythrough aCCS7 signaling network. The query is delivered to an SCP that contains the line information database (LIDB) application software. The LIDB application can provide routing information, such as identifying the customer-speciﬁed carrier that is to handle the call, as well as provide screening functions, such as the calling card validation used to authorize a call. The LIDB then returns the appropriate information to the OSS so that the call can be completed. The LIDBs are supported by the database administration system (DBAS), which is an operations supportsystem that processes updates for calling cardservice as well as other services. Multiple DBAS systems typically supporteach LIDB.   During 1991, each of the Regional Bell Operating Companies and anumber of large independent telephone companies interconnectedtheir CCS7 networks, mostly through STP hubs, to create anational signaling network; it was aprocess called LIDB interconnect. When it was ﬁnished, it meant that aperson carrying a particular company’scalling cardcould, from anywhere in the United States, querythe LIDB containing the associated calling card number.   Althoughthe LIDB was originally developed to supportcalling card service, it has since found wide application in the telecommunications industry. For example, the LIDB is used to translate the telephone number of acalling party to aname as partofcalling name delivery service,ortoconvert that number to a nine-digit ZIP code as partofsingle number service. The LIDB databases now contain more than aquarter of abillion customer records that are updated at arate of morethan amillion changes per day.Although physically distributed, the LIDBs appear logically as asingle database. They represent anational resource.  Other  Services Foralternate billing services, the SCP is essentially designed to perform two functions: carrier identiﬁcation and billing authorization. For800 service,the SCP provides carrier identiﬁcation and address translation. These basic functions of authorization, address translation, and carrier identiﬁcation can be used again and again in manydifferent ways. Forexample, the intelligent network has been used to supportprivate virtual networks (PVNs). PVNs makeuse of the public telephone network but, by means of softwarecontrol, appear to havethe characteristics of private networks. APVN servesaclosed-user group,and acaller requires authorization to gain access to the network. This screening function on originating calls uses an authorization function. Second, aPVN may offer an abbreviated dialing plan, for example, four-digit dialing. In this instance,the SCP performs an address translation function, converting afour-digit number to aten- digit POTS number.There may also be acustomer-speciﬁed routing information function that involves selecting from ahierarchyoffacilities; this can be accomplished through use of the SCP carrier identiﬁcation function.   The SCP in the intelligent network can supportavast number of services ranging from calling name delivery service to messaging service. With calling name delivery,aswitch sends aquerytothe SCP with the ten-digit calling party number;the response is the calling party name that is then forwardedbythe switch to a displayunit attached to the called party station set. In supportofmessaging services, the address translation capabilityofthe SCP can be used to translate aperson’s telephone number to an electronic-mail address. As a result, the sender of electronicmail need only knowaperson’s telephone number.  The  Advanced   Intelligent Network The intelligent network architecturediscussed thus far is often referred to in the literatureasIntelligent Network/1; this architecture has addressed the deployment problem and the service uniformityproblem. The next phase in the evolution of this network has come to be called the advanced intelligent network (AIN), with the AIN standards deﬁned by Telcordia Technologies (formerly known as Bellcore).   The concept of AIN is that new services can be developed and introduced into the network without requiring carriers to wait for switch generics to be upgraded. Some AIN applications introduce powerful service-creation capabilities that allow nonprogrammers to invokebasic functions offered in the network andComputer Networks                                                                  4 -29  stitch together those functions, as illustrated in Figure 4.14, to constitute anew service.Asaresult, AIN promises to dramatically shorten the interval required to develop new services. Perhaps of greater signiﬁcance, it promises to broaden the participation in service creation. In addition, it offers the opportunityto personalize or customize services. The silicon revolution has driven the cost of memorydowntothe point whereitiseconomically viable to haveenoughmemoryinthe network to storethe service scripts or call processing scenarios that are unique to individuals.   Many people think of the AIN as acollection of network elements, network systems, and operations systems; this view might be called atechnologist’s view.Perhaps abetter representation is shown in Figure4.15; it shows acollection of people—people empoweredtocreate services.   Historically,the creation of new servicesprovided by the telephone network has been the sole domain of the network element and network system suppliers. There is perhaps agood analogywith the automobile industry. Amarket study in the early 1900s predicted that 200,000 was the maximum number of cars that   FIGURE 4.14 Creating the service script or scenario for acall by stitching together functional blocks. ( Source: R.B. Robrock II, ‘‘Theintelligent network—Changing the face of telecommunications,’’ Proc. IEEE, vol. 79, no.1.pp. 7–20, January1991. # 1991 IEEE.)   FIGURE 4.15 The advanced intelligent network—a business perspective. ( Source: R.B. Robrock II, ‘‘Putting the Telephone User in the Driver’sSeat,’’ International Council for Computer Communication ConferenceonIntelligent Networks, pp.144–150, May1992.)4 -30                                  Broadcasting and Optical Communication Technology  could ever be sold in asingle year in the United States; the reasoning was that 200,000 was the maximum number of chauffeurs that could enter the workforce in asingle year.Inthe telecommunications business, the network element and network system suppliers havebeen the chauffeurs of the network services business.   The service-creation tools offered by the AIN, however,empowertelephone companystafftocreate new services. Moreover, similar tools maywell be used by the telecommunications staffoflarge corporations, or by third-party application providers or even by some segment of the telephone user population. As aresult, we maysee an explosion in the number of network services.   The AIN introduces verypowerful service-creation tools that are used to produce service-logic scripts (programs). In one arrangement, the service creation is done by assembling service-logic graphs from graphical icons that represent functional componentsofservices. The completed graph is then validated with an expertsystem and tested off-line by executing everyleg of the service-logic graph. At this point the service- logic program can be downloaded into the service control point so that it is ready for execution.   To makeuse of the new service,itisthen necessarytoset ‘‘triggers’’ in the appropriate service switching point. These triggers can be set for both originating and terminating calls, and they represent events that, should they occur,indicate the need for the switch to launch aquerytothe SCP for information the switch needs to process the call.   The ﬁrst phase of the AIN, called AIN 0, became reality in late 1991 when friendly user trials began in two of the Regional Bell Operating Companies. The AIN 0call model introduced three trigger check points: the off- hook immediate trigger,the off-hook delayed trigger,and the digit collection and analysis trigger.The release was based on the American National Standards Industry(ANSI) transaction capabilityapplication part (TCAP) issue 1, where TCAP is at layer 7ofthe SS7 protocol stack.   AIN 0evolved to AIN 0.1 and then AIN 0.2, with each new version of AIN containing additional triggers. AIN 0.1, based on ANSI TCAP issue 2, separated the formal call model into an originating call model and a terminating call model and introduced threenew triggers: the N11 trigger,the 3-6-10-digit trigger,and the termination attempt trigger.AIN 0.2 introduced anew network element, the intelligent peripheral (IP), and supported personal communication service and voice-activated dialing.Inthe process it introduced busy and no answer triggers.   Today over 100 AIN services—such as single number service, ﬂexible hot line, inbound call restriction,500 access service,etc.—are deployed in NorthAmerica, and the number is growing rapidly.Moreover,the U.S. Telecommunications Actof1996 required local number portability, that is, customer were allowed to keep their telephone numbers if they switched local carriers. The concept soon spread from the wireline network to the wireless or cellular network. The technologies of AIN werethe answer.   The concept of AIN is not limited to the United States. The European Telecommunications Standards Institute (ETSI) has deﬁned aEuropean AIN standardreferred to as CoreINAP, and deployment of Core INAP systems in Europe began in 1996.   The architectural concepts of AIN are now beginning to carryoverinto wireless networks as well as broadband networks. Althoughthe standards in these domains are just being developed, the value added by the wireless intelligent network (WIN) and the future broadband intelligent network (BIN) promises to surpass the value seen in the narrowband wireline world.  Back  to the Future The intelligent network, with its centralized databases, has offered ameans to rapidly introducenew services in aubiquitous fashion and with operational uniformityasseen by the end user.The advanced intelligent network has gone on to provide aservice-independent architecture, and, with its powerful service-creation capabilities, has empowered nonprogrammers to participate in the development of new services. In many ways, as we go into the future, we are going back to atime when operators were the ‘‘human intelligence’’ in the network. The human intelligencewas all but eliminated with the introduction of switching systems, but nowthe intelligent networkisworking to put the intelligenceofthe human operator back into the network.Computer Networks                                                                  4 -31  Deﬁning   Terms Common-channel signaling (CCS):   Atechnique for routing signaling information throughapacket-      switched network. Database administration systems (DBAS): An operations supportsystem that administers updates for      the line information database. Line information database (LIDB): An application running on the service control point that contains      information on telephone lines and calling cards. Service control point (SCP): An on-line, real-time, fault-tolerant, transaction-processing database that      provides call-handling information in response to network queries. Service management system  (SMS):  An operations supportsystem that administers customer records      for the service control point. Signal transfer point (STP): Apacket switch found in the common-channel signaling network; it is used      to route signaling messages between network access nodes such as switches and SCPs. Signaling system 7(SS7): Acommunications protocol used in common-channel signaling networks.  References AT&T Bell Laboratories, ‘‘Common channel signaling,’’ The Bell System Tech. J., vol. 57, no.2,pp. 221–477,      February1978. AT&T Bell Laboratories, ‘‘Storedprogram controlled network,’’ The Bell System Tech. J., vol. 61, no.7,part3,      pp.1573–1815, September 1982. Bell Communications Research, ‘‘Advanced intelligent network (AIN) 0.1 switch-service control point (SCP)      application protocol interface generic requirements,’’ Bell Commun. Res. Technical Ref., TR-NWT-      001285, Issue1,August 1992. European Telecommunications Standards Institute, ‘‘Intelligent network (IN): Intelligent networkcapability      set 1(CS1) core intelligent network applications protocol (INAP) part1:Protocol speciﬁcation,’’ Eur.      Telecom. Stds. Inst., ETS 300 374–1, draft, May1994. Globecom ’86: The Global Telecommunications Conference, Conference Record, vol. 3, pp.1311– 1335,      December 1986. R.J.Hass and R.W.Humes,   ‘‘Intelligent network/2: Anetwork architecture concept for the 1990s,’’      International Switching Symposium, Conference Record, vol. 4, pp.944–951, March 1987. R.B. Robrock, II, ‘‘The intelligent network—Changing the faceoftelecommunications,’’ Proc. IEEE, vol.79,      no.1.pp. 7–20, January1991. R.B. Robrock, II, ‘‘Putting the telephone user in the driver’sseat,’’ International Council for Computer      Communication Intelligent NetworksConference, pp.144–150, May1992. R.B. Robrock, II, ‘‘The manyfaces of the LIDB data base,’’ International Conference on Communications,      Conference Record, June 1992. Telcordia Technologies, ‘‘Advanced intelligent network (AIN) switch-service control point (SCP)/Adjunct      interface generic requirements,’’ Telcordia Technologies, Generic Requirements, GR-1299-CORE, Issue      9, November 2003.  Further Information The magazine Bellcore Exchange contained numerousarticles on the intelligent network, particularly in the following issues: July/August 1986, November/December 1987, July/August 1988, and March/April 1989. Articles on AIN service creation appeared in the January/February1992 issue.   The monthly publication IEEE Communications Magazine contains numerousarticles on the intelligent network. Aspecial issue on the subject was published in January1992. Copies are available from the IEEE Service Center,445 HoesLane, Piscataway, NJ 08854–4150.   The monthly publication The Telcordia Technologies Digest lists recent publications available from Telcordia Technologies. There are aseries of technical advisories, technical requirements, generic requirements, and4 -32                                  Broadcasting and Optical Communication Technology  special reports that havebeen issued on the intelligent network. Copies are available by contacting Telcordia Customer Service toll-free 1–800–521-CORE (2673).   The bimonthly publication The AT&T Technical Journal contains numerousarticles on the intelligent network. The advanced intelligent network is the subject of aspecial issue: Summer 1991, vol.70, nos. 3–4. Current or recent issues may be obtained from the AT&T Customer Information Center,P.O.Box 19901, Indianapolis, IN 46219.  4.4    Mobile    Internet Apostolis   K. Salkintzis   and  Nikos   Passas  Introduction The mobile Internet can be considered as the migration of the standard Internet applications and services to the mobile environment. The introduction of mobilityitself raises anumber of concerns and challenges. For instance,what wireless technologyismost appropriate for the provision of Internet services? Is this technologyequally appropriate for applications withdissimilar requirements such as e-mail and video broadcasting? Howdoweprovide ubiquitous Internet services while users move across different locations in which the same wireless service may not be available? Will it be moreappropriate to consider several wireless technologies such as cellular data networks, wireless local area networks (WLANs), wireless personal area networks (WPANs)? If yes, then how do we combine them into aseamless wireless service?How do we optimize the utilization of wireless resourcesinordertoaccommodateasmanymobile Internet users as possible? Howdowehandle the securityissues raised by wireless transmission and possibly by the use of different wireless services provided by different operators?   These are only asmall number of the questions we need to address in our attempt to makethe mobile Internet areality. One importantclariﬁcation is in orderhere: The fact that we can now use our laptop or personal digital assistant (PDA) along withawireless device (e.g., cellular phone, WLAN adapter,etc.) to haveaccess to our Internet/intranet, e-mail, or other Internet Protocol (IP) service does not really mean the mobile Internet is already available. In reality, what is deﬁned as mobile Internet is far more complex than that. By deﬁnition, the Internet is anetwork of millions of users who communicate by means of standardIP protocols. Therefore, in the mobile Internet we need also to supportmillions of always-connected mobile/ wireless users. The keywords we need to bear in mind are ‘‘always-connected’’and ‘‘millions of users.’’ To be always-connected means that we do not havetomake aconnection beforeevery transaction or after we change wireless service.Our assumption is that we always haveconnectivitytopublic Internet services and we are always reachable throughapublic IP address, no matter whereweare.This calls for extensive mobilitymanagement (discussed in section ‘‘MobilityManagement Schemes for Mobile Internet’’) and seamless handover across differentwireless networks. We do need several wireless networks (and several wireless technologies, as we will see later on) because no sole wireless network can provide ubiquitous wireless services and hencenosole wireless network can meet the always-connected requirement. Atypical mobile Internet user is assumed to move seamlessly between different wireless networks (or even between ﬁxed and wireless networks), which may or maynot employthe same radio-access technology. This is schematically illustrated in Figure 4.16. The seamless fashion of movement suggests that our (virtual) connection to the public Internet is transferred from one access network to the other without anyactions on the user’sside. In effect, this creates avirtual wireless network from the user’sperspective that provides ubiquitous Internet connectivity.   We need also to support‘‘millions of users’’wirelessly connected to the Internet in ordertobecompliant with the large scale of users supported by the Internet. This creates capacity concerns and further justiﬁes why no single wireless network is sufﬁcient. The capacityconcerns havedirect implications in several design aspects. Forinstance,the wireless technologyneeds to be as spectrum-efﬁcient as possible; it has to implement arandom  access scheme that can accommodate alarge number of users and degrade gracefullyinoverloadComputer Networks                                                                  4 -33                             FIGURE 4.16 The concept of always-connected.  conditions. The IP addressing scheme should be able to supportthe required user capacity, the end-to-end transport schemes, and applications should probably take into account mobility.   It is interesting to note that most recent research projects, as well as standardization activities, are moving around such goals. In this context, the correlation between the mobile Internet and the so-called ‘‘beyond 3G’’ technologies is evident. In reality,most of the beyond 3G technologies are tailored to supportthe requirements for mobile Internet: increased capacity, QoS, mobility, security, TCP/IP enhancedperformance, and integration of diverse technologies into one ubiquitous virtual network.   Despite the fact that herein we focus entirely on wireless networks as potential access means for the mobile Internet, it is important to note that ﬁxed access networks and their associated technologies (e.g., xDSL, cable modems, etc.) do play an important role. In fact, the vision towards the mobile Internet entails both wireless and ﬁxed access technologies, as well as methods to seamlessly integrate them into an IP-based corenetwork (see Figure4.17). Atypical user scenario,which shows the fundamental interworking requirements between ﬁxed and wireless access networks, occurs when auser downloads aﬁle over the Internet using acable modem, and in the middle of the ﬁle transfer takeshis laptop to the car and drives to the airport(see Figure 4.16). In a mobile Internet environment, the ﬁle download would be seamlessly switched from the cable connection to a wireless connection, e.g., acellular data connection and would be carried on while on the go.   In our mobile Internet environment, as deﬁned above,itisevident that enhanced mobilityisakey requirement, thus we discuss it in moredetail in section ‘‘MobilityManagement Schemes for Mobile Internet.’’  Key Aspects of the Evolution toward the Mobile Internet Bearing in mind the abovediscussion, we list belowthe keyaspects of the evolution towards the mobile Internet. Several organizations worldwide such as 3GPP,3GPP2, IETF,and IEEE develop activities relevant to these aspects [1].      . Mobile networks will evolvetoanarchitecture encompassing an IP-based core network and many       wireless access networks. The keyaspect in this architectureisthat signaling with the corenetwork is       based on IP protocols (morecorrectly,onprotocols developed by IETF) and it is independent of the       access network (be it UMTS, cdma2000, WLAN, etc.). Therefore, the same IP-based servicescould be       accessed over anyaccess network. An IP-based corenetwork uses IP-based protocols for all purposes       including data transport, networking,application-level signaling,mobility, etc. The ﬁrst commercial4 -34                                  Broadcasting and Optical Communication Technology     FIGURE 4.17 The architectureofmobile Internet is tightly coupled with the ‘‘beyond-3G’’ network architecture.        approach towards this IP-based corenetwork is the well-known IP Multimedia Core Network Subsystem       (IMS) standardized by 3GPP and 3GPP2. IMS is further discussed in Ref. [1] and its relevant       speciﬁcations can be found at www.3gpp.org/ftp/specs/.     . The long-term trend is towards all-IP mobile networks wherenot only the corenetwork, but also the       radio access network is solely based on IP technology. In this approach, the base stations in acellular       system are IP access routers and mobility/session management is carried out with IP-based protocols       (possibly substituting the cellular-speciﬁc mobility/session management protocols).     . Enhanced IP Multimedia/Internet applications willbeenabled by means of application-level signaling       protocols standardized by IETF (e.g., SIP,HTTP, etc.). This again is addressed by the 3GPP/3GPP2 IP       Multimedia CoreNetwork Subsystem.     . End-to-end QoS provisioning will be important for supporting the demanding multimedia/Internet       applications. In this context extended interworking between, e.g., UMTS, QoS, and IP QoS schemes is       needed, or moregenerally,interworking between layer-2 QoS schemes and IP QoS is required for end-       to-end QoS provision.     . Voice over IP (VoIP) will be akey technology. Several standardsorganizations are developing activities       to enable VoIP,e.g., the ETSI TISPAN project (http://portal.etsi.org/tispan), the IETF SIP Working       Group (http://www.softarmor.com/sipwg/),etc.     . The mobile terminals will be based on software-conﬁgurable radios with capabilitytosupportmany       radio access technologies across manyfrequency bands.     . The abilitytomoveacross hybrid access technologies will be an important requirement, which calls for       efﬁcient and fast vertical handovers and seamless session mobility.The IETF SEAMOBY (http://       www.ietf.org/html.charters/seamoby-charter.html) and the legacy MOBILE-IP (http://www.ietf.org/       html.charters/OLD/ mobileip-charter.html) working groups haveaddressed some of the issues related       to seamless mobility. Fast Mobile IP and Micro-mobilityschemes (see section ‘‘MobilityManagement       Schemes for Mobile Internet’’) are keytechnologies in this area.     . In the highly hybrid access environment of the mobile Internet security will also play akey role. IEEE       802.11 task group I(TGi) is standardizing new mechanisms for enhanced securityinWLANs. The IETF       SEAMOBY  WG also addresses the protocols that deal with (security) context transfer during handovers.Computer Networks                                                                  4 -35                          FIGURE 4.18 Simpliﬁed architecture of mobile Internet.       . Forextended roaming between different administrativedomains and/or different access technologies,       advanced AAA protocols and AAA interworking mechanisms will be required. AAA interworking       between WLANs and 3GPP networks is being studied by 3GPP and 3GPP2 [1–5].     . Enhanced networking APIs for QoS-, multicast-, and location-aware applications will be needed.     . Wireless Personal Area Networks (WPANs) will startspreading, initially based on Bluetooth       technology(see www.bluetooth.com) and later on IEEE 802.15.3 high-speed wireless PANtechnology,       which satisﬁes the requirementofthe digital consumer electronicmarket (e.g., wireless video       communications between aPCand avideo camera).     . Millions of users are envisioned being ‘‘always-connected’’tothe IP coreinfrastructure.Market       forecasts suggest that about 1billion addresses will be needed by 2005 for the integration of Internet-       based systems into transportation means (cars, aircraft, trains, ships, and freight transport) and       associated infrastructures for mobile e-commerce. This indicates that there is astrong need for IPv6—       IPv6 is being adapted in 3GPP.The European Union, in particular,ispushing hard for the fast adoption       of IPv6.     . Wireless communication technologywill evolvefurther and willsupporthigher bit rates. For instance,       WLANs willsoon support bit rates of more than 100 Mbps.This is being addressed by the IEEE       Wireless Next Generation Standing Committee (see www.ieee802.org/11). Higher bit rates are typically       accompanied by smaller coverage areas, and thereforeefﬁcient, fast, and secure horizontal handovers       will be required.   The above evolutionaryaspects lead to ahigh-level network architecture as the one depicted in Figure 4.18. Note that each administrativedomain represents anetwork partthat is typically operated by asingle mobile network operator.  Evolution to IP-based Core Networks Without doubt the most widely supported evolution towards the mobile Internet is the evolution to IP-based core networks, also referred to as all-IP corenetworks. The term ‘‘all-IP’’emphasizes the fact that IP-based protocols are used for all purposes including transport, application-level signaling,mobility, security, QoS, etc. Typically,several wireless and ﬁxed access networks are connectedtoanall-IP corenetwork, as illustrated in Figure4.17 and Figure4.18. Users will be able to use multimedia applications over terminals with software- conﬁgurable radios capable of supporting avast range of radio access technologies such as WLANs, WPANs, UMTS, and cdma2000. In this environment seamless mobilityacross the different access technologies is considered as akey issue by manyvendors and operators.4 -36                                  Broadcasting and Optical Communication Technology                              HTTP   FTP           RTSP   RTCP    RTP                                    TCP                   UDP                                                IP                             UTRAN    GPRS                    Cdma2000                             User    User    802.11 HIPERLAN   User                             plane   plane                    plane                       Access                         network                                              (a)                                                Core Network Signaling                 RSVP            SIP                                                    e.g. SIP                        TCP/UDP                                                                         All-IP                          IP                                             Core                                                                        Network                                        Access Network Signaling GSM-MAP                                                             IS-41                GPRS   Cdma2000  UTRAN    e.g., 24.008, IS-136                Control Control  Control                    UTRAN,                plane    plane   plane   Radio Access Signaling GERAN,           Access            network                                            e.g., RRC, RLC  cdma2000                                              (b)   FIGURE 4.19 Simpliﬁed protocol architecture in an all-IP network architecture; (a) Control Plane, (b) User Plane.    In the all-IP networkarchitecturethe mobile terminals use the IP-based protocols deﬁned by IETF to communicate with the corenetwork and perform, e.g., session/call control and trafﬁc routing.All services in this architecture are provided on top of the IP protocol. As shown in the protocol architectureofFigure 4.19, the mobile networks (such as UMTS, cdma2000, etc.) turn into access networks that provide only mobile bearerservices. The teleservices in these networks (e.g., cellular voice) are only used to supportthe legacy 2G and 3G terminals, which do not supportIP-based applications (e.g.IPtelephony). For the provision of mobile bearerservices, the access networks mainly implement micromobilitymanagement, radio resource management, and trafﬁc management for provisioning of qualityofservice.Micromobilitymanagement in 3GPP access networks is based on GPRS Tunneling Protocol(GTP), and uses ahierarchical tunneling scheme for data forwarding. On the other hand, micromobilitymanagement in 3GPP2 access networks is typically based on IP micromobilityprotocols. Macromobility (inter-domain mobility) is typically based on Mobile-IP, as speciﬁed in RFC 3220 (see http://www.ietf.org/rfc/rfc3220.txt). All these mobilityschemes are discussed in moredetail in the section on ‘‘MobilityManagement Schemes for Mobile Internet.’’   In the shortterm the all-IP corenetwork architecture would be based on the IMS architecture speciﬁed by 3GPP/3GPP2, which in turn is based on the IP multimedia architecture and protocols speciﬁed by IETF.This IMS architecture would provide anew communications paradigm based on integrated voice, video,and data. Youcould call auser’sIMS number,for instance,and be redirected to his Webpage whereyou could have several options, e.g., write amessage for him, recordavoicemessage, click on an alternative number to call if he is on vacation, etc. Youcould placeaSession Initiation Protocol (SIP) call to aserverand update your communication preferences, e.g., ‘‘only my manager can call me, all others are redirected to my Webpage’’ (or vice versa!). At the same time, you could be on aconferencecall.  IP-based Core Networks in the Enterprise Figure4.20 shows how an enterprise could take advantage of an all-IP core network(which is partofamobile Internet environment) in order to minimize its communication costs and increase communications efﬁciency.Computer Networks                                                                  4 -37                       FIGURE 4.20 Deployment of all-IP networks in the enterprise.   The typical IP network of the enterprise could be evolved to an IP multimedia network, which would support (i) IP signaling with the end terminals for establishing and controlling multimedia sessions, (ii) provisioning of QoS, (iii) policy-based admission control, and (iv) authentication, authorization, and possibly accounting. The all-IP network provides an integrated infrastructure for efﬁciently supporting avast range of applications with diverse QoS requirements, and, in addition, provides robust securitymechanisms. The architecture of this all-IP network could be based on the IMS architecture speciﬁed by 3GPP/3GPP2 (see an up-to-date 3GPP TS 23.228 speciﬁcation at www.3gpp.org/ftp/specs/ for adetailed description).   In the example shown in Figure 4.20, an employee in the European ofﬁce could request avoicecall to another employee, e.g., in the U.S. ofﬁce.This request would be routed to the default Proxy-Call Session Control Function (P-CSCF) that servesthe European ofﬁce. This P-CSCF relays the request to the Serving CSCF (S-CSCF) of the calling employee, i.e., to the CSCF that this employeehas previously registered. This S-CSCF holds subscription information of the calling employee and can verify whether he/she is allowed to placethe requested call. In turn, the S-CSCF ﬁnds another S-CSCF with which the called subscriber has registeredand relays the request to this S-CSCF.Note that if the calling employeeinthe European ofﬁce werecalling anormal telephone (PSTN) number in the United States, the call would be routed through the IP networktoabreak-out gatewayclosest to the called PSTN number.This way, the long-distancecharges are saved.   The S-CSCF of the called U.S. employee holds information on the employee’swhereabouts and can route the call to the correct location. In case the called U.S. employee happens to be roaming in Europe, the call would be routed to the appropriate P-CSCF,which currently servesthis employee. It is important to note that, althoughsignaling can travel along path (e.g., from Europe to the United States and then back to Europe), the user-plane path would be the shortest possible.   The supportofroaming is another important advantage of the above architecture. Forinstance,aEuropean employee could take his dual-mode mobile device to the U.S. ofﬁce,and after powering-on his device and registering his current IP address (with his S-CSCF), he would be able to receivemultimedia calls at his standardnumber.4 -38                                  Broadcasting and Optical Communication Technology    Even when the employee is away fromhis ofﬁce and cannot directly attach to the enterprise network(e.g., he is driving on the highway), he can still be reached on his standardnumber.Inthis case the employeeuses, for example, the UMTS network to establish amobile signaling channel to his all-IP enterprise network, which assigns him an IPv6 address.The employee registers this IP address with an S-CSCF (via the appropriate P-CSCF), and thereafterhecan receivecalls at his standardnumber.The signaling mobile channel remains activated for as long as the employee uses UMTS to access the enterprise network. In such ascenario,the UMTS  networkisused only to provide access to the enterprise networkand to supportthe mobile bearers required for IP multimedia services. To establish IP multimedia calls the employee would need to request the appropriate UMTS bearers, each one with the appropriate QoS properties. Forinstance,toreceive an audio- video call, two additional UMTS bearers would be required, one for each media component. The mapping between the application-level QoS and the UMTS QoS, as well as the procedures required to establish the appropriate UMTS bearersare speciﬁed in 3GPP Rel-5 speciﬁcations, speciﬁcally in 3GPP TS 24.008 and 3GPP TS 24.229 (available at www.3gpp.org/ftp/specs/).   If the enterprise network supports amacromobilityprotocol, e.g., Mobile-IP (see section ‘‘Mobility Management Schemes for Mobile Internet’’), it could be possible to provide session mobilityacross the enterprise WLAN and the UMTS network. In this case when the employeemoves fromthe WLAN to the UMTS  he uses Mobile-IP to registerhis new IPv6 address with his home agent. After that anysubsequent terminating trafﬁc would be tunneled from the employee’s home agent to the foreign agent that serves the employee over the UMTS network.   Another roaming scenario is illustrated in Figure 4.21, which involves interworking between the enterprise all-IP network and the mobile operator’sall-IP network. In this scenario,the key aspect is that the employee uses aP-CSCF in the mobile operator’sdomain, i.e., it uses the operator’sIMS system. It is noted that this scenario corresponds to the roaming scenario considered in 3GPP Rel-5 speciﬁcations, and thereforeitmight be the most frequently used in practice.             FIGURE 4.21 Deployment of all-IP networks in the enterprise (mobile operator uses IMS).Computer Networks                                                                  4 -39  Ubiquitous Mobile Internet: Combining Cellular and       Wireless LAN Networks The interworking between 3G cellular and Wireless Local AreaNetworks(WLANs) has been considered as a suitable and viable evolution path towards the next generation of wireless networks, which would be capable of providing anearly ubiquitous mobile Internet service.A3G cellular network can provide relatively low-speed (up to 384 kbps per user) Internet services, but over alarge coverage area. On the other hand, aWLAN can typically provide high-speed Internet services(with an effectivethroughput of tens of Mbps), but over a geographically small area. An integrated 3G/WLAN network combines the strengths of each, resulting in a wide-area wireless system capable of providing users with ubiquitous Internet services, which range from low- speed to high-speed in strategic locations.   However,the 3G/WLAN interworking raises considerable challenges, especially when we demand seamless continuityofInternet sessions across the twonetworks. To deal with these challenges, several 3G/WLAN interworking requirements need to be identiﬁed and fulﬁlled.   Typically the 3G/WLAN interworking requirements are speciﬁed and categorized in terms of several usage scenarios [3,4]. For example, acommon usage scenario is when a3Gsubscriber is admitted to aWLAN environment by reusing his regular 3G credentials, and then obtains an IP connectivityservice (e.g., access to the Internet). In this case, the interworking requirements include supportof3G-based access control, signaling between the WLAN and the 3G network for authentication, authorization, and accounting (AAA) purposes, etc. Other scenarios can call for moredemanding interworking requirements. We may envision, for instance,ascenario in which a3Gsubscriber initiates avideo session in his home 3G network and subsequently transits to aWLAN environment, wherein the video session is continued seamlessly,i.e., without anynoticeable change to the qualityofservice (QoS). In this case, not only 3G-based access control is required, but also access to 3G-based services is needed over the WLAN network, which in turn calls for appropriate routing enforcement mechanisms. More importantly,however,there is need for QoS consistency across 3G and WLAN, which appears to be not verystraightforward given the different QoS featuresoffered by these networks. Indeed, WLANs haveinitially been speciﬁed without paying much attention to QoS aspects and aimed primarily at simple and cost-effectivedesigns. Even with the recent IEEE 802.11e developments [10,11], WLAN QoS still exhibits several deﬁciencies with respect to the 3G QoS. In contrast, 3G cellular networks were built withthe multimedia/Internet applications, trade simplicity, and cost in mind for inherently providing enhanced QoS in wide-area environments.   In the rest of this section we examine some aspects of the seamless continuityofInternet servicesacross UMTS and WLAN. In this context we address several issues such as routing enforcement, access control, differentiation between the trafﬁc of regular WLAN data users and UMTS roamers, etc. The framework for this discussion is the consideration of apractical UMTS/WLAN interworking architecture, which conforms to the 3GPP speciﬁcations [4,5] and other interworking proposals found in the technical literature[3].  UMTS/WLAN Interworking Architecture The end-to-end interworking architectureweare considering is illustrated in Figure4.22 and it is compliant with the proposals[3,4]. Below we brieﬂy discuss the main characteristics of this architecture. Note that our goal is not to provide acomprehensivedescription, but rather to deﬁne the keyaspects of apractical environment that can enable ubiquitous mobile Internet services. For more detailed information on the considered architecture the interested reader is referred to Ref. [3] and Ref. [4].   As shown in Figure4.22, the 3G network supports access (via the Internet) to avarietyofIPmultimedia services over tworadio access technologies: UMTS Terrestrial Radio Access (UTRA) and WLAN access. Access control and trafﬁc routing for 3G subscribers in UTRA is entirely handled by the UMTS Packet-Switched (PS) network elements, which encompass the Serving GPRS SupportNode (SGSN) and the GatewayGPRS SupportNode (GGSN) [6]. On the other hand, access control and trafﬁc routingfor 3G subscribers in WLAN  ( UMTS roamers)isshared among the WLAN and the UMTS network elements as discussed below.The importantassumption we make, as shown in Figure4.22, is that 3G subscribers can change radio access technologyand keep using their ongoing multimedia/Internet sessions in aseamless fashion. Thus, we assume that seamless Internet service continuity is provided.4 -40                                  Broadcasting and Optical Communication Technology      FIGURE 4.22 The considered end-to-end interworking architecturefor seamless multimedia session continuity.    The WLAN  access network maybeowned either by the UMTS operator or by anyother party (e.g., apublic WLAN   operator or an airportauthority), in which case the interworking is enabled and governed by appropriate business and roaming agreements. As shown in Figure4.22 in atypical deployment scenario the WLAN  network supports various user classes, e.g., UMTS roamers and regular WLAN data users (i.e., no 3G subscribers). Differentiation between these user classes and enforcement of corresponding policies is typically enabled by employing several Service Set Identiﬁers (SSIDs) [7]. Forexample, the regular WLAN data users mayassociate with the SSID that is periodically broadcast by the Access Point (AP) (denoted as SSID(b)), whereas the UMTS roamers may associate with another SSID that is also conﬁgured in the AP,but not broadcast (denoted as SSID[g]). In this case, the WLAN can apply distinct access control and routing policies for the twouser classes, and can forward the trafﬁc of WLAN data users, e.g., to the Internet and the trafﬁc of UMTS  roamers to the UMTS PS corenetwork (as shown in Figure4.22). Such routingenforcement is vital for supporting seamless service continuityand can be implemented as discussed [3]. Moreover,different AAA mechanisms could be used for the different user classes.   Forenabling interworking with WLANs, the UMTS PS core network incorporates three new functional elements: the 3G AAA Server,the WLAN Access Gateway (WAG), and the Packet Data Gateway (PDG). The WLAN   needs also to supportsimilar interworking functionality to meet access control and routing enforcement requirements. The 3G AAA Serverinthe UMTS domain terminates all AAA signaling originated in the WLAN  that pertains to UMTS roamers. This signaling is securely transferred across the Wr/Wb interface, which is typically based on Radius [8] or Diameter [9] protocols. The 3G AAA Serverinterfaces with other 3G components such as the WAG, the PDG, and Home Subscriber Server(HSS), which stores information deﬁning the subscription proﬁles of 3G subscribers. The 3G AAA Servercan also route AAA signaling to/from another 3G networks, in which case it servesasaproxy,and it is referred to as 3G AAA Proxy [3].Computer Networks                                                                  4 -41    As shown in Figure4.22, trafﬁc from UMTS roamers is routed to the WAGacross the Wn interface and ﬁnally to the PDG across the Wp interface. This routing is enforced by establishing appropriate trafﬁc tunnels after asuccessful access control procedure. The PDG functions much like aGGSN in aUMTS PS core network. It routes the user data trafﬁc between the mobile station (MS) and an external Packet Data Network (PDN) (in our case, the Internet), and servesasananchor point that hides the mobilityofthe MS within the WLAN domain. The WAGfunctions mainly as aroute policy element, i.e., ensures that user data trafﬁc from authorized MSs is routed to the appropriate PDGs, located either in the same, or in aforeign UMTS network.  Seamless Internet Session Handover Although Figure4.22 shows the architecture that can supportseamless Internet session continuity, it does not address the dynamics of handover procedure, which is especially important for the provision of seamless continuity. To further elaborate on this key procedure, we depict in Figure 4.23 atypical signaling diagram that pertains to asituation whereanMShands over from UMTS to WLAN in the middle of an ongoing Internet session (e.g., aVoIP session). The establishment of the Internet session is triggered at instant A and in response the MS starts the Packet Data Protocol (PDP) context establishment procedure for requesting the appropriate QoS resources(described by the ‘‘Req. QoS’’Information Element [IE], [12]). The UMTS network acknowledges the request and indicates the negotiated QoS resources (speciﬁed by the ‘‘Neg. QoS’’IE) that could be provided. After that, IP trafﬁc on the user plane commences and the Internet session gets in progress. At some point the MS enters aWLAN coverage areaand it starts receiving                         MS                                  UMTS              WLAN                                 Attach to UMTS PS Domain              Initiate an                       A          Internet session                          Activate PDP Context Request (Req. QoS IE)                            Activate PDP Context Accept (Neg. QoS IE)                                UMTS QoS Resources Reserved        Internet session                 Beacon (SSID(b), QoS Capability)           in progress                                            Probe Request (SSID(g))                                           Probe Response (SSID(g))                       B  Preferred WLAN detected                        C  Decision to handover to WLAN                                          Open System Authentication                                    Association Request (SSID(g), QoS Capability)       Internet session                  Association Response (Status)           suspended                                 UMTS-based Access Control and Tunnel Establishment                                        Action.ADDTS Request (TSPEC)                                     Action.ADDTS Response (Status, TSPEC)                                         WLAN QoS Resources Reserved       Internet session             resumes  FIGURE 4.23 Typical signaling during handover of an Internet session from UMTS to WLAN (HCCA availabilityis assumed).4 -42                                  Broadcasting and Optical Communication Technology  Beacons1 from the nearby Access Points (APs). We assume that this can happen concurrentlywith the ongoing Internet session because, althoughthe MS has one transceiver available,itcan periodically decode signals on other frequency channels for inter-system handover purposes. The MS may need to check if the detected WLAN supports one of its preferred SSIDs before considering it valid for inter-system change. For this purpose, the MS probes for apreferred SSID,denoted as SSID(g), according to the applicable procedures in Ref. [7].   At instant C the MS takes the decision to handover to the detected WLAN and thus suspends the ongoing Internet session. This may demand further signaling withthe UMTS, but we omit this for simplicity. After switching to the WLAN channel, the normal 802.11 authentication and association procedures [7] are carried out. Subsequently,the UMTS-based access control procedureisexecuted in which the MS is authenticated and authorized by means of its regular 3G credentials [3,4]. At this stage, atunnel will also be established for routingfurther IP trafﬁc fromthe MS to aUMTS entrypoint (the WAGaccording to Figure4.22). Next, the MS uses 802.11e [10] QoS signaling (assuming it is supported by the WLAN) to reserve the appropriate resourcesfor its suspended Internet session. The Trafﬁc Speciﬁcation (TSPEC) element carries aspeciﬁcation of the requested QoS resources. For the objectives of seamless continuity, it is apparent that TSPEC needs to be set consistently with the QoS negotiated in the UMTS system. After this point, the Internet session is ﬁnally resumed in the WLAN, possibly after some high-layer mobilitymanagement procedures (e.g., Mobile IP or SIP).   From the above discussion it becomesevident that vertical handovers from UMTS to WLAN (and vice versa) present several challenges, especially for minimizing the associated latencies and the interruption of ongoing Internet sessions. Apartfrom that, however,the maintenanceofconsistent QoS across the UMTS and WLAN  networks is equally challenging.  Mobility  Management     Schemes   for Mobile Internet The main aim of mobilitymanagement across different access networks is to ensure that the user is ‘‘always- connected,’’ or better yet ‘‘always-best-connected’’(ABC). ABC means that the network offers aset of access technologies and mobilitymanagement mechanisms that allowthe users to be connectedwith the most appropriate available technologyatall times in order to enjoy the best possible service. ‘‘Best’’isusually deﬁned separately for each user as partofhis/her proﬁle, and it can be afunction of service quality, cost, terminal capabilities, personal preferences, etc. In anycase, the network should havethe ﬂexibilitytoadjust the access technology, and activate the appropriate mobilitymanagement mechanisms in ordertobeconsistent with the user’sproﬁle. This should be performed withnoorminimum intervention of the user,leading to what is called ‘‘invisible network.’’ Consequently,aset of available access technologies and mechanisms should be integrated in asingle architecture supporting multiple services, adjustments at all layers, and vertical handover capabilities between different technologies [13].   The ABC concept contains the idea of ubiquitous connectivityatany time and anyplace. To achieve this goal the underlying assumption is that the user that is always-connected is not hindered in geographic or motion restrictions. The user can moveeither by foot or through other means (car,train, ship,etc.) and still maintain the best levelofconnectivitypossible. Mobilitysupportisinherent for anynew ABC architecture. The ﬁrst levelofmobilitysupportfocuses on the infrastructure design. The mobile access networks deployed consist of geographically dispersed base stations connected in ahierarchical fashion that allows the mobile device to connect successively to neighboring base stations as it moves. This is the model that all current cellular networks employ(GSM, GPRS, UMTS), and also the model upon which the Internet communityhas built mobilitysupportfor IP enabled devices through Mobile IP [14]. Macromobility Mobile IP [14] allows an MS to maintain connectivitywhile changing its point of attachment. This is not possible with standardIPwhererouting assumes apermanent position for each terminal. According to    1 From the Beacons the MS discovers what particular QoS features the WLAN supports, if any.Computer Networks                                                                  4 -43                                                                  Home Agent                  Communicating Node                                            Tunnel                                                      Foreign Agent                  Mobile Node                              FIGURE 4.24 Basic Mobile IP functionality.   Mobile IP,anMSisgiven  along-term IP address on the network that it is registered(i.e., the home network), referred to as the ‘‘home address.’’This home address is administered in the same way as a permanent IP address provided to astationaryhost. When away from its home network, atemporary‘‘care- of address’’isassociated with the MS and reﬂects its current point of attachment. The MS uses its home address as the source address of all IP packets that it sends. Mobile IP deﬁnes twobasic functional entities to implement its operation, the Home Agent (HA) and the Foreign Agent (FA). The HA is arouter on an MS’shome network, which accepts all packets destined to the MS and redirects them, throughstandard IP tunnelling,tothe current position of the MS (i.e., the care-of address). To maintain updated location information, the HA is informed by the MS for everychange of the care-of address through aspecial location update message. The FA is arouter on an MS’svisited network, which provides routing services. The FA detunnels and delivers to the MS packets that weretunnelled by the HA. For packets sent by the MS, the FA mayserve as adefault router.   Although Mobile IP provides asolution to the problemofterminal movement, its performancedepends on the distance between the home network and the current point of attachment. More speciﬁcally,when trafﬁc is sent to the MS, packets are ﬁrst routed to the HA, which encapsulates them and tunnels them to the FA for delivery.Asisevident from Figure 4.24, the route taken by these packets is triangular.The most extreme case of routingcan be observedwhen the communicating node and the MS are closely located to each other and far away from the HA. Additionally,incases when the communicating node is behind aﬁrewall, MS’soutgoing trafﬁc transmitted from avisiting network may be rejected if the ﬁrewall does not allowincoming trafﬁc from the speciﬁc direction. Both these problems are addressed by Mobile IPv6, the evolution of Mobile IP that uses capabilities introduced by IPv6 to improve protocol’sperformance.   According to Mobile IPv6 [P2], the functionality of the FA is included in everyMS, which has the ability to de-tunnel incoming packets without the need of an external agent, making the whole scheme much simpler.Additionally,upon receipt of the ﬁrst packet from acommunicating node, the MS can send a location update message directly to this node with the information about its current care-of address.Inthis way, the communicating node can change the destination address of its outgoing packets from the home address to the care-of address of the MS and eliminate the triangular routing problem of legacy Mobile IP. Finally,the problem of ﬁrewalls is solved by setting up areverse tunnel, from the MS to the HA, and sending all outgoing packets throughthis tunnel. Althoughthis results in triangular routing in the reverse direction, packets are routed to the communicating node throughthe MS’shome network, and are accepted by the ﬁrewall.4 -44                                  Broadcasting and Optical Communication Technology  Micromobility As the Internet technology penetrates moreand more into everyconnectivityaspect in the research community, there has been much work done in optimizing mobilitysupportfor IP devices. One of the ﬁrst observations was that the Mobile IP standard was not suitable for high-mobility, small geographic areas. Mobile IP is optimized for macromobility of relatively slow moving hosts, as it requires that, after each migration, alocation update message is sent to apossibly distant HA, potentially increasing handofflatency and load on the network. To handle fast moving hosts in relatively small areas, the so-called micromobility protocols evolved. These protocols operate within an administrativedomain to achieve optimum mobility supportfor fast moving users within the domain’s boundaries. Most micromobilityprotocols establish and maintain soft-state host-speciﬁc routes in the micromobility enhancedrouters. The inter-domain mobility supportisleft to standardMobile IP,however.Three main representatives of this categoryare brieﬂy presented: the Cellular IP,HAWAII, and Hierarchical Mobile IP.   According to Cellular IP,none of the nodes in the access networkknows the exact location of amobile host. Packets are routed to the mobile host on ahop-by-hop basis, where each intermediate node only needs to knowonwhich  of its outgoing ports to forward packets [16]. To minimize control messaging,regular data packets transmitted by mobile hosts on the uplink direction are used to establish host location information. The path taken by these packets is cached in the intermediate nodes in order to locate the mobile node’s current position. To route downlink packets addressed to amobile host, the path used by recent uplink packets transmitted by the mobile host is reversed. When the mobile host has no data to transmit, it periodically sends aroute update packet to the gatewaytomaintain its downlink routingstate. Following the principle of passive connectivity, idle mobile hosts allowtheir respectivesoft-state routing cache mappings to time out. These hosts transmit paging update packets at regular intervals deﬁned by apaging update time. The paging update packet is an emptyIPpacket addressed to the gatewaythat is distinguished from aroute update packet by its IP type parameter.Paging update packets are sent to the base station that offers the best signal quality. Similar to data and route update packets, paging update packets are routed on ahop-by-hop basis to the gateway. Intermediate nodes may optionally maintain paging caches that havethe same format and operation as a routing cache except for twodifferences. First, paging cache mappings havealonger timeout period called paging-timeout. Second, paging cache mappings are updated by anypacket sent by mobile hosts including paging-update packets. Paging cache is used to avoid broadcast searchprocedures found in cellular systems. Intermediate nodes that havepaging cache will only forward the paging packet if the destination has avalid paging cache mapping and only to the mapped interface(s). Without anypaging cache the ﬁrst packet addressed to an idle mobile host is broadcast in the access network. While the packet does not experienceextra delayitdoes, however,load the access network. Using paging caches, the network operator can restrict the paging load in exchange for memoryand processing cost [17].   HAWAII segregates the network into ahierarchyofdomains loosely modeled on the autonomous system hierarchyused in the Internet [18]. The gateway into each domain is called the domain root router. Each host is assumed to haveanIPaddress and ahome domain. While moving in its home domain, the mobile host retains its IP address. Packets destined to the mobile host reachthe domain root router based on the subnet address of the domain, and are then forwarded over special dynamically established paths to the mobile host. When the mobile host moves into aforeign domain, we revert to traditional Mobile-IP mechanisms. If the foreign domain is also based on HAWAII, then the mobile host is assigned acolocated care-of address from its foreign domain. Packetsare tunneled by the home agent to the care-of address, according to Mobile IP.When moving within the foreign domain, the mobile host retainsits care-of address unchanged, and connectivityis maintained using dynamically established paths. The protocol contains three types of messages for path setup: power-up,update, and refresh. Amobile host that ﬁrst powers up and attaches to adomain sends a path setup power-up message. This has the effect of establishing host speciﬁc routes for that mobile host in the domain root router and anyintermediate routers on the path towards the mobile host. Thus, the connectivityfrom that domain root router to the mobile hosts connectedthrough it forms avirtual tree overlay. Note that other routers in the domain havenospeciﬁc knowledge of this mobile host’sIPaddress. While the mobile host moves within adomain, maintaining end-to-end connectivitytothe mobile host requires special techniquesComputer Networks                                                                  4 -45  for managing user mobility. HAWAII uses path setup update messages to establish and update host-based routing entries for the mobile hosts in selective routers in the domain so that packets arriving at the domain root router can reachthe mobile host withlimited disruption. The choice of when, how,and which routers are updated constitutes aparticular path setup scheme. The HAWAII path state maintained in the routers is characterized as ‘‘soft-state.’’ This increases the robustness of the protocol to router and link failures. The mobile host infrequently sends periodic path refresh messages to the base station to which it is attached to maintain the host-based entries, failing which they willberemovedbythe base station. The base station and the intermediate routers, in turn, sends periodic aggregate hop-by-hop refresh messages towards the domain root router.Path setup messages are sent to only selected routers in the domain, resulting in very little overhead associated with maintaining soft state.   Hierarchical Mobile IP (HMIP) [19] is an extension of the traditional Mobile IP protocol used to cover micromobilityscenarios. It introduces anew function, the MobilityAnchor Point (MAP), and minor extensions to the mobile host operation. The correspondent node and HomeAgent operation are not affected. AMAP is arouter located in anetwork visited by the mobile host, and is used as alocal home agent. Just like Mobile IP,HMIP is independent of the underlying access technology, allowing mobilitywithin or between different types of access networks. The operation of the protocol can be brieﬂy described as follows. Amobile host entering aforeign network will receiveRouter Advertisements containing information on one or more local MAPs. The mobile host can bind its current location with atemporaryaddress on the foreign subnet. Acting as alocal home agent, the MAP will receiveall packets on behalf of the mobile node it is serving and will encapsulate and forward them directly to the mobile node’scurrent address.Ifthe mobile node changes its current address within the foreign network, it only needs to register the new address with the MAP.Hence, only the regional address needs to be registered with correspondent nodes and the home agent, which does not havetochange as long as the MS moveswithin the same network. This makes the mobile node’smobility transparent to the correspondent nodes with which it is communicating and its home network. An HMIP- aware mobile host with an implementation of Mobile IP should choose to use the MAP when discovering such capabilityinavisited network. However,insome cases the mobile node mayprefer to simply use the standard Mobile IP implementation. Forinstance,the mobile host may be located in avisited network within its home site. In this case, the home agent is located near the visited network and could be used instead of aMAP.Inthis scenario,the mobile host would havetoupdate the home agent whenever it moves.  QoS and Mobility Support In order to improvethe QoS provided during handovers, the mobilitysupporttechniques described above should be combined with IP QoS mechanisms such as the ResourcereSerVation Protocol (RSVP) [20]. Designed for ﬁxed networks, RSVP assumes ﬁxed end-points, and for that reason its performanceis problematic in mobile networks. When an active MS changes its point of attachment with the network (e.g., in handover), it has to reestablish reservations with all its communicating nodes along the new paths. For an outgoing ﬂowthe MS has to issue aPATHmessage immediately after the routing change, and wait for the corresponding RESV message beforestarting data transmission throughthe new attachment point. Depending on the hops between the sender and the receiver,this can cause considerable delays resulting in temporary service disruption. The effects of handover are even moreannoying in an incoming ﬂow because the MS has no powertoimmediately invokethe path reestablishment procedure. Instead, it has to wait for anew PATH message, issued by the sender,beforeresponding with aRESV message in order to complete the path reestablishment. Simply decreasing the period of the soft state timers is not an efﬁcient solution, because this could increase signaling overhead signiﬁcantly.   Anumber of proposals can be found in the literatureextending RSVP for either inter-subnet or intra- subnet scenarios. Forintra-subnet scenarios, proposals that combine RSVP with micromobilitysolutions, such as Cellular IP,can reduce the effects of handover on RSVP sinceonly the last partofthe virtual circuit has to be reestablished. Forinter-subnet scenarios, the existing proposals include advancereservations, multicasting,RSVP tunneling,etc. Belowwefocus in intra-subnet solutions, which can better ﬁt in WLANs. Talukdar et al. [21] proposed Mobile RSVP (MRSVP), an extension of RSVP that allows the MS to preestablish paths to all the neighboring cells. All reservations to these cells are referred to as passive4 -46                                  Broadcasting and Optical Communication Technology  reservations, in contrast to the active reservations in the cell that the MS actually is. When the MS movesfrom an old cell to anew one, the reservations in the new cell become active,while the reservations in the old cell change to passive. Althoughthis proposal reduces the handover delays for path reestablishment, it requires RSVP to be enhanced to supportapossible large number of passive reservations, while each AP has to maintain alot of state information regarding activeand passivereservations. Additionally,new real-time ﬂows havetowait for all the necessary(passiveand active)reservations beforestarting transmission, resulting in apossible highblocking rate. Tseng et al. [22] proposed the Hierarchical MRSVP (HMRSVP) in an attempt to reduce the number of required passivereservations. According to HMRSVP,passive reservations are performed only when an MS is moving in the overlapping area of two or more cells.   According to Kuoetal. [23], RSVP is extended with twoadditional processes, aresourceclear and a resourcerereservation in order not to release and reallocate reservations in the common routers of the old and the new path. This solution performs well in reducing the path reestablishment time, but modiﬁes the RSVP protocol signiﬁcantly.Chen et al. [24] proposed an RSVP extension based on IP multicast to supportMSs. RSVP messages and actual IP datagrams are delivered to an MS using IP multicast routing.The multicast tree, rootedateach MS, is modiﬁed dynamically everytime an MS roamstoaneighboring cell. Hence, the mobility of an MS is modeled as atransition in multicast group membership.Inthis way, when the MS movestoa neighboring cell that is covered by the multicast tree, the ﬂow of data packets can be delivered to it immediately.This method can minimize service disruption due to reroutingofthe data path during handovers, but it introduces extra overhead for the dynamic multicast tree management and requires for multiple reservations in everymulticast tree.   All these approaches, while trying to improvethe performanceofRSVP in mobile networks, either result in low resource utilization due to advancereservations, or requireconsiderable modiﬁcations of protocols and network components operation. In micromobilityenvironments only asmall partofthe path is changed, while the remaining circuit can be reused.Accordingly,ascheme for partial path reestablishment can be considered, which handles discovery and setup of the new partbetween the crossover router and the MS. The number of hops required to set up the partial path during ahandover depends on the position of the crossover router.Suchascheme can reduceresource reservation delays and provide better performancefor real-time services, without affecting the operation of RSVP considerably.Paskalis et al. [25] haveproposed ascheme that reduces the delay in data path reestablishment without reserving extra resources, while it requires modiﬁcations only in the crossover router between the old and the new path. According to this scheme, an MS mayacquire different ‘‘local’’care-of addresses while moving inside an access network, but is always reachable by a‘‘global’’care-of address through tunneling,address translation, host routing,orany other routingvariety, as suggested in various hierarchical mobilitymanagement schemes. The crossoverrouter,referred to as RSVP Mobility Proxy,handles resource reservations in the last partofthe path and performs appropriate mappings of the global care-of address to the appropriate local care-of address.Asimilar approach for partial path reestablishment has also been proposed by Moon et al. [26]. Accordingtothis scheme, if an MS is asender,it sends an RSVP PATH message after the route update is completed. When the RSVP daemon on the crossover router,which is determined by the route update message, receives an RSVP PATH message after amobility event, it immediately sends an RSVP RESV message to the MS without delivering it to the original receiver.If an MS  is areceiver,the RSVP daemon on the crossoverrouter can trigger an RSVP PATH message immediately after detecting anychanges to the stored PATH state or receiving anotiﬁcation from the underlying routing daemon. This PATH message can be generated based on the PATH state stored for the ﬂow during previous RSVP message exchanges.  Mobility Management  Architectures Althoughthe mechanisms described above provide the means of how to perform handover in mobile IP networks, they do not include the intelligencetodecide when to handover,and where to handover.Inthe case of UMTS/WLAN  interworking (as described above), the decision for the MS is easy,assuming that thereare simple rules based on aspects such as the services used and the terminal capabilities (for example, the WLAN could simply be the preferred access system for all servicesand under all conditions). But what happens whenComputer Networks                                                                  4 -47  the terminal has to choose from awide set of available access technologies (e.g., UMTS, GSM, WLAN, Satellite) or even multiple access systems of the same technology(e.g., multiple WLANs)? Nowadays, it is relatively standardized for an MS to moveseamlessly based on the infrastructureofasingle provider,i.e., using its base stations, accounting services, and other facilities. The notion of ABC, however,supports the use of the ﬁttest existing infrastructureatany time. According to this idea, the user is aware of the multiple surrounding mobilitysupportinfrastructures and can choose to connect to the ﬁttest at anytime (possibly judging from multiple parameters such as cost, bandwidth, technologycapabilities, available services, etc.). Multiple connectionsmay be initiated and deployed for the optimum result. The connectionscan stem from the same mobile device to multiple access networks, or even from cooperating mobile devices to different access network technologies. Such multi-homing capabilities must be built into the operating system of the terminal handling the devices, and are still in research stages. Examples of available infrastructurealternatives include the base station of another network provider (of the same technology), the base station in an access network of another technology, or even connectivityshared in an ad-hoc manner from apeer MS. To provide such an increased ﬂexibilitytothe user,anadvanced system architecture is required consisting of functional entities in both the MS and the network, able to built adistributed decision system that guides the MS to decide when to handover and to which of the available access systems.   Anumber of researchprojects havebeen initiated in the last few years towards designing an integrated architecture that can provide ABC provision to the user [27–29]. To give ahint on the design principles of such an architecture, the basic points of the CREDO system [27] are brieﬂy described. CREDO’s approach is sketched in Figure4.25. Asalient feature of this approach is that both the networkand the terminals contribute towards optimal system operation. At the network’sside, an advanced Network and Service Management System (NSMS) coordinates the various access systems towards achieving joint management of their resources, in particular through trafﬁc load balancing among these segments. NSMS is also capable for QoS provisioning to users and for resource provisioning in reply to requests from Service Providers. The multimode MSs, besides being capable of operating over diverse access systems, incorporate functionality provided through amanagement module called Terminal Station Management System (TSMS) for the exploitation of this capability. The TSMS on aterminal interacts with NSMS (throughmessage exchanges, according to an appropriate protocol) towards network-drivenselection of the access system to which the terminal is assigned. The interaction ensures that the ‘‘local view’’ofthe terminal (radio conditions in the area,                                FIGURE 4.25 The CREDO architecture.4 -48                                  Broadcasting and Optical Communication Technology  speciﬁc content servicesreceived over that terminal, the QoS levelsassociated with these services, etc.) and the global view of the network (trafﬁc load over the various segments, the need to avoid congestion towards preserving QoS, etc.), are beneﬁcially combined.   Twoprimaryobjectives are addressed by the NSMS in CREDO:    1. To guide individual users (terminals) towards the selection of the appropriate access network and      the appropriate QoS level per service,based on the requested services, user preferences, terminal      proﬁles,and networkavailability. This is achieved throughashort-term optimization process, operating      in near real-time.    2. To monitor the network infrastructure,assess the network and service-level performance, and employ      this assessment for an optimal accommodation of the aggregate demand volume. The latter is achieved      through amid-term optimization process that takesinto account system-wideinformation about the      status and policies of the networks in the composite environment and the user preferences and terminal      proﬁles.The results of the mid-term optimization (which also serve as apreconﬁguration for      subsequent short-term optimizations) are applied througharedistribution of terminals over the      available radio networks.   The TSMS, on the other hand, is the core of the CREDO terminal and provides the following functions:    1. It receives service start and stop requests from the applications. This way it can keep track of all the      currently running applications.    2. It monitors the terminal status: TCP/IP status, network interface status, application status, etc.    3. It reports all the gathered information to the NSMS.    4. Together with the NSMS, it selects the best access network to use at each moment.    5. It manages the terminal network conﬁguration and provides the Mobile IP implementation. It      conﬁgures the network drivers and the TCP/IP stack according to the decisions it takes.   As clearly shown from this brief overview,increased functionality is required in both the network and the terminal, in order to implement adecision-making mechanism that allows smooth ABC provision. Research in this area is ongoing,but the complexityofthe problem and the large number of parameters ask for more detailed, complete, and yet feasible solutions.  Concluding   Remarks From the abovediscussion, it is clear that the mobile Internet is both an opportunityand abig challenge. The opportunity is to provide afully integrated system, offering always-best-connected serviceswithout anyuser intervention. Asystem that willgivethe user the conﬁdence that, by simply opening his mobile device,hewill get the best possible service in terms of quality, cost, availability, device capabilities, etc. The challenge is to fulﬁll the large number of requirements imposed towards this provision.   In short, the keyaspects of the evolution toward afully integrated mobile Internet include the following:      . Evolution towards highly heterogeneousnetworks with several access technologies (both wiredand       wireless) and intelligent mobilitymanagement that supports fast vertical handovers and seamless       session mobility. In this context, the interworking between 3G cellular and WLANs provides akey       evolutionarypath that can pavethe way to other types of integration, e.g., with ﬁxed and wireless access       systems such as DVB-T and ADSL.     . Evolution towards all-IP-based networks, i.e., networks that supportIP-based application signaling,       mobilitymanagement, qualityofservice,aswell as IP-based transportinthe coreand network access.     . Advanced IP Multimedia applications, which are tightly integrated with the equivalent applications in       the Internet. VoiceoverIP, video/audio streaming,instant messaging,presence, press to talk, and other       serviceswill be enabled through the use of IP-based signaling protocols such as SIP.     . Provisioning of end-to-end QoS in order to supportthe demanding multimedia applications over       diverse access media.Computer Networks                                                                  4 -49       . Evolution of mobile terminals towards software-conﬁgurable radios with capabilities to be       reprogrammed over the air and supportmanyradio access technologies across manyfrequency bands.     . Robust and highly sophisticated securityand AAA mechanisms and protocols. See Ref. [1] for further       discussion.     . Adaptation of IPv6.     . Integration with Wireless Personal Area Network(WPAN) technologies.  References  1.  A.K. Salkintzis, Mobile Internet: Enabling Technologies and Services,inElectrical Engineering and Applied      Signal Processing Series, Boca Raton, FL: CRCPress, 2004, ISBN 0-8493-1631-6.  2.  A.K. Salkintzis, C. Fors, and R.S. Pazhyannur,‘‘WLAN-GPRS integration for next generation mobile      data networks,’’ IEEE Wireless Commun.,vol.9,no. 5, pp.112–124, 2002.  3.  A.K. Salkintzis, ‘‘Interworking techniques and architecturesfor WLAN/3G integration towards 4G      mobile data networks,’’ IEEE Wireless Commun.,vol. 11, no.3,pp. 50–61, 2004.  4.  3GPP TS 23.234 v6.0.0, ‘‘3GPP system to WLAN Interworking; System Description (Release 6),’’      March 2004.  5.  3GPP TR  22.934 v6.2.0, Feasibility Study on 3GPP System to WLAN Interworking (Release 6),      September,2003.  6.  3GPP TS 23.060 v5.6.0, General Packet Radio Service (GPRS); Service description; Stage 2(Release 5),      June 2003.  7.  IEEE standard 802.11, Wireless LAN Medium Access Control (MAC) and Physical Layer (PHY)      Speciﬁcations,1999.  8.  C. Rigney et al., Remote Authentication Dial in User Services (RADIUS),IETF RFC 2138, April 1997.  9.  P. Calhoun et al., Diameter Base Protocol,IETF RFC 3588, September,2003. 10.  IEEE draft standard802.11e/D8.0, Medium Access Control (MAC) Quality of Service (QoS)      Enhancements,February2004. 11.  Stefan Mangold et al., ‘‘Analysis of IEEE 802.11e for QoS supportinwireless LANs,’’ IEEE Wireless      Commun.,6,40–50, 2003. 12.  3GPP TS 24.008 v5.12.0, Mobile Radio Interface Layer 3Speciﬁcation; Core Network Protocols; Stage 3      (Release 5),June 2004. 13.  E. Gustafsson and A. Jonsson, ‘‘Always best connected,’’ IEEE Wireless Commun.,February2003. 14.  C. Perkins, Ed., IP Mobility Support for IPv4,IETF RFC 3344, August 2002. 15.  D. Johnson, C. Perkins, and J. Arkko, Mobility Support in IPv6,IETF RFC3775, http://www.ietf.org/rfc/      rfc3775.txt, June 2004. 16.  A.G. Valko,‘‘Cellular IP —anew approach to internet host mobility,’’ ACMComput. Commun. Rev.,      January1999. 17.  A.T.Campbell, J.Gomez, and A.G. Valko, An Overview of Cellular IP,IEEE Wireless Communications      and Networking Conference (WCNC), NewOrleans, September 1999. 18.  R. Ramjee, K. Varadhan, L. Salgarelli, S. Thuel, S.Y.Wang,and T. La Porta, ‘‘HAWAII: adomain-based      approach for supporting mobilityinwide-area wireless networks,’’ IEEE/ACM Trans. Network.,      June 2002. 19.  H. Soliman, C. Castelluccia, K. El-Malki, and L. Bellier, Hierarchical MIPv6 Mobility Manage-      ment (HMIPv6),Internet Draft, http://www.ietf.org/internet-drafts/draft-ietf-mipshop-hmipv6-02.txt,      June 2004. 20.  R. Braden et al., Resource ReSerVation Protocol (RSVP) —Version 1Functional Speciﬁcation,IETF      RFC2205, September 1997. 21.  A. Talukdar et al., ‘‘MRSVP:aresource reservation protocol for an integrated services network with      mobile hosts,’’ J. Wireless Networks,vol. 7, no.1,2001. 22.  C.-C. Tseng et al., ‘‘HMRSVP: ahierarchical mobile RSVP protocol,’’ in Proc. Int.Workshop Wireless      Networks Mobile Comput. (WNMC),Valencia, Spain, April 2001.4 -50                                  Broadcasting and Optical Communication Technology  23.  G.-S. Kuoand P.-C. Ko,‘‘Dynamic RSVP for mobile IPv6 in wireless networks,’’ in Proc. IEEE Veh.      Technol. Conf. (VTC) ,Tokyo,Japan, May2000. 24.  W.-T.Chen and L.-C. Huang,‘‘RSVP mobilitysupport: asignalling protocol for integrated services      internet with mobile hosts,’’ in Proc. INFOCOM,Tel Aviv,Israel, March 2000. 25.  S. Paskalis et al., ‘‘An efﬁcient RSVP/mobile IP interworking scheme,’’ ACMMobile Networks J.,vol. 8,      no.3,2003. 26.  B. Moon and A.H. Aghvami, ‘‘Reliable RSVP path reservation for multimedia communications under IP      micromobilityscenario,’’ IEEE Wireless Commun.,October 2002. 27.  H.Y.Lach and M. Catalyna, Network access co-ordination to complement IP mobility protocols,http://      www.watersprings.org/pub/id/draft-lach-nac-00.txt, June 2003. 28.  W. Zhang,J.Jaehnert, and K. Dolzer,‘‘Design and evaluation of ahandover decision strategyfor fourth      generation mobile networks,’’ Proc. VTC2003-Spring,Jeju, Korea, April 2003. 29.  M.A. Ro´ nai, R. To¨ njes, M. Wolf, and A. Petrescu, ‘‘Mobilityissues in OverDRiVE mobile networks,’’      Proc. IST Mobile Wireless Commun. Summit,Aveiro, Portugal, pp.287–291, June 2003.   4.5    Quality    of Service   in Packet-Switched       Networks Stan  McClellan    and  Remzi    Seker  This chapter discusses several technologies and concepts that are important in the provisioning of ‘‘Qualityof Service’’ (QoS) in packet-switched networks. Unfortunately,‘‘QoS’’isanoverloaded term often used to describe manythings unrelated to data transmission. In packet-based networks, the techniques that provide performanceguarantees for data streamsare somewhat limited. Here, we use ‘‘QoS’’todescribe anetwork’s abilitytoprovide performanceguarantees for the preferential deliveryofpacket streams, and we discuss the technologies and architectures that are generally incorporated into aQoS frameworkfor IP-based networks. This framework lets network elements discriminate between particular trafﬁc streams and then treat those streams in aparticular manner,subject to broad constraints on forwardingperformance.  Introduction  and  Background Much  like beauty, the notion of QualityofService (QoS) for network communications is in the eye of the beholder.For example, under similar transmission conditions, the ‘‘quality’’ofvoice, video,orother data maybeinterpreted very differently at the receiver.Asaresult, it is almost impossible to separate the practical deployment of QoS capabilities in anetwork from athorough evaluation of those capabilities in the context of an application (or class of applications). Unfortunately,since application-level requirements are manyand highly variable, athorough evaluation of QoS technologies in the context of all applications is impossible. Additionally,due to the real-world constraints of existing networkresources, acompromise   between ‘‘theoretically desirable’’ and ‘‘practically implementable’’isanoverriding requirement.   Althoughmuch  progress has been made towards an acceptable balancebetween the complexityand performanceofpacket-based QoS technologies, several signiﬁcant deterrents remain, particularly for networks based on the Internet Protocol suite (IP). Some of the most signiﬁcant issues relate to the heterogeneous, multi-domain, production nature of the distributed Internet and the lack of arobust approach to ensuring QoS requests across domain boundaries. Additionally,the lack of auniﬁed interface between user and network for communicating and negotiating QoS requirements prevents activeparticipation by applications in the end-to-end process. Even if these issues had tractable solutions, the difﬁcult issue of performanceoptimization for complex,IP-based distributed systems remains. End-to-end network performancedepends on many factors, from the digital signal processing techniques used in media compression to the architecture, management, and policy enforcement technologies for the overall network. These considerationsareComputer Networks                                                                  4 -51  particularly important for multimedia streams, which maydemand higher bandwidth or havelower tolerance for delays than other data. Fortunately,recent developments in network optimization may overcome existing IP QoS issues for certain cases.   Because of some fundamental architectural characteristics of IP networks, the techniques available for providing performanceguarantees are somewhat limited. These techniques tend to shape the aggregate trafﬁc stream with simplistic regard for the needs of individual streams, or to ensurespeciﬁc service characteristics for individual streams at the expense of impractical complexity. The shaping and performance guarantees for aggregated trafﬁc are often called ‘‘Class of Service’’ (CoS) approaches. CoS technologies deﬁne asmall set of generic behaviors that can approximate the actual service requirements of data streams if the streams are mapped into suitable, properly deﬁned classes. In contrast, approaches capable of ‘‘true QoS’’often require ahomogeneous underlying network structure,and exchange alarge amount of complexityand effective throughput for highly speciﬁc constraints on multiple statistical parameters of the data stream.  Network Architecture In general, communication networks can be decomposed into functional elements in ‘‘planes’’orcategories of operation. Foradiscussion of QoS, auseful decomposition is in terms of Access, Transport, Management,and Application subnetworksasshown in Figure 4.26. In the ﬁgure, Users interface with an Access network that is responsible for performing or enabling the authentication of subscribers or devices as well as authorizing those entities to use network resources. As partofthe authorization process, the Access networkperforms admission control and trafﬁc grooming for the collection of subscriber trafﬁc, subject to some constraints imposed by the Management network. The Access networkinterfaces with the Core or Transport networktoexchange trafﬁc that is within the terms established by a Service Level Agreement (SLA), or desiredboundaries for aggregated data streams. The Transport network ensures that conformant data inserted at an ingress point is delivered to its appropriate egress point intact and without disruption or violation of the SLA. As shown in the ﬁgure,the User maybeinteracting with an Application    FIGURE. 4.26 Network architecturedecomposed into Access, Transport, Management, and Application subnetworks.4 -52                                  Broadcasting and Optical Communication Technology  resident in aspecial type of access networkadjunct to the transport network, or with another User in a different Access network.   Regardless of the speciﬁcs of the interaction, and assuming that Application resources are adequately provisioned, the characteristics and capabilities of the Access and Transport networks are largely responsible for the QualityofService (or lack thereof)seen by the User;these capabilities are intimately tied to the Management architecture of the aggregate network.  Conﬁguration  and States. The network of Figure 4.26 cycles throughfunctional states generally aligned with Initialization (or Conﬁguration)and Operation modes or states.1 In each mode, the elements in different ‘‘functional planes’’ofthe network havekey responsibilities, which involve the QoS technologies discussed in later sections.   In the Initialization or Conﬁguration state, the primaryactivityisone of conﬁguration or reconﬁguration. This activitymay be driven by administrativeinputs or data related to an autonomous process, and maybe initiated viascheduled activities or asynchronous events. Forexample, the negotiation of SLA parameters between subnetworks or administrativedomains may depend on economic factors or business relationships, and typically lead to bounding conditions on network conﬁguration. These items amount to asortof‘‘initial conditions’’for subsequent network operations or conﬁguration inputs. Additionally,alignment between services speciﬁed by the SLA and capabilities offered by network elements must logically followthe creation of the SLA in amapping of ‘‘theory’’to‘‘reality.’’The partitioning of resources for end-to-end service ﬁdelityisa signiﬁcant consideration,particularly given the fundamental impact of QoS technologies on the stabilityof network elements. Of course, in adynamically responsivenetwork, the activities involved in the Initialization state can lead to reconﬁguration of subnetworkorelement functions to address time-dependent conditions or priorities. The SLA and element conﬁguration are indicated in Figure4.26 wheretopologyor resourceinformation is used to driveconﬁguration activity. Note that the reconﬁguration or distribution of ‘‘rules’’can be initiated by user interactions, network state or conditions, or centralized, policy-based administrativeactions.   In the Operation state, the primarynetworkactivities are very different for the Access and Transport subnetworks, and are carried out accordingtothe conﬁguration or resource partitioning established during Initialization.For instance,the operational Access network is responsible for complex ingress functions, which include shaping,policing,and logical differentiation of user data streams. In this respect, shaping data streams involves the enforcement of stabilityfor pre-established trafﬁc classes, policing involves the enforcement of policy or access rules, and differentiation involves the parsing and mapping of user or ﬂow data to the available transport facilities. In similar fashion, the operational Transport network is responsible for ensuring the ﬁdelityofend-to-end stream differentiation according to the Access indication and the boundaries established by the SLA(s). In modern packet-switched networks, this ﬁdelityisgenerally accomplished by examining alabel or tag attached to each packet during ingress differentiation, and placing the packet in asuitable, pre-conﬁgured transport queue. In addition to these functions, overall management activities take placewithin and between subnetworks to effect coarse adjustment between conﬁgurable trafﬁc classes and to provide feedback to ingress admission control that is based on the current networkstate. This feedback component is indicated in Figure4.26 as topologyor resourceinformation ﬂowing from Transport entities towards Management entities.   As an example of the preceding discussion, the interactions of various subnetworks can be viewed in the context of asingle, simpliﬁed trafﬁc ﬂow, as shown in Figure4.27. In the ﬁgure,aQoS-bearing request from the user invokes access-control procedures in the Access network and interaction with the conﬁguration management and policy entities in the Management domain. As partofthe authentication, authorization, and access-control process, the Management entities must verify resourceavailabilityagainst dynamic and static    1 Of course, sincethe network maybeavery large distributed system, the notion of ahomogeneous ‘‘state’’for all parts of the system is somewhat unrealistic. However,these ‘‘states’’ or ‘‘modes’’ maybeidentiﬁable for isolated subnetworks or collections of elements, so the classiﬁcation is reasonable for the purposes of discussion.Computer Networks                                                                  4 -53                  FIGURE 4.27 Subnetwork interaction based on QoS-sensitive trafﬁc request.  network parameters and SLA boundaries. Based on the result of these admission processes, resources in the Access and Transport network may be adjusted to accommodate the bearer trafﬁc ﬂow, or the ﬂowmay be mapped onto an existing transport facilitywithpre-established service characteristics.  Industry Speciﬁcations. Based in partonthe functionally segregated network architecture of Figure 4.26, several Internet Engineering Task Forcegroups (IETF) [1] havebeen working on standardized approaches for IP-based QoS technologies. The IETF approaches of interest herefall primarily into the following categories:      . prioritization using differentiated services (RFC 2475),     . reservation using integrated services(RFC 2210), and     . multiprotocol label switching (RFC 3031).   The QoS mechanisms recommended by the IETF are important, because in addition to their implicit functional decomposition (compatible with Figure4.26), they are designed to enable some forms of QoS without disrupting the basic transportarchitecture of the Internet. This architecture relies heavily on conventional packet forwarding devices (routers) that must efﬁciently and accurately process large numbers of statistically multiplexed packets. Unfortunately,IPnetwork design is based on best-effort packet forwarding, an approach that does not distinguish explicitly between the needs of particular streams. In fact, some of the efﬁciency and scalabilityofIPnetworking is based on an architecture where intermediate forwarding devices do not need to consider explicit per-packet or per-stream requirements. So, althoughthe IETF QoS mechanisms really only provide performanceguarantees for aggregated packet streams (i.e., they are CoS approaches), they may be the only viable approach to service differentiation in an IP network.  Structure of Discussion QoS technologies that are important in the Core or Transport sections of packet-switched networks are discussed in Section ‘‘TransportMechanisms.’’ This discussion centers on the deﬁnition of ‘‘Qualityof Service’’ and technologies enabling packet-switched networks to adapt to the requirements of multiservice dataﬂows. In concertwith the burgeoning popularityofbroadband Internet access, several important access networktechnologies are discussed in Section ‘‘Access Mechanisms.’’ The composite effects of4 -54                                  Broadcasting and Optical Communication Technology  transport,access control, and other architectural factors are particularly important for end-to-end QoS guarantees. Finally,the ‘‘Conclusion’’ section concludes that the deployment of end-to-end QoS mechanisms for application data- ﬂows is adifﬁcult but crucial requirement for next-generation IP networks.  Transport  Mechanisms The current structureofthe Internet relies heavily on conventional routers to store, examine, and transmit packets. These routers, with their routing protocols and forwarding mechanisms, are fundamental components of the Transport section of Figure 4.26. The reliable, ﬂexible structure of the commodity Internet is largely based on the routing protocols, which determine packet forwarding,and on the deterministic behavior of those forwardingmechanisms. Forreference, Figure 4.28 contains afunctional diagram of ageneric router showing basic componentssuch as the route database and route selection, packet forwardingprocess, and topologyupdates.  Routing The actions of routers are logically similar to the actions of ashipping clerk or production-lineworker.Each item from astream of items (the assembly line) is inspected, wrapped, boxed, and loaded for transport. Broken items must be tossed aside, and maximum efﬁciency is achievedwhen all items are similar.The introduction of ‘‘special’’or‘‘nonstandard’’items causes conﬂicts in resourceallocation and efﬁciency. The introduction of QoS-sensitivedata streams into IP-based networks is equivalent to interlacing items needing special handling into ahighly optimized assembly line. Depending on the nature of the data (size, shape, characteristics of the item), the ramiﬁcations of differentiated transportcan affect all subsequent operations [2].   Forexample, current-generation IP routers use routingprotocols to distribute topologyand reachability information. This information is used to compute optimal forwardingornext-hop paths. Even in best-effort (conventional) networks, the coreforwardingfunction can be asigniﬁcant bottleneck because of complex,       FIGURE 4.28 Generic routerarchitecture, showing routing decisions, packet scheduler,and output queuing.Computer Networks                                                                  4 -55  per-packet calculations.1 The complexityofthese computations does not scale gracefully,evenfor the one- dimensional service characteristics of current IP trafﬁc. Unfortunately,queuing and forwarding mechanisms in QoS-capable networks may be signiﬁcantly morecomplex. In addition, the introduction of higher-order, application-speciﬁc information may requiresigniﬁcant changes to the ‘‘reconnaissance’’ that supplies data for these computations.   To illustrate these factors, the router in Figure4.28 is augmented with ‘‘QoS-aware’’ functions such as a queuing policy database, sophisticated packet scheduler,and several customizable, individually prioritized output queues. Additionally,the route updates (reconnaissance) are assumed to contain higher-order information to feed the QoS-aware route computations. It is helpful to view the architecture of IP packet forwardingelements in this rather simplistic fashion to highlight concepts of shortcut routing and switching, integrated and layered routing protocols, and per-stream differentiation viaclass-of-service mechanisms.  Shortcut Routing and   Switching.  To keep pace with rapidly increasing data rates, forwarding algorithms are often implemented in hardware-based, connection-oriented ‘‘shortcut routing,’’ which combines considerationsfrom the transport,network, and link layers of the canonical Open Systems Interconnection (OSI) model. This integration of routingand switching in IP networks can address some of the special requirements of QoS-sensitivestreams. However,althoughefﬁcient implementation mayreduce the computations and memoryaccesses required for simple forwardingoperations, additional poorly deﬁned burdens such as policy-based ﬁltering,QoS assurance metrics, and sophisticated scheduling algorithms will impose additional complexitythat may not be amenable to implementation in silicon [4,5].   Switching architectures tend to be implemented in special-purpose hardware, such as ‘‘network processors,’’ which can efﬁciently process structureddata streams. The ‘‘layer 3switch’’ is an evolutionarystep towards atwo-stage process where forward tactical information gatheredfrom the data stream or network topologyis used to rapidly and temporarily reconﬁguredevices for subsequent packet ﬂows. In the context of data networking, ﬂow detection, shortcut routing,and IP switching are terms that are commonly used to describe this process. Typically,the efﬁcient, synchronous data handling capabilityofaswitch architecture is slaved to the slower, higher-layer processes that distribute networktopology[2].   This process is partofthe Initialization or Reconﬁguration state in the network of Figure4.26, and is shown in the ﬁgure as the distribution of ‘‘rules’’tonetwork elements. However,akeyassumption for this technology is the deployment of ahomogeneous transport infrastructure,such as in alocal-areaorsingle-domain network. In contrast, layered and integrated approaches to route determination tend to be more ﬂexible and amenable to deployment for larger-scale, multi-domain networks, at the expense of ‘‘precision’’ in the match between QoS requirements and service availability.  Layered Approaches—OSPF.     Alayered approach to integrating IP with various transporttechnologies maintains alevel of independencebetween the routingprotocols of the separate OSI layers. The independence of these protocols in layered solutions leads to ascattering of layer-2 and layer-3 connectivityinformation between separately maintained databases.   The predominant IP routingprotocol is Open ShortestPath First (OSPF,RFC 2328), which was developed for conventional routed IP networks. Large routed networks are typically brokeninto areas that are interconnected by areaborder routers and backbone networks. Each router has acomplete view of the topologyfor its area, but an incomplete view of the topologyfor neighboring areas. The primaryfunction of OSPF is to exchange network topologyinformation between network areas, which reduces the amount of routing trafﬁc. When link states change, routing information is ﬂooded between nodes, and topologycalculations are performed in parallel. In this fashion, large networks of routers are able to converge on acommontopologyquickly,and each router is able to use the link state and bandwidth information to calculate the ‘‘best’’path across the network for subsequent packets. In this case, best generally means fewest router hops. This mechanism is shown in Figure4.26 and Figure 4.28 as the feedback of state and resource information (‘‘topologyupdates’’) from routers to each other or to a    1 For example, OSPF (discussed later) uses Dijkstra’s algorithm on distributedtopology and reachability informationtoproduce weighted per-hop forwardingrules [3].4 -56                                  Broadcasting and Optical Communication Technology                        Ingress Router or                     Access Concentrator                                                  Label-switched paths or                                                         ,,                                                  tunnels,, (service classes)                                                     to egress points        FIGURE 4.29 Label-switched ‘‘tunnels’’inanIPtransportnetworkare independent of network topology. conﬁguration entity, as well as the computation and use of routing information. However,eventhoughOSPF topologycalculations are efﬁcient1 each router must still examine each packet and make aforwarding decision that may not include the multi-dimensional QoS requirements, policy rules, or accurate forwarding conﬁguration for all routers in the data path. This requirement for ‘‘hightouch’’ packet handling is cumbersome and not optimal for high-rate or high-complexityforwarding.Asaresult, other technologies are likely to be prevalent in QoS-enabled networks. Integrated Approaches—MPLS.     As the leading candidate for next-generation IP routing technology, Multi-Protocol Label Switching (MPLS, RFCs 3031 and 3270) addresses the requirements for QoS and availabilityinbackbone networks by simplifying the packet forwarding function. This simpliﬁcation is accomplished, in the context of Figure4.26, by pushing complex analysis and processing of per-packet routing information to the ingress point ( Access network), instead of repeating the process at intermediate or interior nodes ( Transport network). This function can be viewed as acompromise between the signaling function of true connection-oriented networks and the best-effort packet forwarding of IP networks [2,4,6]. In effect, MPLS  establishes some number of presignaled paths with particular service characteristics in the Transport network, and then maps incomingﬂowsinto those paths based on ametric applied in the Access network.   The general concept behind label switching refers to atypeoftrafﬁc engineering which allows arouter to efﬁciently determine apacket’snext hop without looking at the header of the packet or referring to routing lookup tables. Instead, an MPLS-capable networkarchitecture, such as shown in Figure4.26, attaches ﬁxed-length labels to packets at their ingress point in the Access network so that the Transport network can use the labels to make forwardingdecisions. The MPLS concept relies on the deﬁnition of preestablished or ‘‘overlay’’label-switched paths (LSP) or tunnels through the network that, when combined with appropriate resource partitioning,can be used as ‘‘service classes’’ for aClass of Service (CoS) implementation. Asimpliﬁed view of this conﬁguration is shown in Figure4.29. In the ﬁgure,the structure of the overlay LSP’sare independent of the actual network topologyorphysical connections. However,the level of service differentiation depends directly on the characteristics offorwarding nodes along the LSP.During operation, ingress routers in the Access network classify packets into particular service classes (tunnels) based on the best match between data requirements and path availability. The data requirements used in the classiﬁcation process maybeinferred from aﬂow’s source or destination, user proﬁle, or other data characteristics, or it may be dictated by administrativemeans. The mapping function between the data requirements and the available service classes (tunnels) is afunction of network implementation. This mapping is also an important aspect of the SLA because subsequent packet forwardinginthe Transport network automatically moves packets into paths indicated by their ingress label.   The limitation of available service classes (tunnels) can be viewed as aﬁnite number of lanes on ahighway, or as acomputing process with independent threads. While bottlenecks may occur in aparticular trafﬁc lane or individual thread, other options are available and are relatively independent. Aresult of this compromise between best-effortforwardingand application-speciﬁc QoS is asortofquantization of the possible QoS    1 OSPF uses less than 1percent of link bandwidth and less than 2percent of CPU capacityfor routing calculations [3].Computer Networks                                                                  4 -57  mechanisms. As aresult, some distortion must be introduced in the servicing of individual data streams [2]. The resulting distortion is an abstract combination of the Access network’sclassiﬁcation and the Transport network’sservice classes and queuing/forwarding functions.   Trafﬁc differentiation methods in the Access section, combined with MPLS in the Transport section and some form of dynamic resourcepartitioning,may be anatural and effectivehybrid approach to QoS in IP networks. However,the issues of end-to-end coordination between the speciﬁc meaning of trafﬁc classiﬁcations, the accurate mapping of these classiﬁcations to labels, and the deployment of consistent queuing and forwarding behaviors at intermediate nodes is an extremely complex undertaking.   These issues are far morecomplex if fault management is required for the LSPsasacondition of the SLA. In this case, the failure of an LSP may requirethe use of adifferent tunnel with equivalent service characteristics. This problemmay requiredual provisioning of paths, with accompanying difﬁculties in trafﬁc engineering. Maintaining network stabilityduring the dynamic allocation of resources for these tunnels is an open research areadue to the tight coupling between routing architecture and forwarding behavior [7].  Per-Stream Differentiation Akey factor to enabling effective service differentiation (CoS or QoS) in data communication lies in the Transport network’sabilitytohandle multiple classes of trafﬁc. On one extreme (exact QoS), the network must dynamically allocate resources based on per-stream requirements. On another extreme (CoS), the network is preconﬁgured for asubset of services, and the streamspassing through the network must be mapped onto the ‘‘closest match’’ for their requirements. The current Internet is an example of the latter architecture, whereasingle service class is available (best effortforwarding) that has arbitrarybandwidth but nondeterministic latency and jitter characteristics. This service class is very good for applications such as ﬁle transfers, which are insensitive to latency.The existing public telephone network is another example of the latter architecture, wherethe single available service class (a dedicated circuit) has excellent latency and jitter characteristics, but limited bandwidth. This service class is optimized for voicetransport, but can perform well for small volumes of data.   In IP networks, twogeneral QoS frameworks havebeen explored: Differentiated Services or ‘‘DiffServ’’ (RFC 2475) and Integrated Services or ‘‘IntServ’’ (RFC 2210). Both of these approachesrely on some form of advance conﬁguration or resourcepartitioning at intermediate forwardingnodes, including deterministic buffering, queuing,and packet scheduling algorithms. However, DiffServ is speciﬁcally designed to achievelow complexityand stabilityinexisting IP network deployments, whereas an IntServ architecture would result in higher implementation complexityand cost of deployment.   The Intservmodel refers to an approach that emulates circuit switching to reserve resources accordingto requests made by individual data streams. In the IntServarchitecture, aprotocol such as the Resource Reservation Protocol (RSVP,RFC 2205) is used to specify requirements for QoS parameters such as bandwidth, packet loss, delay, and jitter,inhopes that the intervening network will be able to satisfy this request on aper-ﬂow basis. Note that the IntServmodel infers arequirement for core transportnodes to maintain some amount of per-ﬂow state information, which is not particularly compatible with typical IP architecture.IntServalso depends on the forwarding behaviors of the corenodes, and has been extended to interfacewith some of these mechanisms (for instance, RFC 2750 and RFC 2872).   In contrast, the DiffServmodel aggregates similar ﬂows into equivalence classes,which eliminates the need for transport nodes to maintain per-ﬂow state or signaling information. With DiffServ,examination of per-ﬂow characteristics is done once at ingress, with simple per-hop behaviors and aggregate policing performed by task-optimized corenodes. Clearly,these mechanisms are compatible with the decomposed network of Figure4.26. Unfortunately,asisthe case with IntServ, the transport effectiveness of DiffServ- marked packets depends heavily on the per-hop forwarding behaviors implemented in the intervening nodes.  Diffserv. The DiffServmodel refers to an approach for implementing service differentiation that has been deﬁned to be broadly compatible with the existing Internet architecture. In the DiffServarchitecture, service differentiation is not provided on aper-ﬂow or per-request basis. Instead, packets are classiﬁed into predeﬁned per-hop behaviors (PHBs) that approximate the QoS requirements of the data stream. The DiffServ4 -58                                  Broadcasting and Optical Communication Technology   FIGURE 4.30 Examples of DiffServcodepoints (DSCP). The DSCP bits occupy the six most signiﬁcant bits of the type- of-service byte in the IP header.The two least signiﬁcant ToSbits are used for congestion notiﬁcation, and are unrelated to QoS.  classiﬁcation mechanism uses atag called the DiffServcode point (DSCP) that is attached to the six most- signiﬁcant bits of the type-of-service byte (ToS, RFC 791) in the IP header.DSCP values are deﬁned by RFC 2474 and illustrated in Figure 4.30 in relation to the structureofanIPpacket. The idea behind DSCP classiﬁcation is to enable transport routers to easily and quickly classify packets into different types of output queues or PHB’sbased on the DSCP.Square tags go in square queues. Round tags go in round queues. Packets in the square queue get treated differently than packets in the round queue [8].   To enable distributed conﬁguration, independently administrated networks implement an all-knowing, overlord node, sometimes called a bandwidth broker.This node acts as aproxy on behalf of auser or application to request trafﬁc ﬂowthroughtransit networks. This overarching function is shown in Figure 4.26 as the ‘‘Policy Database’’and ‘‘QoS Server’’ofthe Management network. An issue with this architecture is that separately administered networks maybeimplemented with differing technologies, policies, and network management strategies, making end-to-end coordination difﬁcult. Additionally,sinceDiffServ leverages the architecture whereper-stream complexityispushed to the edge of the network, trafﬁc policing functions may be performed only in the Access network. This conﬁguration is tremendously sensitivetomisconﬁguration, since implied internal trust domains mean that internal forwarders do not perform shaping,etc. As aresult, SLA veriﬁcation and enforcement are critical to prevent the effects of ‘‘relative ethics’’ between the deﬁnitions of particular trafﬁc classes in different network domains.   Althoughthe DiffServmodel deﬁnes acollection of PHB’sand trafﬁc classiﬁcations, it doesn’t specify explicit mechanisms for achieving the service classes indicated by the PHB’s. The DiffServ expedited forwarding class (EF) is intended to provide the highest levelofaggregate QoS for streams requiring minimal delayand jitter.Other classes, such as assured forwarding (AF) and best effort (BE) classes detail coarse loss bounds and relative prioritization between streams. Expedited Forwarding The highest priorityDiffServ classiﬁcation is called expedited forwarding (EF,deﬁned in RFC 3246). The idea behind EF is to simulate a‘‘virtual leased line’’byensuring minimal queuing of packets within each router along the transport path. In this fashion, the EF class hopes to provide guarantees on delayand jitter,which are important for isochronousdata streams (i.e., video and audio). Unfortunately,due to an inability to distinguish between individual trafﬁc streams, only the aggregate EF ﬂow receives the desiredtreatment. This produces jitter in the delivery of individual EF streams. The only waytominimize these effects is to practice ‘‘gross overprovisioning,’’ whereonly asmall percentage of the available bandwidth is made available to the EF class, and only afew EF streams are allowed[8]. Assured Forwarding The most complex Diffservclassiﬁcation is called assured forwarding (AF,deﬁned in RFC 2597). The AF designation creates four independent classes of AF,each with threesubtypes. This classiﬁcation requires atotal of 12 DSCP values, as shown in Table 4.1.Computer Networks                                                                  4 -59       TABLE 4.1 Assured Forwarding (AF) DSCP values in binaryand decimal. The DSCP values occupy the six      most signiﬁcant bits of the type-of-service byte in the IP header                 Drop Precedence Subtype        CoS       (likelihood)    ( x )AF1x            AF2x       AF3x        AF4x       ‘‘Gold’’Low (unlikely)    x ¼ 1001010 (10)    010010 (18) 011010 (26) 100010 (34)      ‘‘Silver’’Medium (moderate) x ¼ 2001100 (12)  010100 (20) 011100 (28) 100100 (36)      ‘‘Bronze’’ High (likely)  x ¼ 3001110 (14)    010110 (22) 011110 (30) 100110 (38)    The difference between AF classes is related to different levels of forwarding assurance.The subtypes in each AF class indicate a‘‘drop precedence,’’ or relative importancewithin the class, as shown in Table 4.1. Each forwardingnode (router) allocates resources such as buffer space, bandwidth, etc., for each AF class. Based on these resources, trafﬁc streamshavesome levelofassurance that packets from each class will be forwardedas desired. Transmissions can exceed these resources at their own peril, described by the ‘‘dropprecedence.’’ Acongested DiffServnode is morelikely to drop AF packets that haveahigher drop precedence. So,within the AF designation, forwarding depends on the relationships between the instantaneous trafﬁc load at arouter,the available resources compared to the desired resources, and the dropprecedenceofeach packet [8].  Best-Effort Forwarding The lowest Diffservclassiﬁcation is the well-known best effort (BE) behavior of the current Internet. So,coarse differentiation between service levels is made by classifying packets as BE (poor), AF (better,withconditions), or EF (best).  Queuing and Forwarding Mechanisms.    Of course, the Diffservclassiﬁcations or DSCP tags are merely suggestions by the Access network as to howthe packets should be handled in transport. These suggestions must be acted upon by individual forwarding nodes in the Transport network. Some popular queuing and forwardingmechanisms (PHB’s)are listed in Table 4.2 and described below.Under the proper circumstances, these mechanisms can appropriately handle avarietyofservice classes, including isochronous(or time- sensitive) trafﬁc.  Weighted Fair Queueing (WFQ) WFQ is acongestion management algorithm that enables routers to provide consistent, fair response time for packet forwarding when trafﬁc load is high. Ageneral diagram of aWFQ mechanism is shown in Figure4.31, whereincoming packets are classiﬁed into output queues based on some innate characteristic of the trafﬁc. Common classiﬁcation schemes for IP-based networks include transport address or protocol, application protocol, or user information.   After classiﬁcation, packets are scheduled for output (forwarding) based on priorities or weights derived from some external inputs. The WFQ weights can be derived from administrativesources, feedback from network conditions, trafﬁc handling policies, or other inputs. The weight that WFQ assigns to aﬂow         TABLE 4.2. Some queuing mechanisms for IP service differentiation            Queuing Mechanism                          Comment         First In First Out (FIFO) or Packets are forwarded in the order of their arrival, and no service          Best-Effort(BE)             differentiation is performed or guaranteed.        Weighted Fair Queuing (WFQ) Interactivetrafﬁc is inserted into the front of the queue to reduce                                      response time; then, remaining bandwidth is shared                                      between additional ﬂows.        Class-Based Weighted Fair   Packets are parsed into multiple categories (‘‘classes’’)ofqueues          Queuing (CBWFQ)             for ﬁner-grained differentiation. WFQ scheduling is applied                                      separately for each category.4 -60                                  Broadcasting and Optical Communication Technology   FIGURE 4.31 Weighted Fair Queuing (WFQ) packet ﬂowdiagram showing classiﬁcation, queuing,and priority-based output scheduling.  determines the transmit order for packets queued in that ﬂow.With proper conﬁguration, WFQ can be used to schedule interactiveorisochronoustrafﬁc to the front of the output queue to reduceresponse time. Remaining trafﬁc then fairly sharesthe remaining bandwidth accordingtothe assigned weights. Class Based Weighted Fair Queueing (CBWFQ) Class Based Weighted Fair Queueing (CBWFQ) is aspeciﬁc type of WFQ that uses aspects of the incoming trafﬁc stream to classify or forward packets. CBWFQ extends the basic WFQ to include supportfor user- deﬁned trafﬁc classes with particular characteristics. In this case, trafﬁc classes can be designed withtarget parameters or behaviors such as minimum bandwidth or maximum queue depth. These characteristics provide boundaries for packet forwardingduring network congestion.   The behavior of CBWFQ when the forwarding node encounters these boundaryconditions can include class-speciﬁc policy-based actions for queue management, such as ‘‘taildrop’’or‘‘time in queue’’ metrics. Generally,trafﬁc not matching anyofthe conﬁgured classes is given best-efforttreatment. Once classiﬁed, the trafﬁc classes are prioritized and scheduled for output based on apolicy map that maybeattached to particular physical interfaces.   Various queuing disciplines can be implemented within atrafﬁc class. Forexample, strict priority queuing in atrafﬁc class would allowdelay-sensitive data such as voicetobescheduled for transmission before non-voice packets in other queues. This conﬁguration effectively gives isochronous trafﬁc preferential treatmentover other trafﬁc. Other Approaches As mentioned previously,afundamental issue in conventional IP networks is the workload inferredon coreforwarders (routers). This workload might include complex per-packet calculations or use of unavailable or voluminous per-packetdata. Alternative approachestothe fundamental tradeoffbetween ‘‘per ﬂowstate’’ (QoS) and ‘‘scalability’’(CoS) may be related to preprocessing packet ﬂows, adapting network-based congestion control mechanisms, or augmenting packet headers with dynamic per-ﬂow,per- packet state.  Dynamic  Packet Scheduling (DPS). One of the signiﬁcant problems with IntServ-typemechanisms such as RSVP is that per-ﬂow state can grow exponentially,which is not ascalable requirementfor core forwarding devices. Dynamic Packet State (DPS) is an alternativeapproach to per-ﬂow state that augments the packet header to include ﬂow-speciﬁc information that is updated by forwarding nodes [9]. The DPS architecture uses ingress classiﬁcation and efﬁcient coreforwarding as in the DiffServ model, but generalizes the concept ofComputer Networks                                                                  4 -61  a‘‘tag’’ to be an executable object or indicator with variable per-hop content. With DPS, ingress routers in the Access network precompute necessaryparameters and classify packets, while Transport nodes dynamically combine per-packetinformation (‘‘ﬂowstate’’)with instantaneous network conditions (‘‘network state’’)and update the packet headers for processing at subsequent nodes. This architecture is acompromise between the relatively static DiffServarchitecture and the explicit per-ﬂow state requirements of RSVP,where corerouters perform simpliﬁed scheduling based on packet classiﬁcation and network conditions. The structure is logically very similar to modern, efﬁcient medical provider networks where‘‘ingress’’ general practicephysicians preclassify patients before referring them to upstream specialists, along with partial diagnosis and treatment. However,DPS technologycould be very difﬁcult to implement with legacy router capabilities.  Congestion Control in Trafﬁc Classes. The joint optimization between QoS and congestion control mechanisms is another approach to IP QoS that could havesigniﬁcant merit, particularly since it is compatible with the decomposed network architectureofFigure 4.26 and DiffServ/MPLS [9]. In one sense, the legacy requirement for new IP technologies to exhibit ‘‘TCP-friendly’’behavior fundamentally limits the potential for QoS alternatives. However,ﬂow aggregation schemes such as DiffServ/MPLS require some form of congestion control for each aggregate. In the CoS paradigm, the isolation of trafﬁc classes may alter the strict requirement for TCP-friendliness across all trafﬁc classes, which could lead to better QoS mechanisms in certain trafﬁc classes. In particular,per-class congestion control mechanisms could bridge the gap between CoS and QoS by segregating isochronoustrafﬁc fromother packet streamsand applying fundamentally different, more appropriate congestion control.  Access Mechanisms In addition to the dependenceoncore technologies, the thorough evaluation of access technologies is crucial in developing and validating afunctional, broadly deployable QoS framework.Here, the terminology‘‘access technologies’’ is used to include the interaction between application-level requirements, the network protocols that are present at the boundarybetween application and network, the logical topologyofthe local network, and the unique physical layer capabilities that are provided by various network architectures. In discussing QoS capabilities of various access technologies, it is clear that QoS requirements are expressed at the edge of the network, or at the point of ingress, prior to (or instead of)propagation through/between subdomains. Additionally,there are generally verydistinct physical contexts through which ingress is achieved. These QoS requirements must be coordinated with overall network policies, as well as access control and resource authorization mechanisms. With this understanding, the DiffServmodel can be an important aspect of Access network architecture[10].  Policies and Access Control Aside from the transport-related issues of network and trafﬁc engineering, service provider handoff agreements, and architectures that satisfy latency,jitter,and forwardingconstraints, an importantQoS consideration is the interplaybetween subscriber proﬁle, dynamic service provisioning, and service-level validation in the access network. Althoughtransport networks can be overprovisioned to effectivelyhandle trafﬁc in certain ‘‘behavior aggregates,’’ bandwidth in access networks is often strictly limited, or shared in a nonhierarchical manner.For instance,the use of IEEE 802.11 wireless LAN (WLAN) ‘‘hotspots’’for broadband Internet access has extensive commercial appeal [11]. However,the integration of network usage policies, access control and authorization mechanisms, and OSI layer 1, 2, and 3QoS technologies of WLAN ‘‘hotspots’’withanupstream DiffServ/MPLS network is neither trivial nor well-addressed by standards.  Policy-Based Networking.  Technologies that enable, manage, and validate QoS based on network- prescribed policies will be critical in modern IP-based access networks. In particular,QoS provisioning and SLA management in the interworking of ‘‘edge’’oraccess capabilities (Layer 2) and ‘‘upstream’’ or transport capabilities (Layer 3) is not well deﬁned. Although Access networks maysupportQoS mechanisms, the disconnect between layer-2 and layer-3 QoS or access control results in alack of integrated QoS provisioning,4 -62                                  Broadcasting and Optical Communication Technology               FIGURE 4.32 General architecturefor policy-based networking viasubscriber proﬁle.   policy distribution/enforcement, and SLA veriﬁcation for end-to-end ﬂows. Forinstance, althoughthe IEEE has proposed standardstoaddress some aspects of QoS in the WLAN arena, these approaches are distinct from IP-based mechanisms, and may not be directly deployable without some interworking function.   An area of commonalitybetween most technologypropositions is the implicit reliance on acentral infrastructure to deﬁne, propagate, and manage QoS features. Of course, these centralized facilities depend on atightly controlled, distributed infrastructuretoenforce,evaluate, and reportsuch policies. This architecture is compatible with the network decomposition of Figure 4.26, and importantelements of QoS-aware,policy- based access control are indicated in Figure 4.32.   In Figure 4.32, the tasks of authentication, authorization, and accounting are shown as logically different functions hosted in separate parts of the network. Additionally,the Access subnetwork(in this case, aWLAN hotspot) is shown as distinct from the access control function, which is comprisedofanauthorization capability, network policies, and a‘‘deliveryﬁlter’’orgating function. As shown, the attachment of a subscriber to the network ﬁrst triggers the authentication process, which depends both on network policies (‘‘static’’) and network information (‘‘dynamic’’). After authentication, the authorization process also uses these network parameters to compile and distribute speciﬁc, per-subscriber policies (‘‘access control’’) to elements in the Access network. These elements control access to network resourcesand communication of trafﬁc or usage statistics to the accounting function. The technologies of interest in this architectureinclude Transport considerationssuch as Diffserv/IntServ, MPLS, SLA veriﬁcation and provisioning,and Access considerations such as Digital Subscriber Loop (DSL), wireless LAN technologies (IEEE 802.11), and their relationships with telephonynetworks. Additionally,multi-network access control architectures such as IEEE 802.1x mayplay an important role in enabling QoS in avarietyofsituations.   Of course, in commercial enterprises, the most important functional elements are contained in the ‘‘accounting’’area. Although these functions are gated and fed by authentication and access control functions, speciﬁc discussions of rating and billing mechanisms are out of scope here.  Access Control. Access control and policy distribution in amanaged network is adifﬁcult proposition. An existing technologywhich maybeveryuseful in this respect for IP networks is the IEEE 802.1x framework for port-based authentication [12]. In the 802.1x architecture, network access is limited to rudimentaryfunctions until the authentication process is completed. Based on the result of authentication, network access can be gated and services can be authorized. The 802.1x architecture is also interestingComputer Networks                                                                  4 -63  because it explicitly prescribes centralized control of user credentials and coordination with network-wide QoS or CoS policies, as described in Figure 4.32 and Figure 4.26.   In 802.1x, the participants in an authentication conversation are the Supplicant, Authenticator,and Authentication Server.Typically,the Authentication Server is partofaManagement subnetwork, and the Authenticator is partofthe Access network that the Supplicant is trying to use. Beforeusing network services, the Supplicant initiates an authentication request to the Authentication Server.This authentication conversation is brokered by the Authenticator and can be based on standardized protocols such as the Extensible Authentication Protocol (EAP,deﬁned in RFC 2284). For mobile subscribers, the authentication conversation is typically relayed between Authentication Server entities in differentnetworkdomains. Until the authentication process completes successfully,the Authenticator prevents the Supplicant from gaining network access while translating between downstream, point-to-point network transports and the upstream authentication infrastructure.   Typically,the authentication conversation is transported by awell-known Authentication, Authorization, and Accounting (AAA) protocol such as RADIUS (RFC 2865). The interworking of EAP with RADIUS is well deﬁned (RFC 3579), and several authentication protocols can be mediated viathe EAP/RADIUS combination, including public keysand digital certiﬁcates. The RADIUS authentication architecture is averyﬂexible request/response conversation, whereinformation is exchanged in ‘‘Attribute/Value’’pairs. In this architecture, the Authentication Server can consult alocal (internal) or remote (external) subscriber database, as well as proxyauthentication requests to RADIUS servers in other administrativedomains.   Because 802.1x does not directly depend on the AAA transport protocol, it is also compatible withenhanced authentication infrastructuressuch as DIAMETER (RFC 3588), which may become moreimportantasIPQoS is deployed. In comparison to RADIUS, the DIAMETER transport has improved scalability, session control, security, and other reﬁnements that can be leveraged in QoS-sensitiveenvironments. Table 4.3 summarizes some of the importantdifferencesbetween RADIUS and DIAMETER. Aspects of DIAMETER that maybe particularly useful in administration of QoS-based requests are related to session control (command extensions and resolution of accounting events) and security(ﬂexible encryption).   In addition to simple RADIUS authorization, and in combination withsophisticated authentication infrastructures, the 802.1x framework can be used in aQoS/CoS environment to dynamically provide privileges, policies, conﬁgurations, and accounting requirements to networkcomponents in both the Transport and Access subnetworks. Gated by the authentication process, 802.1x enables network elements to modify access privileges according to individual entitlement. AlthoughRADIUS-authenticated sessions can already provide limited service authorization, 802.1x creates arobust framework for service conﬁguration, resulting in the potential for ﬁne-grained, dynamic privilege authorization. This dynamic user/network conﬁguration control is particularly important in popular broadband access networks.  Broadband Access With the accelerating price erosion of broadband access, QoS is becoming an importantdifferentiator for network operators. With reference to Figure 4.26 and Figure 4.32, the Access network, such as aDSL interface or WLAN hotspot, maybeoperated by an independent ‘‘network access provider’’(NAP) that is operationally distinct from the ‘‘network service provider’’(NSP) operating the Transport network. Either of these networks maycontain Application subnetworks, and both likely contain a Management subnetwork.   Forapplication services in the Application network, the NSP and/or NAPmay also haveabusiness relationship with an external ‘‘application service provider’’(ASP). The boundaries between these separate enterprises are complicated by the technologies, functional responsibilities, and networkpolicies that affect the realization of end-to-end QoS. In the following section, we focus on some particular aspects of broadband access networks that are compatible with the prevailing QoS/CoS architecture of Figure 4.26 and Figure4.32, and the scenario of Figure4.27.  Wireless LAN  and Wireless Telephony. The Universal Mobile Telecommunications System (UMTS) has been extensively promoted as the future high-speed, high-bandwidth mobile telecommunications system to supportmultimedia services. However,due in parttothe economic and logistical issues of ‘‘upgrading’’ 4 -64                                               Broadcasting    and  Optical  Communication        Technology   TABLE  4.3   Diameter vs. Radius  CategoryItemDIAMETER                                    RADIUS                           Comment   Transport    IP Transport            TCP, SCTP        UDP              TCPand  SCTP  are connection-oriented                 Protocols                                                 transports, with ﬂow-control and congestion                                                                           avoidancemechanisms,  whereas RADIUS  uses                                                                           connectionless UDP.DIAMETER    has reliable                                                                           message delivery(with per-hop retransmission                                                                           and heartbeat), which enhances failover and                                                                           makes reachability status for peer servers                                                                           moreprecise. Scalability   AttributeLength         24-bit           8-bit            DIAMETERallows   larger Attribute values per                                                                           transmission (‘‘payload’’).               Identiﬁer               32-bit           8-bit            DIAMETERincreases   the number of concurrent                                                                           pending messages (‘‘window’’).               Alignment32-bit                          8-bit            DIAMETERHeader    entries and Attribute data                                                                           must align on 32-bit boundariesfor processing                                                                           efﬁciency. Indirection   Proxysupport            Distributed      Centralized      Localized (per-hop) failuredetection in                                                                           DIAMETER    allows proxy servers to initiate                                                                           failover and retransmission.RADIUS uses                                                                           end-to-end retransmission by peer servers. Session       Vendor-speciﬁc          Attributes and   Attributes only  DIAMETERhas    vendor-speciﬁc Commands    Control      extensions              Commands                          as well as vendor-speciﬁc Attributes to enable                                                                           customization and preserve interoperability.               Server capability       Peer-to-peer     Client-Server    The peer-to-peer relationship of DIAMETER                 increased                                                 servers enable session termination and user                                                                           reauthentication or reauthorization,                                                                           whereas the client/server RADIUS architecture                                                                           often requiresproprietaryprotocol extensions.               Accounting decoupled    Independent      Joint STOP       DIAMETERAuthentication/Authorization                 from Authentication     STOP                              messages can be routed and handled differently                 and Authorization                                         than Accounting messages to improve session                                                                           accounting ﬁdelity               Accounting resolution   Various          Session only     DIAMETERallows   User,Session, Subsession, and                                                                           Multisession IDs to correlate accounting.  Security     Encryption              IPsec, TLS       Sharedsecret     Individual DIAMETERAttribute/Value  pairs                                                                           can be digitally signed/encrypted to prevent                                                                           observation/tampering by intervening                                                                           proxy/relay servers.    existing networks  to UMTS     standards, wireless LAN   technologies   havebeen   used  as an  Access network   for some   UMTS    services [11,13].    The  use  of WLAN-based     technologyrequires    speciﬁc  QoS   levels that are acceptable  by the standards   of UMTS    networks.   Unfortunately,coordination    between   the OSI  layer-1/layer-2 WLAN    QoS   technologies  and IP-based   layer-3  technologies  is  not  addressed   speciﬁcally by  disparate   standards  bodies.  Additionally, provisioning   and billing of commercial   UMTS    data servicescan   be complex   when  alternative Access network technologies  such  as WLAN    allow  for intrinsically shared network  attachment.   To bridge  this gap,the  Third Generation   Partnership  Program   (3GPP)   has described  various scenarios for interworking   UMTS    and WLAN technologies,  which  include  the billable use of UMTS   core networkservicesvia    aWLAN     Access network[14].    In addition  to issues of separate  QoS  technologies  for various  network   layers, and in atwist  on  issues of separately  owned   and  managed   Access, Transport,and   Management     networks,  the use  of WLAN    ‘‘hotspots’’ introduces  another  permutation:   an Access network  sharedbycompeting      networkoperators    who  use  QoS  as a leverage  point for subscriber  afﬁnity. In this scenario,multiple  UMTS    operators  shareaWLAN-based       Access network,   which  maybeindependently       owned   and operated.  In such  an  interconnection,the   architecture  of Figure4.28   is essentially replicated in each participating subnetwork,and    the  policy management    frameworkComputer Networks                                                                  4 -65  between subnetworks becomes an explicit, critical partofthe overall SLA between subscribers, operators, and subnetworks.   In general, a‘‘master’’policy broker is established in each of the independent network areas. These master brokers then control QoS policies related to the ﬂow of subscriber data into their respective networks. In this case, the QoS enforcement mechanisms of the Access network, which may include network-speciﬁc CoS mapping and classiﬁcation functions, must be applied independently for data streams subject to the policy structures of different Transport networks. Clearly,this scenario presents tremendous logistical problems, particularly in cases whereconﬂicts, omissions, or inaccuracies are present in the QoS policies of the constituent networks. Althoughthe UMTS network operator would like to control the deliveryofservices to the user as if their terminal were directly connected to the UMTS network, the intervening non-UMTS Access network creates alevel of ‘‘indirection’’ in the datapath. In particular,dynamic service requirements may be dependent on variable resourcesorcapabilities in both networks, and the determination of enforceable network-level policies maybesubject to some negotiation. In these scenarios, the relationship between QoS and policy entities in different subnetworks may be hierarchical, peered, or ahybrid architecture.The structureand administration of this ‘‘QoS/policy architecture’’ is particularly important to facilitate dynamic trafﬁc streams[13].  Digital Subscriber Loop. Another example of access technologies that requireQoS/CoS mechanisms is ﬁxed broadband networks such as Digital Subscriber Loop (DSL). Various mechanisms exist that are compatible withthe architecturedescribed by Figure4.26 and Figure4.32. In fact, it is not uncommon for DSL subscribers to haveprivately owned WLAN access points interfacing to aprovider-offered DSL-based Access network. This setup is only aslight perturbation of Figure4.32, and closely related to the UMTS/WLAN scenarios discussed previously.For DSL-based Access networks, the entities controlling each subnetwork may be combined, or the Internet Service Provider (ISP) or NAPmay be distinct from the NSP,which is also distinct from the ASP or collection of ASPs. Regardless of the business arrangements and relationships, if the ASP offers applications that require service guarantees such as streaming video,VoIP,etc., the NSP must reserve Transport resourcesorprovide asuitable mapping to existing CoS tunnels, and the NAPmust reserve or enable Access resourcesthat map appropriately to the requested QoS. In such scenarios, the technological barriers tend to be moretractable than the ‘‘business to business’’ policy coordination issues between independent entities, and simpler than in the case of asingle WLAN Access network being sharedamongst multiple UMTS service providers.   In the DSL scenario,resource enforcement in the Access network (NAP) based on conﬁguration parameters indicated by the Transport network (NSP) is apreferable hierarchy. Also,due to bearer network securityand stabilityissues, a‘‘transparent’’ proxy-based request/conﬁguration architecture is necessary, as indicated in the ‘‘QoS server’’ofFigure 4.26. With this centralized architecture, the potential for an explicit relationship between asubscriber proﬁle and adynamic network conﬁguration, particularly in the context of RADIUS/ DIAMETER AAA protocols, is very reasonable—aslong as the Transport network is either overprovisioned or conﬁgured with sufﬁcient CoS resolution.   To realize such an architecture,anentityineither the Access or Transport networks must perform ‘‘route alignment’’ between the physical location of ingress/egress for asubscriber (‘‘networkservice access point’’ or NSAP) and the available overlay tunnels or QoS facilities. Forinstance,for an ethernet-based access network, frame-based CoS could be used to map onto preconﬁgured circuits at an access multiplexer.After the authentication process is complete, the subscriber and NSAP information must be forwardedtoacentralized brokertocross-referencenetwork policies with subscriber privileges, and then downloaded to reconﬁgure ingress and coredevices. In anycase, admission control feedback from the NSP’s Transport network mayresult in different queuing/forwarding strategies in border or ingress routers, as well as trafﬁc control (shaping, policing) in the upstream sections of the DSL-based Access network [15].  Conclusion True ‘‘QoS’’guarantees are anecessarycharacteristic of next generation packet-based networks that will havea profound impact on the deployment of advanced, network-sensitive applications. In particular,the interaction4 -66                                  Broadcasting and Optical Communication Technology  between application-speciﬁc trafﬁc requirements and the real-world constraints of existing technologies must be carefully considered. Unfortunately,the effective deliveryofQoS for data and multimedia streams presupposes sufﬁcient bandwidth, latency,jitter,and higher-order guarantees than most networks can generally accommodate. Adequate QoS for packet-based multimedia is difﬁcult for telephone networks, which are structuredfor voicetransport.Adequate QoS for isochronous streams is difﬁcult for IP networks, which are structured for best-effortdata. In IP-based networks, the result of these constraints is an overriding requirement for practical implementation. This situation has led to the deﬁnition of CoS mechanisms where Transport networks are provisioned with some collection of service aggregates, and ingress Access networks are tasked with mapping the requirements of incoming streams onto these available transports—subject to some network-imposed metrics.   Sinceend-to-end performanceoptimization on large-scale distributed systems is difﬁcult at best, the notion of strict guarantees on QoS/CoS will be dependent on the Service Level Agreements (SLAs) deﬁned between neighboring network domains. The negotiation and enforcement mechanisms that comprise these SLAs at the boundaries between network operators and subnetworkdomains must be coupled with, and measured against, the actual performance of real applications, in ordertoestablish their validity. In addition, each user’s expectation for QoS will depend on the conﬁguration and capabilities of the Access, Transport,and Application sections of each network.   An area of particular interest in the quest for end-to-end QoS/CoS guarantees is the Access subnetworkand its relation to the Transport and Management subnetworks. In an IP-based paradigm, the bulk of per-stream complexityisexplicitly transferredtothe Access network, whereitisdealt with once in afashion that is assumed to be acceptable and easily aggregated. In adynamic environment, the mapping between per-stream needs and special Access capabilities, such as those available in WLAN or DSL implementations, is more effectively addressed by architectures where Access network capabilities are slaved to the administration of CoS capabilities of upstream subnetworks. In essence, this implementation requires speciﬁc Access network capabilities to be integrated with the Transport network’sconﬁguration (available service classes) and per-hop- behaviors (available queuing mechanisms).   In this context, aviable next step in the evolution of network-based QoS guarantees maybethe enablement of ‘‘feedback’’between the various subnetworks of Figure 4.26. With the proper combination of network-based policies, programmable networkelements, and dynamic feedback between subnetworks and applications, even limited service classes maybeadjusted to provide the responsiveness necessaryfor QoS- sensitiveapplications.  References  1.  The Internet Engineering Task Force(IETF), The completetext of IETF Requests for Comments (RFCs),      http: //www.ietf.org/rfc/rfcNNNN.txt, ‘‘NNNN’’isthe RFC number.  2.  S. McClellan, K. Burst, and G. Grimes, ‘‘Issues and techniques in network-based distributed healthcare:      advanced networking techniques,’’ in Proc. Fourth World Conf. Integ. Des. Process Technol.,Kusadasi,      Turkey,1999.  3.  J.T. Moy, OSPF: Anatomy of an Internet Routing Protocol,Reading,MA: Addison-Wesley,1998.  4.  C. Metz, ‘‘Ingredients for better routing? Read the label,’’ IEEE Internet Comput.,vol. 2, no.5,pp. 10–15,      1998.  5.  P. Dumortier,‘‘Towardanew IP over ATMroutingparadigm,’’ IEEE Commun. Mag.,vol. 36, no.1,      pp.82–86, 1988.  6.  G. Hagard and M. Wolf, ‘‘Multiprotocol label switching in ATMnetworks,’’ Ericsson Rev.,no. 3, 1–12, 1998.  7.  J.L. Marzo,E.Calle, C. Scoglio,and T. Anjali, ‘‘QoS online routing and MPLS multilevelprotection: a      survey,’’ IEEE Commun. Mag.,vol. 41, no.10, pp.126–131, 2003.  8.  M. Stricklen, B. Cummings, and S. McClellan, ‘‘Linux and the next generation Internet,’’ Linux J.,73,      90–98, 2000.  9.  M. Welzl and M. Muhlhauser,‘‘Scalabilityand qualityofservice:atrade-off?,’’ IEEE Commun. Mag.,      vol.7,no. 6, pp.32–36, 2003.Computer Networks                                                                  4 -67  10.  J. Evans and C. Filsﬁls, ‘‘Deploying diffservatthe networkedge for tight SLAs, part1,’’ IEEE Internet      Comput.,vol.8,no. 1, pp.61–65, 2004. 11.  S. McClellan, S. Low,and W.-T.Tan, ‘‘Disruptivetechnologies and their affect on global telecommuni-      cations,’’ in Advances in Computers,M.Zelkowitz Ed., vol. 61, Academic Press–Elsevier:San Diego,CA,      2004. 12.  IEEE standardfor local and metropolitan area networks —port-based networkaccess control, IEEE Std      802.1X-2001, 2001. 13.  W. Zhuang,Y.-S. Gan, K.-J.Loh, and K.-C. Chua, ‘‘Policy-based QoS management architecture in an      integrated UMTS and WLAN environment,’’ IEEE Commun. Mag.,vol. 41, no.11, pp.118–125, 2003. 14.  3GPP TR 22.934, ver.6.2, Feasibility Study on 3GPP System to Wireless Local Area Network (WLAN)      Interworking (Release 6),September 2003. 15.  C. Bouchat, S. van den Bosch, and T. Pollet, ‘‘QoS in DSL access,’’ IEEE Commun. Mag.,vol.7,no. 9,      pp.108–114, 2003.This page intentionally left blank                                                                                 5                                                 Ad Hoc              Wireless                                                                 Networks                                 5.1  Introduction ........................................................................ 5 -1                               5.2  Applications and Opportunities.............................................. 5 -2                                     Searchand Rescue Applications * Defense Applications *                                     HealthcareApplications * Academic Environment Applications *                                    Industrial/Corporate Environment Applications Mohammad Ilyas                5.3  Challenges ........................................................................... 5 -4 Florida Atlantic University   5.4  Summaryand Conclusions .................................................... 5 -5  Ad hoc wireless networks are communication networks without awell-deﬁned infrastructure. These networks are established on demand to satisfy aspeciﬁc need for communication and exist for ashorter time period. The communication devicesinanadhoc network may be mobile and without ties to aﬁxedtopological infrastructure. As the devicesmove, the network topologychanges. The devicesinadhoc networks are the source and destination of information being exchanged, and they also act as intermediate devicestorelay information between two devices located outside the communication range of each other.These networks havedynamic topology, bandwidth-constrained variable capacitywireless links, energy-constrained operation, and limited physical security. These networks havetremendous potential for commercialand military applications. They are useful for providing supportwherenocommunication infrastructureexists, or its deployment is economically not feasible. Their potential applications include emergency situations, healthcare, home networking,and disaster recoveryoperations. In this chapter,wediscuss opportunities and challenges posed by ad hoc wireless communication networks.  5.1    Introduction  To meet the growing need for fast and reliable information exchange, communications networks have become an integral partofour society. Recent advancements in information technologyand miniaturization of mobile communication devices haveincreased the use and efﬁciency of the wireless communication environment manifold. Because of this signiﬁcant growth, wireless networks havewitnessed rapid changes and development of new applications. One such change is the development of mobile ad hoc wireless networks [4,8].   Wireless networks havebeen in existence for centuries, but the medium of communication and its usage mechanisms havechanged throughout history. Earlier wireless networks could only use audible and visual means of communication such as human voice, smoke signals, and/or reﬂective surfaces for conveying the information. With the advancement of technology, new and efﬁcient means (electromagnetic and optical wireless) of communication and efﬁcient mechanisms for their use are nowavailable.   Ad hoc wireless networks havecommunication devices without aﬁxedtopological infrastructure.They are self-organizing and adaptive [5]. As the devices movethe network topologychanges. The devices in these networks are not only sourceand destination of information exchanged, but they act as intermediate devices                                                                                      5 -15 -2                                   Broadcasting and Optical Communication Technology  to relay information between deviceslocated outside the communication range. These networks are characterized by dynamic topologies, bandwidth-constrained variable capacitywireless links, energy- constrained operation, and limited physical security[4,11].   Ad hoc networks havetremendous potential in commercial and militaryapplications [8,9,11,15]. These networks are particularly useful for providing communication supportwherenoﬁxedinfrastructure exists, or wherethe deployment of aﬁxedinfrastructureiseconomically not feasible. They can be used in the areaof militaryoperations, emergency situations, healthcare, academic settings, and home networking.Examples of disaster situations are:earthquakeand ﬂooding,when the rescue teams need to coordinate themselves without the availabilityofﬁxednetworks; militaryoperations, when communication is in ahostile environment; businesses, whereemployees share information in aconference; students using laptop computers to participate in an interactive lecture; and manyother similar situations. These networks meet atemporarynetworking need for aspeciﬁc duration of time; when the need disappears, so do the networks.   Ensuring effective communication among the devices is one of the major challenges in wireless ad hoc networks. Because of the limited range of each device’s wireless transmission, adevice needs to act as arelay station to communicate with devices outside its transmission range by forwarding packets to the destination. Therefore, arouting protocol that addresses adiverse range of issues such as low bandwidth, mobility, and low powerconsumption is necessaryinadhoc wireless networks. The mobile devices work together in adynamic but cooperative environment to maintain the communication channels.   It is possible for the mobile devices in an ad hoc wireless networktodrift and lose communication with other mobile devices. In this situation, an ad hoc networkmay be divided into twoormoreindependent ad hoc networks. Moreover, it is also possible when mobile devices in twoormoreadhoc networks are in close proximitytoeach other,they can fuse together into one larger ad hoc network. One can imagine the challenges of managing such adynamic communication environment [2,12,13].   Sensor networks are another form of ad hoc wireless networks. In sensor networks, the mobile devicescould be as small as agrain of rice and their numbers in the tens of thousands. These devices are self-sufﬁcient in transmitting,receiving, processing,and power. The sensors are programmable for anygiven application [8].   This chapter discusses opportunities and challenges faced in development of ad hoc wireless networks. The next section discusses application and opportunities that ad hoc wireless networks provide. Section ‘‘Challenges’’ discusses the challenges of using ad hoc wireless networks. Finally,Section ‘‘Summaryand Conclusions’’ provides asummaryand conclusions for this chapter.  5.2     Applications    and   Opportunities  Ad hoc wireless networks are used when anetworking environment is needed for alimited duration of time. These networks provide signiﬁcant opportunities and are used in numerous situations wherea communication infrastructure is nonexistent or difﬁcult to establish within timing constraints. Typically, these applications include:      . Searchand rescue applications in disaster situations     . Defense (army,navy, air force) applications     . Healthcare applications     . Academic environment applications     . Industrial/corporate environment applications   Manyother applications can utilize ad hoc wireless networks. However,inthis chapter,wewillonly focus on ﬁve listed above [6].  Search  and Rescue   Applications When we face an unfortunate situation such as an earthquake, hurricane, or similar disasters, ad hoc wireless networks can be very useful in search and rescue operations. In populated areas, disasters deplete powerand communication capabilities because they destroyinfrastructures. Ad hoc wireless networks can be establishedAd Hoc Wireless Networks                                                            5 -3  without infrastructuresand provide communication among various relief organizations for coordination of rescue operations. Wirelesssensor networks (another form of ad hoc network) can be used to conduct search for survivors and provide care in atimely manner.   Rescue operations also use robots to searchfor survivors. These robots communicate witheach other using wireless ad hoc networks to coordinate their activities. Based on the size of area affected by adisaster,robots can expedite the search by forming an ad hoc network, searching the area, and gathering information. The information gathered can be analyzed and processed, and appropriate relief/help can be readily directed whereneeded.  Defense Applications Securecommunications is one of the key aspects of anysuccessful defense operation. Manydefense operations take placeinlocations wherecommunication infrastructureisnot available.The use of wireless ad hoc and sensor networks in these situations are very useful and expedient. Different units (army,navy, and air force) involved in defense operations also need to maintain communication witheach other.Air forceplanes ﬂying information mayestablish an ad hoc wireless network for communicating,sharing images, and data among themselves. Armyand navy groups on the movecan also establish use ad hoc wireless networks. The advantage of this type of communication is that the ad hoc network moveswiththe individuals and equipment.   Information gathering is one of the manyadhoc wireless (particularly sensor) network applications. Intelligencegathering for defense purposes can makeuse of sensor networks very effectively. The sensors used for these applications are essentially disposable and are used only once.The sensors are deployed in large quantities to gather intelligenceoveraselected area by air or other appropriate means. Because of their tiny size, these sensors will remainsuspended in the air for some time. While suspended, they can gather the information they havebeen programmed to collect, process the information, share among nearby sensors, reach aconsensus, and transmit information to acentral location. This information can then be analyzed at a central processing facilityand adecision concerning the next step can be made. Sensor networks can also be used for tracking objects or targets, which is one of the critical applications in defense settings.   With rapid advancements in semiconductor technologies, the size of electronic devicesisdecreasing. At the same time, these devices are able to muster higher and higher processing powerontinychips. These advancements led to the development of wearable computers. The idea of wearable computers is not new,but the idea of asmartdress, consisting of manytinycomputers (or sensors), is relatively recent. Asmartdress is essentially an ad hoc network of tinycomputers that goes with you wherever you go because your wear it. Tiny computers connected by tinywires or by wireless means, can exchange information with each other,process information, and take an action that they are programmed to do if all of the prerequisite conditions are satisﬁed. Asmartdress may be programmed to monitor certain conditions and vital signs of an individual on aregular basis. This could become very useful for defense personnel in combat situation. The monitored information can be processed and appropriate action by yourown dress, if needed. Asmartdress may even be able to indicate the exact location of the problem, or call for help if needed.  Healthcare Applications Exchanging multimedia (audio,video,and data) information between the patient and healthcarefacilities is very helpful in critical and emergency situations. An individual in transit to ahospital by an ambulancemay exchange information using ad hoc communication networks. In manysituations, ahealthcare professional can better diagnose and prepare atreatment plan for an individual if he has video information instead of audio or data information alone. Forinstance, the video information maybehelpful in assessing the patient’s reﬂexes and coordination. In addition, determining the degree of injurybyvisual information rather than by audio or other descriptive information alone can expedite treatment when the patient reaches the hospital.   Real-time ultrasound scans of apatient’s kidneys, heart, or other organs may be veryhelpful in preparing a treatment plan for apatient who is in transit to ahospital, prior to his/her arrival. This information can be5 -4                                   Broadcasting and Optical Communication Technology  transmitted through wireless communication networks, from an ambulance to ahospital, or to healthcare professionals who are converging toward the hospital frommultiple locations to treat the patient.   An ad hoc wireless networkestablished within ahome (smart homes) can also be very useful for monitoring homebound patients. Such homes may be able to make some basic decisions (based on information exchanged between various sensors participating in an ad hoc network) that are beneﬁcial to an elderly population. Some of the actions that smarthomes can take include monitoring the movement patterns inside ahome, recognizing when aperson has fallen, recognizing an unusual situation, and informing arelevant agency so that appropriate help can be provided.   The concept of the smartdress discussed in the subsection on defense applications can also be used for monitors health conditions of patients. Such dresses may become very useful for providing healthcarefor our elderly population.  Academic   Environment    Applications Most academic institutions already have, or are in process of establishing, wireless communication networks. These networks provide students and facultywith aconvenient environment in which to interact and accomplish their studies or research.Inthis setting, ad hoc wireless networks can enhance this type of environment and add manyattractive features. For instance,anadhoc wireless communication network between an instructor and the students enrolled in his/her class can provide an easy and convenient mechanism for the instructor to distribute handouts to all the students in the class, and for students to submit their assignments. Sharing information among the class participants can be as easy as click of akey on the keyboard. Because of the mobilityofadhoc wireless networks, they can be established while on ﬁeld trips and industrial visits. Staying in touch cannot be anyeasier than this.  Industrial/Corporate  Environment    Applications Most industrial/corporate sites havewireless communication networks in place, particularly in manufacturing environments. Manufacturing facilities, in general, havenumerouselectronicdevices that are interconnected. Having wiredconnectivityleads to cluttering and crowding of space and poses not only safetyhazards, but also adversely affects reliability.Using wireless communication networks eliminates manysafetyand spatial concerns. Connectivityinthe form of ad hoc wireless communication networks adds manyattractive aspects, including mobility. These devices can be relocated, and the networks reconﬁgured based on their requirements. Additionally,communication among various entities can be maintained, and corporate meetings can take placewithout employees gathering in the same room.  5.3     Challenges  Althoughadhoc  wireless networking represents technological advancements, thereare manychallenges associated with fully using its beneﬁts. As with all mobile communication environments, ad hoc wireless communications operate with the following constraints:      . Limited communication bandwidth and capacity     . Limited batterypowerand life     . Size of the mobile devices     . Information security     . Communication overhead   Wireless communication operates with limited bandwidth, which implies only alimited amount of information can be transmitted over aperiod of time. Efﬁcient transmission techniques will pavethe way for increased capacity. However,thereisaneed for innovativeapproaches to optimal use of available bandwidth and capacity. The concept of cellular communication structuresand the use of transmission techniques such asAd Hoc Wireless Networks                                                            5 -5  CDMA are veryhelpful. Additional research is needed to provide moreefﬁcient mechanisms for using the available communication bandwidth in awireless communication environment [8].   Mobile communication devicesdonot haveaccess to unlimited power.They use batteries and havea limited supply of power. Higher powerusage shortens the batterylife. Efforts are being made to design a device that consumes less power and adjusts the strength of communication signals based on the distance between communicating points. In addition, efﬁcient signal processing techniques and algorithms requiring less powerusage are being developed. We havemade signiﬁcant progress in these areas, but more research is needed to makeiteven better [4,7,10,16].   With the advancements in semiconductor technologies, moreelectroniccomponents can be placed on smaller chips. That has led to the development of mobile devicesthat are more powerful and less power hungry. As the size of these mobile devicesdecreases, morefeatures can be added to these deviceswithout increased powerusage. The challenge is to maintain that trend.   Wireless communication environments are more prone to securityrisks than other communication networks, and ad hoc wireless networks are no exception. The levelofdesired information securitycan be achieved, but it adds processing overhead and requires additional bandwidth for transmission. Researchers are working to discover mechanisms that will provide secureinformation transfer and at the same time willnot add prohibitive overhead [9,10,18].   Reducing the communication overhead for transferring information in ad hoc wireless communication networks is one of the biggest and most formidable challenges. When information needs to be transmitted from one node to another,aroute or path needs to be established. In addition, some procedurefor sharing the common pool of resources such as bandwidth has to be established. Bluetooth technologyand IEEE 802.11 protocols provide mechanisms for sharing the resources [1,2,14,15]. In addition, there were several routing mechanisms proposed to establish aroute between twocommunicating devices. The challenge that ad hoc wireless networks pose is that they havedynamic topology. In order to establish aroute between two communicating devices, the network componentsneed to be aware of their location. To make things complicated, mobile devicesmay keep changing their locations. The procedures for establishing routes need to be dynamic and adaptive. The route established at the startofinformation transfer between twodevicesmay change by the time the information reaches its destination. Therefore, routinginformation needs to be as current as possible all the time [7,12,13,16,17].   There are several possibilities for establishing and maintaining routes. Routes can be established proactively or on-demand. The procedures that establish routes proactivelyincur moreoverhead because the establishment of all routes maynot be necessary. If routes are established proactivelyand frequently,they will be current and immediately available to anycommunication device that needs to send information to another device. If routes are established on-demand, the overhead incurred will be less because the routes will be established as needed. However,the on-demand routingmechanism will introducedelayfor devices because they will havetowait for an established route before initiating communication. Manyhybrid routing mechanisms havebeen proposed. The challenge remains establishing the best possible routes with the least possible overhead [8,9].  5.4     Summaryand Conclusions  This chapter has discussed opportunities and challenges related to ad hoc wireless communication networks. Development of ad hoc wireless networks and sensor networks are useful in manyareas including disaster recovery, defense, healthcare, academic, and industrial environments. However,thereare manyother challenges. These include the development of mechanisms that efﬁciently use limited bandwidth and communication capacity,mechanisms for reducing power consumption and extending the batterylife, developing smaller and morepowerful mobile devices, developing algorithms for enhancing information security, and developing efﬁcient routing procedures. These are major challenges to overcome, but steady progress is being made to address them.5 -6                                   Broadcasting and Optical Communication Technology  References  1.  ‘‘Bluetooth Wireless Technology,’’ Ofﬁcial Bluetooth Website: http://www.bluetooth.com.  2.  S. Chakrabartiand A. Mishra, ‘‘QoS issues in ad hoc wireless networks,’’ IEEE Commun. Mag.,vol. 39,      no.2,pp. 142–148, 2001.  3.  I. Chlamtac, M. Conti, and J. Liu, ‘‘Mobile ad hoc networking: imperatives and challenges,’’ Ad Hoc      Networks,vol. 1, no.1,pp. 13–64, 2003.  4.  A. Ephermides, ‘‘Energyconcerns in wireless networks,’’ IEEE Mag. Wireless Commun.,vol. 9, no.4,pp.      48–59, 2002.  5.  M. Frodigh, ‘‘Wireless ad hoc networking —the artofnetworking without anetwork,’’ Ericsson Rev.,      2001.  6.  B. Furht and M. Ilyas, Eds., Wireless Internet Handbook:Technologies, Standards,and Applications,Boca      Raton, FL: CRCPress, 2003.  7.  A.J.Goldsmith and S.B. Wicker,‘‘Design challenges for energy-constrained ad hoc wireless networks,’’      IEEE Wireless Commun.,vol. 9, no.4,pp. 8–27, 2002.  8.  M. Ilyas, Ed., The Handbook of Ad HocWireless Networks,Boca Raton, FL: CRCPress, 2003.  9.  M. Ilyas and I. Mahgoub,Eds., Mobile Computing Handbook,Boca Raton, FL: Auerbach Publications,      2005. 10.  P. Papadimitratos and Z.J.Haas, ‘‘Securing mobile ad hoc networks,’’ in Mobile Computing Handbook,      M. Ilyas and I. Mahgoub,Eds., Boca Raton, FL: Auerbach Publications, 2005. 11.  C. Perkins, Ad HocNetworking,Reading,MA: Addison-Wesley,2001. 12.  R. Sankar,‘‘Routing and mobilitymanagement in wireless ad hoc networks,’’ in Mobile Computing      Handbook,M.Ilyas and I. Mahgoub,Eds., Boca Raton, FL: Auerbach Publications, 2005. 13.  Y. Shu, O. Yang,and L. Wang,‘‘Adaptiveroutinginadhoc networks,’’ in The Handbook of Ad Hoc      Wireless Networks,M.Ilyas, Ed., Boca Raton, FL: CRCPress, 2003. 14.  C-K. Toh, Ad HocMobile Wireless Networks: Protocols and Systems,Englewood Cliffs, NJ: Prentice Hall,      2002. 15.  C-K. Toh, M. Delawar,and D. Allen, ‘‘Evaluating the communication performanceofanadhoc wireless      networks,’’ IEEE Trans. Wireless Commun.,vol.1,no. 3, pp.402–414, 2002. 16.  C-K. Toh, ‘‘Maximum batterylife routing to supportubiquitous mobile computing in wireless ad hoc      networks,’’ IEEE Commun. Mag.,39, 138–147, 2001. 17.  J. Wu and F. Dai, ‘‘Broadcasting in ad hoc networks based on self-pruning,’’in Proc. IEEE INFOCOM,      2003, pp.2240–2250. 18.  C-H. Yeh, ‘‘Protection and restoration in ad hoc wireless networks,’’ IEEE 10th Int. Conf. Networks,      August, 266–273, 2002.                                                                                                           6                                                       Information Theory                                          6.1   Signal Detection  ................................................................... 6 -1                                               General Considerations * Detection of Known Signals *                                               Detection of Parametrized Signals * Detection of Random Signals *                                               Deciding Among  Multiple Signals * Detection of Signals in More                                               General Noise Processes * Robust and Nonparametric Detection *                                               Distributed and SequentialDetection * Detection with                                              Continuous-Time  Measurements                                        6.2   Noise  ................................................................................ 6 -10                                               Statistics of Noise * Noise Power * Effect of Linear Transformations                                               on Autocorrelationand Power Spectral Density * White, Gaussian,                                               and Pink Noise Models * Thermal Noise as Gaussian                                               White  Noise * Some Examples * Measuring Thermal Noise *                                               EffectiveNoise and Antenna Noise * Noise Factor and Noise Ratio *                                               Equivalent Input Noise * Other Electrical Noise *                                               Measurement and QuantizationNoise   * Coping with Noise                                        6.3   Stochastic Processes ............................................................ 6 -23                                               Introduction to Random  Variables * Stochastic Processes *                                               Classiﬁcations of Stochastic Processes * Stationarity of Processes *  H. Vincent Poor                              Gaussian and Markov Processes * Examples of Stochastic Princeton University                         Processes * Linear Filtering of Weakly StationaryProcesses *                                               Cross-Correlation of Processes * Coherence * Ergodicity Carl G. Looney                         6.4   The Sampling Theorem     ....................................................... 6 -34  University of Nevada                         The Cardinal Series * Proof of the Sampling Theorem *                                               The Time-Bandwidth Product  * Sources of Error * Data Noise * RobertJ.Marks II                                              Generalizations of the Sampling Theorem * Final Remarks University of Washington               6.5   Channel Capacity............................................................... 6 -41 Sergio  Verdu´                               Information Rates * Communication Channels *                                              Reliable Information Transmission: Shannon’sTheorem * Princeton University                                              Bandwidth and Capacity * Channel Coding Theorems Joy A. Thomas                          6.6   Data Compression    .............................................................. 6 -49  Stratify                                     Entropy * The Huffman Algorithm * Entropy Rate *                                               Arithmetic Coding * Lempel–Ziv Coding * Rate Distortion  Thomas     M.  Cover                         Theory  * Quantizationand Vector Quantization * Kolmogorov  Stanford University                          Complexity  * Data Compression in Practice  6.1       Signal Detection H.  Vincent Poor  The ﬁeld of signal detection and estimation is concerned      with the processing of information-bearing signals for the purpose of extracting the information they contain. The applications of this methodologyare quite broad, ranging from areas of electrical engineering such as automatic control, digital communications, image proces- sing,and remote sensing,into other engineering disciplines and the physical, biological, and social sciences.                                                                                                                  6 -16 -2                                   Broadcasting and Optical Communication Technology    There are twobasic types of problems of interest in this context. Signal detection problems are concerned primarily withsituations in which the information to be extracted from asignal is discrete in nature.That is, signal detection procedures are techniques for deciding among adiscrete (usually ﬁnite) number of possible alternatives. An example of such aproblemisthe demodulation of adigital communication signal, in which the task of interest is to decide which of several possible transmitted symbols has elicited agiven received signal. Estimation problems, on the other hand, deal withthe determination of some numerical quantity taking values in acontinuum. An example of an estimation problemisthat of determining the phase or frequency of the carrier underlying acommunication signal.   Althoughsignal detection and estimation is an area of considerable current research activity, the fundamental principles are quite well developed. These principles, which are based on the theoryofstatistical inference, explain and motivate most of the basic signal detection and estimation procedures used in practice. In this section, we will giveabrief overview of the basic principles underlying the ﬁeld of signal detection. Amorecomplete  introduction to these subjects is found in Poor [1994]. General  Considerations The basic principles of signal detection can be conveniently discussed in the context of decision-making  between twopossible statistical models for aset of real-valued measurements, Y 1 , Y 2 , ... , Y n .Inparticular,on observing Y 1 , Y 2 , ... , Y n ,wewish to decide whether these measurements are most consistent with the model                                Y k ¼ N k ;   k ¼ 1 ; 2 ; ... ; n                    ð 6 : 1 Þ  or with the model                              Y k ¼ N k þ S k ; k ¼ 1 ; 2 ; ... ; n                  ð 6 : 2 Þ   where N 1 , N 2 , ... , N n is arandom sequence representingnoise, and where S 1 , S 2 , ... , S n is asequence representinga(possibly random) signal.   In deciding between Equation (6.1) and Equation (6.2), there are twotypes of errors possible: a false alarm, in which Equation (6.2) is falsely chosen, and a miss, in which Equation (6.1) is falsely chosen. The probabilities of these two types of errors can be used as performanceindices in the optimization of rules for deciding between Equation (6.1) and Equation (6.2). Obviously,itisdesirable to minimize both of these probabilities to the extent possible. However,the minimization of the false-alarmprobability and the minimization of the miss probability are opposing criteria. So,itisnecessarytoeffect atrade-offbetween them in ordertodesign asignal detection procedure. There are several ways of trading offthe probabilities of miss and false alarm: the Bayesian detector minimizes an average of the twoprobabilities taken with respect to prior probabilities of the twoconditions Equation (6.1) and Equation (6.2), the minimax detector minimizes the maximum  of the twoerror probabilities, and the Neyman-Pearson detector minimizes the miss probabilityunder an upper-bound constraint on the false-alarm probability.   If the statistics of noise and signal are known,the Bayesian, minimax, and Neyman-Pearson detectors are all of the same form. Namely,they reduce the measurements to asingle number by computing the likelihood ratio                                           D  p S þ N ð Y 1 ; Y 2 ; ... ; Y n Þ                          L ð Y 1 ; Y 2 ; ...; Y n Þ¼                               ð 6 : 3 Þ                                              p N ð Y 1 ; Y 2 ; ... ; Y n Þ   where p S þ N and p N denote the probabilitydensityfunctions of the measurements under signal-plus-noise (Equation (6.2)) and noise-only (Equation (6.1)) conditions, respectively.The likelihood ratio is then compared to a decision threshold,with the signal-present model (Equation (6.2)) being chosen if the threshold is exceeded, and the signal-absent model (Equation (6.1)) being chosen otherwise. Choice of the decision threshold determines atrade-offofthe twoerror probabilities, and the optimum procedures for the three criteria mentioned abovediffer only in this choice.Information Theory                                                                  6 -3    There are several basic signal detection structuresthat can be derived from Equation (6.1) to Equation (6.3) under the assumption that the noise sequenceconsists of aset of independent and identically distributed (i.i.d.) Gaussian random variables with zero means. Such asequence is known as discrete-time white Gaussian noise.Thus, until further notice, we will makethis assumption about the noise. It should be noted that this assumption is physically justiﬁable in manyapplications.  Detection of Known Signals  If the signal sequence S 1 , S 2 , ... , S n is known to be given by aspeciﬁc sequence,say s 1 ,s2 , ... ,sn (a situation known as coherent detection), then the likelihood ratio (Equation (6.3)) is given in the white Gaussian noise case by                                  () !                                     X n      1 X n                               exp      s Y        s 2 = s 2                        ð 6 : 4 Þ                                         k k  2    k                                     k ¼ 1      k ¼ 1  where s 2 is the variance of the noise samples. The only partofEquation (6.4) that depends on the                       P n measurements is the term k ¼ 1 s k Y k and the likelihood ratio is amonotonically increasing function of this quantity. Thus, optimum detection of acoherent signal can be accomplished viaacorrelation detector,which operates by comparing the quantity                                          X n                                            s k Y k                                 ð 6 : 5 Þ                                         k ¼ 1  to athreshold, announcing signal presencewhen the threshold is exceeded.   Note that this detector works on the principle that the signal will correlate well with itself, yielding alarge value of Equation (6.5) when present, whereas the random noise will tend to average out in the sum Equation (6.5), yielding arelatively small value when the signal is absent. This detector is illustrated in Figure 6.1.  Detection of Parametrized Signals The correlation detector cannot usually be used directly unless the signal is known exactly.If, alternatively,the signal is known up to ashortvector y of random parameters (such as frequencies or phases) that are independent of the noise, then an optimum test can be implemented by threshold comparison of the quantity                            () !                       Z       X n         1 X n                         exp      s ð yyyyy Þ Y   ½ s ð yyyyy Þ 2 = s 2 p ð yyyyy Þ d yyyyy ð 6 : 6 Þ                                   k   k   2     k                               k ¼ 1        k ¼ 1                       L   wherewehavewritten S k ¼ s k ( yyyyy )toindicate the functional dependenceofthe signal on the parameters, and where L and p ( yyyyy )denote the range and probabilitydensityfunction, respectively,ofthe parameters.              FIGURE 6.1 Correlation detector for acoherent signal in additive white Gaussian noise.6 -4                                   Broadcasting and Optical Communication Technology    The most importantexample of such aparametrized signal is that in which the signal is amodulated sinusoid with random phase; i.e.,                           S k ¼ a k cos ð o c k þ y Þ ; k ¼ 1 ; 2 ; ... ; n         ð 6 : 7 Þ  where a 1 ,a2 , ... , a n is aknown amplitude modulation sequence, o c is aknown (discrete-time) carrier frequency,and the random phase y is uniformly distributed in the interval [–p , p ]. In this case, the likelihood ratio is amonotonically increasing function of the quantity                       "#"#                         X n              2     X n              2                            a k cos ð o c k Þ Y k þ a k sin ð o c k Þ Y k           ð 6 : 8 Þ                         k ¼ 1                  k ¼ 1  Thus, optimum detection can be implemented viacomparison of Equation (6.8) with athreshold, astructure known as an envelope detector.Note that this detector correlates the measurements with twoorthogonal  components of the signal, a k cos ð o c k Þ and a k sin ð o c k Þ .These two correlations, known as the in-phase and quadraturecomponents of the measurements, respectively, capture all of the energyinthe signal, regardless of the value of y .Since y is unknown, however,these two correlations cannot be combined coherently,and thus they are combined noncoherently viaEquation (6.8) beforethe result is compared with athreshold. This detector is illustrated in Figure 6.2.   Parametrized signals also arise in situations in which it is not appropriate to model the unknown parameters as random variables with aknown distribution. In such cases, it is not possible to compute the likelihood ratio (Equation (6.6)) so an alternativetothe likelihood ratio detector must then be used. (An exception is that in which the likelihood ratio detector is invariant to the unknown parameters—a case known as uniformly most powerful detection.) Several alternatives to the likelihood ratio detector exist for these cases.   One useful such procedureistotest for the signal’spresence by threshold comparison of the generalized likelihood ratio,given by                                   max  L y ð Y 1 ; Y 2 ; ... ; Y n Þð6                : 9 Þ                                   y 2 L   whereLy denotes the likelihood ratio for Equation (6.1) and Equation (6.2) for the known-signal problem with the parameter vector ﬁxed at y .Inthe case of white Gaussian noise, we have                                       () !                                          X n         1 X n                 L ð Y ; Y ; ... ; Y Þ¼exp    s ð yyyyy Þ Y   ½ s ð yyyyy Þ 2 = s 2 ð 6 : 10Þ                  y  1  2      n              k    k  2      k                                          k ¼ 1         k ¼ 1  It should be noted that this formulation is also valid if the statistics of the noise haveunknown parameters, e.g., the noise variance in the white Gaussian case.   One common  application in which the generalized likelihood ratio detector is useful is that of detecting a signal that is known except for its time of arrival. That is, we are often interested in signals parametrized as                                        s k ð y Þ¼a k   y                           ð 6 : 11Þ  where{a k }isaknown ﬁnite-duration signal sequence and where y ranges over the integers. Assuming white Gaussian noise and an observation interval much longer than the duration of { a k }, the generalized likelihood ratio detector in this case announces the presence of the signal if the quantity                                           X                                      max    a k   y Y k                           ð 6 : 12Þ                                       y                                           kInformation Theory                                                                  6 -5             FIGURE 6.2 Envelope detector for anoncoherent signal in additive white Gaussian noise.   exceeds aﬁxedthreshold. This type of detector is known as a matched ﬁlter,sinceitcan be implemented by ﬁltering the measurements with adigital ﬁlter whose pulse response is atime-reversed version of the known  signal { a k }(hence it is ‘‘matched’’tothe signal), and announcing the signal’spresence if the ﬁlter output exceeds the decision threshold at anytime.  Detection of Random     Signals In some applications, particularly in remote sensing applications such as sonar and radio astronomy,itis  appropriate to consider the signal sequence S 1 ,S2 , ... , S n itself to be arandom sequence, statistically independent of the noise. In such cases, the likelihood ratio formula of Equation (6.6) is still valid with the parameter vector y simply taken to be the signal itself. However,for long measurement records(i.e., large n), Equation (6.6) is not averypractical formula except in some speciﬁc cases, the most importantofwhich is the case in which the signal is Gaussian.                                                                         D   In particular,ifthe signal is Gaussian with zero-mean and autocorrelation sequence r k ; l ¼ ESfgk S l ,then the likelihood ratio is amonotonically increasing function of the quantity                                       X n X n                                            q k ; l Y k Y l                        ð 6 : 13Þ                                      k ¼ 1 l ¼ 1  with q k,l the element in the k th rowand l th columnofthe positive-deﬁnite matrix                                    Q ¼ D I  ðI þ R = s 2 Þ   1                     ð 6 : 14Þ  where I denotes the n · n identitymatrix, and R is the covariance matrix of the signal, i.e., it is the n · n  matrix with elements r k,l.   Note that Equation (6.13) is aquadratic function of the measurements; thus, adetector based on the comparison of this quantitytoathreshold is known as a quadratic detector. The simplest form of this detector results from the situation in which the signal samples are, like the noise samples, i.i.d. In this case, the6 -6                                   Broadcasting and Optical Communication Technology  quadratic function (Equation (6.13)) reduces to apositive constant multiple of the quantity                                           X n                                              2                                             Y k                                   ð 6 : 15Þ                                          k ¼ 1  Adetector based on Equation (6.15) simply measures the energyinthe measurements and then announces the presence of the signal if this energyislarge enough. This type of detector is known as a radiometer.   Thus, radiometryisoptimum in the case in which both signal and noise are i.i.d. Gaussian sequences with zeromeans. Sinceinthis case the presenceofthe signal is manifested only by an increase in energylevel, it is intuitively obvious that radiometryisthe only wayofdetecting the signal’spresence. More generally,when the signal is correlated, the quadratic function (Equation (6.13)) exploits both the increased energyleveland the correlation structure introduced by the presence of the signal. For example, if the signal is anarrowband Gaussian process, then the quadratic function (Equation (6.13)) acts as anarrowband radiometer with bandpass characteristic that approximately matches that of the signal. In general, the quadratic detector will makeuse of whatever spectral properties the signal exhibits.   If the signal is random but not Gaussian, then its optimum detection (described by Equation (6.6)) typically requires morecomplicated nonlinear processing than the quadratic processing of Equation (6.13) in orderto exploit the distributional differencesbetween signal and noise. This type of processing is often not practical for implementation, and thus approximations to the optimum detector are typically used. An interesting family of such detectors uses cubic or quartic functions of the measurements, which exploit the higher-order spectral properties of the signal [Nikias and Petropulu, 1993]. As with deterministic signals, random signals can be parametrized. In this case, however,itisthe distribution of the signal that is parametrized. For example, the powerspectrum of the signal of interest maybeknown only up to aset of unknown parameters. Generalized likelihood ratio detectors (Equation (6.9)) are often used to detect such signals.  Deciding  Among    Multiple  Signals The preceding results havebeen developed under the model (Equations 6.1–6.2) that there is asingle signal that is either present or absent. In digital communications applications, it is morecommon to havethe situation in which we wish to decide between the presenceoftwo (or more) possible signals in agiven set of measurements. The foregoing results can be adapted straightforwardly to such problems. This can be seen most easily in the case of deciding among known signals. In particular,consider the problem of deciding between two alternatives:                                          ð 0 Þ                              Y k ¼ N k þ s k ; k ¼ 1 ; 2 ; ... ; n                ð 6 : 16Þ  and                                          ð 1 Þ                              Y k ¼ N k þ s k ; k ¼ 1 ; 2 ; ... ; n                ð 6 : 17Þ        ð 0 Þ ð 0 Þ ð 0 Þ ð 1 Þ ð 1 Þ ð 1 Þ where s 1 ; s 2 ; ...; s n and s 1 ; s 2 ; ... ; s n are twoknown equi-energysignals. Such problems arise in data transmission problems, in which the two signals s (0) and s (1) correspond to the waveforms received after transmission of alogical ‘‘zero’’ and ‘‘one,’’ respectively.Insuch problems, we are generally interested in minimizing the averageprobability of error, which is the average of the twoerror probabilities weighted by the prior probabilities of occurrence of the twosignals. This is aBayesian performancecriterion, and the optimum decision rule is astraightforward extension of the correlation detector based on Equation (6.5). In particular, under the assumptions that the two signals are equally likely to occur prior to measurement, and that the noise is white and Gaussian, the optimum decision between Equation (6.16) and Equation (6.17) is to choose the                         n  ð 0 Þ            n  ð 1 Þ model (Equation (6.16)) if S k ¼ 1 s k Y k is larger thanS k ¼ 1 s k Y k ,and to choose the model (Equation (6.17)) otherwise.Information Theory                                                                  6 -7    More generally,manyproblems in digital communications involvedeciding among M equally likely signals with M . 2. In this case, again assuming equi-energysignals and white Gaussian noise, the decision rule that                                             ð j Þ ð j Þ ð j Þ minimizes the error probabilityistochoose the signal s 1 ; s 2 ; ... ; s n ,where j is asolution of the maximization problem                                X n                X n                                   ð j Þ              ð m Þ                                  s Y k ¼   max      s  Y k                        ð 6 : 18Þ                                   k      0 < m < M   1 k                               k ¼ 1              k ¼ 1  There are two basic types of digital communications applications in which the problem(Equation (6.18)) arises. One is in M-arydata transmission, in which asymbol alphabet with M elements is used to transmit data, and adecision among these M symbols must be made in each symbol interval [Proakis, 2000]. The other type of application in which (Equation (6.18)) arises is that in which data symbols are correlated in some waybecause of intersymbol interference,coding,ormultiuser transmission. In such cases, each of the M possible signals represents aframe of data symbols, and ajoint decision must be made about the entire frame sinceindividual symbol decisions cannot be decoupled. Within this latter framework,the problem (Equation (6.18)) is known as sequence detection. The basic distinction between M -arytransmission and sequence detection is one of degree. In typical M -arytransmission, the number of elements in the signaling alphabet is typically asmall powerof2(say8or 32), whereas the number of symbols in aframe of data could be on the orderofthousands. Thus, solution of Equation (6.18) by exhaustive search is prohibitive for sequence detection, and less complex algorithms must be used. Typical digital communications applications in which sequence detection is necessaryadmit dynamic programming solutions to Equation (6.18) (see, e.g., Poor [2002]).  Detection of Signals in More General Noise Processes In the foregoing paragraphs, we havedescribed threebasic detection procedures: correlation detection of signals that are completely known,envelope detection of signals that are known except for arandom phase, and quadratic detection for Gaussian random signals. These detectors were all derivedunder an assumption of white Gaussian noise. This assumption provides an accurate model for the dominant noise arising in many communication channels. Forexample, the thermal noise generated in signal processing electronics is ade- quately described as being white and Gaussian. However,there are also manychannels in which the statistical behavior of the noise is not well described in this way, particularly when the dominant noise is produced in the physical channel rather than in the receiver electronics.   One type of noise that often arises is noise that is Gaussian but not white. In this case, the detection problem Equations (6.1)–(6.2) can be converted to an equivalent problem with white noise by applying alinear ﬁltering process known as prewhitening to the measurements. In particular,ondenoting the noise covariance matrix by S ,wecan write                                         S ¼  CCT                                   ð 6 : 19Þ  where C is an n · n invertible, lower-triangular matrix and wherethe superscript T denotes matrix transposition. The representation (6.19) is known as the Cholesky decomposition.Onmultiplying the                    D             T                                                     1 measurement vector Y ¼ ð Y 1 ; Y 2 ; ...; Y n Þ satisfying Equations (6.1)–(6.2) with noise covariance S ,by C , we produce an equivalent (in terms of information content) measurement vector that satistiﬁes the model Equations (6.1)–(6.2) with white Gaussian noise and with the signal conformally transformed. This model can then be treated using the methods described previously.   In other channels, the noise can be modeled as being i.i.d. but with an amplitude distribution that is not Gaussian. This type of model arises, for example, in channels dominated by impulsivephenomena, such as certain radio channels. In the non-Gaussian case the procedures discussed previously lose their optimalityas deﬁned in terms of the error probabilities. These procedures can still be used, and they will work well under6 -8                                   Broadcasting and Optical Communication Technology  manyconditions; however,therewill be aresulting performancepenaltywithrespect to optimum procedures based on the likelihood ratio.Generally speaking, likelihood-ratio-based procedures for non-Gaussian noise channels involvemorecomplex nonlinear processingofthe measurements than is required in the standard detectors, althoughthe retention of the i.i.d. assumption greatly simpliﬁes this problem. Atreatment of methods for such channels can be found in Kassam [1988].   When the noise is both non-Gaussian and dependent, the methodologyisless well developed, although some techniques are available in these cases. An overview can be found in Poor and Thomas [1993].  Robust  and  Nonparametric   Detection All of the procedures outlined above are based on the assumption of aknown (possibly up to aset of unknown parameters) statistical model for signals and noise. In manypractical situations it is not possible to specify accurate statistical models for signals or noise, and so it is of interest to design detection procedures that do not relyheavily on such models. Of course, the parametrized models described in the foregoing paragraphs allowfor uncertaintyinthe statistics of the observations. Such models are known as parametric models, because the set of possible distributions can be parametrized by aﬁnite set of real parameters.   While parametric models can be used to describe manytypes of modeling uncertainty, composite models in which the set of possible distributions is much broader than aparametric model would allow are sometimes morerealistic in practice. Such models are termed nonparametric models.For example, one might be able to assume only some very coarse model for the noise, such as that it is symmetrically distributed. Awidevariety of useful and powerful detectors havebeen developed for signal-detection problems that cannot be para- metrized. These are basically of two types: robust and nonparametric .Robustdetectors are those designed to perform well despite small, but potentially damaging,nonparametric deviations from anominal parametric model, whereas nonparametric detectors are designed to achieve constant false-alarm probabilityoververy wide classes of noise statistics.   Robustness problems are usually treated analytically viaminimax formulations that seek best worst-case performanceasthe design objective. This formulation has proventobeveryuseful in the design and characterization of robust detectors for awide varietyofdetection problems. Solutions typically call for the introduction of light limiting to prevent extremes of gain dictated by an (unrealistic) nominal model. For example, the correlation detector of Figure 6.1 can be made robust against deviations from the Gaussian noise model by introducing asoft-limiter between the multiplier and the accumulator.   Nonparametric detection is usually based on relatively coarse information about the observations, such as the algebraic signs or the ranks of the observations. One such test is the sign test, which bases its decisions on the number of positive observations obtained. This test is nonparametric for the model in which the noise samples are i.i.d. with zeromedian and is reasonably powerful against alternativessuch as the presence of a positive constant signal in such noise. More powerful tests for such problems can be achievedatthe expense of complexitybyincorporating rank information into the test statistic.  Distributed  and Sequential  Detection The detection proceduresdiscussed in the precedingparagraphs are based on the assumption that all measurements can and should be used in the detection of the signal, and moreover that no constraints exist on how measurements can be combined. There are anumber of applications, however,inwhich constraints apply to the information pattern of the measurements.   One type of constrained information pattern that is of interest in anumber of applications is anetwork consisting of anumber of distributed or local decision makers, each of which processes asubset of the measurements, and a fusion center, which combines the outputs of the distributed decision makers to produce aglobal detection decision. The communication between the distributed decision makers and the fusion center is constrained, so that each local decision maker must reduce its subset of measurements to a summarizing local decision to be transmitted to the fusion center.Suchstructuresarise in applications such as the testing of large-scale integrated circuits, in which data collection is decentralized, or in detection problemsInformation Theory                                                                  6 -9  involving very large data sets, in which it is desirable to distribute the computational work of the detection algorithm or to partition the data for securityreasons.Suchproblems lie in the ﬁeld of distributed detection. Except in some trivial special cases, the constraints imposed by distributing the detection algorithm introduce afurther level of difﬁcultyinto the design of optimum detection systems. Nevertheless, considerable progress has been made on this problem, asurveyofwhich can be found in Tsitsiklis [1993].   Another type of nonstandardinformation pattern that arises is that in which the number of measurements is potentially inﬁnite, but in which there is acost associated with taking each measurement. This type of model arises in applications such as the synchronization of wideband communication signals. In such situations, the error probabilities alone do not completely characterize the performanceofadetection system, since consideration must also be given to the cost of sampling.The ﬁeld of sequential detection deals with the optimization of detection systems within such constraints. In sequential detectors, the number of measurements takenbecomesarandom  variable depending on the measurements themselves. Atypical performancecriterion for optimizing such asystem is to seek adetector that minimizes the expected number of measurements for given levels of miss and false-alarm probabilities.   The most commonly used sequential detection procedureisthe sequential probability ratio test, which operates by recursive comparison of the likelihood ratio (Equation (6.3)) to two thresholds. In this detector,if the likelihood ratio for agiven number of samples exceeds the larger of the twothresholds, then the signal’s presence is announced and the test terminates. Alternatively,ifthe likelihood ratio falls belowthe smaller of the twothresholds, the signal’sabsenceisannounced and the test terminates. However,ifneither of the two thresholds is crossed, then another measurement is takenand the test is repeated.  Detection with Continuous-Time Measurements Note that all of the preceding formulations have involved the assumption of discrete-time (i.e., sampled-data) measurements. From apractical point of view,this is the most natural framework within which to consider these problems, since implementations most often involve digital hardware. However,the procedures discussed herein all havecontinuous-time counterparts, which are of both theoretical and practical interest. Mathematically,continuous-time detection problems are moredifﬁcult than discrete-time ones, because they involve probabilistic analysis on function spaces. The theoryofsuch problems is quite elegant, and the interested reader is referred to Poor [1994] or Kailath and Poor [1998] for moredetailed exposition.   Continuous-time models are of primaryinterest in the front-end stages of radio frequency or optical communication receivers. At radio frequencies, continuous-time versions of the models described in the precedingparagraphs can be used. For example, one mayconsider the detection of signals in continuous-time Gaussian white noise. At optical wavelengths, one mayconsider either continuous models (such as Gaussian processes)orpoint-process models (such as Poisson counting processes), depending on the type of detection used (see, e.g., Snyder and Miller [1991]). In the most fundamental analyses of optical detection problems, it is sometimes desirable to consider the quantum mechanical nature of the measurements [Helstrom, 1976].  Deﬁning   Terms Bayesian detector: Adetector that minimizes the average of the false-alarm and miss probabilities,      weighted with respect to prior probabilities of signal-absent and signal-present conditions. Correlation detector: The optimum structure for detecting coherent signals in the presence of additive      white Gaussian noise. Discrete-time white Gaussian noise: Noise samples modeled as independent and identically distributed      Gaussian random variables. Envelope detector: The optimum structure for detecting amodulated sinusoid with random phase in the      presence of additive white Gaussian noise. False-alarm probability: The probabilityoffalsely announcing the presenceofasignal. Likelihood ratio: The optimum processor for reducing aset of signal-detection measurements to asingle      number for subsequent threshold comparison.6 -10                                  Broadcasting and Optical Communication Technology  Miss probability: The probabilityoffalsely announcing the absence of asignal. Neyman-Pearson   detector: Adetector that minimizes the miss probability within an upper-bound      constraint on the false-alarm probability. Quadratic detector: Adetector that makes use of the second-order statistical structure (e.g., the spectral      characteristics) of the measurements. The optimum structurefor detecting azero-mean Gaussian signal      in the presenceofadditiveGaussian noise is of this form.  References C.W.Helstrom, Quantum Detection and Estimation Theory ,New York: Academic Press, 1976. T. Kailath and H.V.Poor,‘‘Detection of stochastic processes’’, IEEE Transactions on Information Theory ,vol. 42,      no.5,pp. 2230–2259, 1998. S.A. Kassam, Signal Detection in Non-Gaussian Noise,New York: Springer-Verlag,1988. C.L. Nikias and A. Petropulu, Higher-Order Spectral Analysis,Englewood Cliffs, NJ: Prentice-Hall,1993. H.V.Poor, An Introduction to Signal Detection and Estimation,2nd ed., NewYork: Springer-Verlag,1994. H.V.Poor,‘‘Dynamic programming  in digital communications: Viterbi decoding to turbo multiuser      detection,’’ Journal of Optimization Theoryand Applications,vol. 115, no.3,pp. 629–657, 2002. H.V.Poor and J.B. Thomas, ‘‘Signal detection in dependent non-Gaussian noise’’, in Advances in Statistical      Signal Processing,vol. 2, Signal Detection, H.V.Poor and J.B. Thomas, Eds., Greenwich, Conn.: JAI      Press, 1993. J.G. Proakis, Digital Communications,4th ed., NewYork: McGraw-Hill, 2000. D.L. Snyderand M.I. Miller, Random Point Processes in Time and Space,New York: Springer-Verlag,1991. J. Tsitsiklis, ‘‘Distributed detection,’’ in Advances in Statistical Signal Processing,vol. 2, Signal Detection,      H.V.Poor and J.B. Thomas, Eds., Greenwich, Conn.: JAIPress, 1993.  Further  Information Except as otherwise noted in the accompanying text, further details on the topics introduced in this section can be found in the textbook: Poor,H.V. An Introduction to Signal Detection and Estimation, 2nd ed., NewYork: Springer-Verlag,1994.   The monthly journal, IEEE Transactions on Information Theory, publishes recent advances in the theoryof signal detection. It is available from the Institute of Electrical and Electronics Engineers, Inc., 445 HoesLane, Piscataway, NJ 08854.   Papers describing applications of signal detection are published in anumber of journals, including the monthly journals IEEE Transactions on Communications, IEEE Transactions on Signal Processing, and the Journal of the Acoustical Society of America. The IEEE journals are available from the IEEE, as above. The Journal of the Acoustical Society of America is available from the American Institute of Physics, 2 Huntington Quadrangle, Melville, NY 11747.  6.2     Noise Carl  G.  Looney The physical processing of everyinformation signal s ( t )corrupts the original signal by adding new ﬂuctuations at the output that were not present at the input. The output signal can be decomposed into twocomponents: the original signal and the added ﬂuctuations, which are referred to as noise. The noise is undesirable because it limits the dynamic range of s ( t ), and therefore effortismade to reduce it to the minimum  theoretical value. These undesirable signals weretermed noise due to early measurements with sensitiveaudio ampliﬁers.   Noise sources are:(1) intrinsic,(2) external,or(3) process induced.Intrinsic noise arises in active devices, passive components, and conductors. The largest noise contribution is from statistical ﬂuctuationsInformation Theory                                                                 6 -11  in current ﬂowinactive devices, the next largest is fromthermal energyofelectrons in resistors, and the least is from microboundaries of impurities and grains with varying potential in conductors. Intrinsic sourcessuch as Johnson (thermal) noise and shot noise are well understood, but the physical processes for additional layers of noise on top of these at low frequencies such as 1/f (one over f )orﬂicker noise, popcornorburst noise are current topics of research.External interference sources can be either electromagnetic or electrostatic ﬁelds, or in the case or radio waves, both. Electromagnetic sources are the largest contributor and include powerlines, incandescent lighting dimmer controls, brush type electric motors, welding apparatus, gasoline engines, switching power supplies, and anydevice that causes alarge and rapid rate of change in current ﬂow. Electrostatic sources include ﬂuorescent lightning and power lines. Other sources include electromagnetic wavesfromradio transmitters, lightning,cosmic rays, plasmas (charged particles) in space, and solar/stellar radiation. Reﬂective objects and other macroboundaries cause multiple paths of transmitted signals and lead to delayed copies of the original signal being superimposed on s ( t ). Process-induced errors include measurement, quantization, truncation, and signal generation errors. These all can corrupt the signal by introducing anoise ﬂoor that masks s ( t )below that level.   Noise can be divided into the categories random, impulse, and periodic. While intrinsic sources are principally random, most extrinsic sourcesare either periodic or impulse. In the case of periodic noise, time domain ﬁltering can achieve large reductions of noise by subtracting acopy of the undesired noise. Atypical extrinsic sourceispower line noise and will havespectra consisting of multiples of the line frequency that can be removedbycomb ﬁltering in the frequency domain. Impulse noise can occur randomly or periodically.In either case, time domain slew rate limiting can reduce the noise. For random noise, frequency domain ﬁltering can removethose portions of noise spectra outside the needed spectra of s ( t ).  Statistics of Noise Statistics allow us to analyze the spectra of noise. We model anoise signal by a random (or stochastic) process  N ( t ), afunction with arealized value N ( t ) ¼ x t at anytime instant t is chosen by the outcome of the random variable N t ¼ N ( t ). N ( t )has aprobabilitydistribution for the values x it can assume. Anyparticular trajectory {(t , x t )} of outcomesiscalled a realization of the noise process. The ﬁrst-order statistic of N ( t )isthe expected value m t ¼ E [ N ( t )]. The second-order statistic is the autocorrelation function R NN( t , t þ t ) ¼ E [ N ( t ) N ( t þ t )], where E [–] is the expected value operator. Autocorrelation measures the extent to which noise random  variables N 1 ¼ N ( t 1 )and N 2 ¼ N ( t 2 )attimes t 1 and t 2 depend on each other in an average sense.   When the ﬁrst- and second-order statistics do not change over time, we call the noise a weakly (or  wide-sense) stationaryprocess .This means that: (1) E [ N ( t )] ¼ m t ¼ m is constant for all t ,and (2) R NN( t , t þ t ) ¼ E [ N ( t ) N ( t þ t )] ¼ E [ N (0)N ( t )] ¼ R NN( t )for all t (see Brown, 1983, p. 82; Gardner,1990, p. 108; or Peebles, 1987, p. 153 for properties of R NN( t )). In this case, the autocorrelation function depends only on the offset t .Therefore, we assume that m ¼ 0(we can subtract m ,which does not change the                                                        2     2 autocorrelation). When t ¼ 0, R NN ð 0 Þ¼E ½ N ð t Þ N ð t þ 0 Þ  ¼ E ½ðN ð t ÞÞ  ¼s N ,which is the ﬁxed varianceof each random variable N t for all t .Weakly stationary(ws) processes are the most commonly encountered cases and are the ones considered here. Evolutionaryprocesses havestatistics that change over time and are difﬁcult to analyze.   Figure6.3 shows arealization of anoise process N ( t ), whereatany particular time tthe probabilitydensity function projects out of the page in athird dimension. Foraws noise, the distributions are the same for each t . The most mathematically tractable noises are Gaussian ws processes, whereateach time t the probability  distribution for the random variable N t ¼ N ( t )isGaussian (also called normal). The ﬁrst- and second-order statistics completely determine Gaussian distributions, and thereforewsmakes the statistics of all orders stationaryovertime also.Itiswell known [Brown,1983, p. 39] that linear transformations of Gaussian random variables are also Gaussian random variables. The probabilitydensity function for aGaussian random                         2 1 = 2        2   2 variable N t is f N ð x Þ¼f 1 = ½ 2 psN   exp½ ðx   m N Þ = 2 s N  g,which is the familiar bell-shaped curve centered on x ¼ m N .The standard Gaussian probabilitytable [Peebles, 1987, p. 314] is useful, e.g., Pr[  s N , N t , s N ] ¼ 2Pr[0 , Nt , s N ] ¼ 0.8413 from the table.6 -12                                  Broadcasting and Optical Communication Technology                                   FIGURE 6.3 Anoise process.  Noise  Power  The noise signal N ( t )represents voltage, thereforethe autocorrelation function at offset 0, R NN(0) ¼ E [ N ( t ) N ( t )] represents expected powerinvolts squaredorwatts per ohm. When R ¼ 1 O ,then N ( t ) N ( t ) ¼ N ( t )[N ( t )/R ] ¼ N ( t ) I ( t )volt-amperes ¼ watts (where I ( t )isthe current in a1- O resistor). The  Fourier transform F [ R NN( t )] of the autocorrelation function R NN( t )isthe power spectrum, called the power spectral density function (psdf), S NN ( w )inW/(rad/s). Then                                Z 1                                              j ws                       S NN ð w Þ¼   R NN ð t Þ e d t ¼ F ½ R NN ð t Þ                                     1                                   Z 1                                             ð 6 : 20Þ                                 1             j ws       1                       R NN ð t Þ¼     S NN ð w Þ e d w ¼ F ½ S NN ð w Þ                                 2 p    1  The psdf at frequency f is deﬁned as the expected powerthat the voltage N ( t ), bandlimited to an incremental band df centered at f ,would dissipate in a1- O resistance, divided by df.    Equation (6.20), known as the Wiener–Khinchin relation, establishes that S NN( w )and R NN( t )are aFourier transform pair for ws random processes [Brown,1983; Gardner,1990, p. 230; Peebles, 1987]. The psdf S NN ( w ) has units of W/(rad/s), whereas the autocorrelation function R NN ( t )has units of watts. When t ¼ 0inthe                                                   0                        2    2 second integral of Equation (6.20), the exponential becomes e ¼ 1, so that R NN ð 0 Þð¼ E ½ N ð t Þ  ¼s N Þ is the integral of the psdf S NN ( w )overall radian frequencies,   1 , w , 1 .The rms (root-mean-square) voltage is N rms ¼ ( s N (the standard deviation). The power spectrum in W/(rad/s) is adensity that is summed up viaan integral over the radian frequency band w 1 to w 2 to obtain the total poweroverthat band.                                 1 Z w 2                  P NN ð w 1 ; w 2 Þ¼ S NN ð w Þ · d w watts                                2 p w 1                                                   Z 1                             ð 6 : 21Þ                                 2          2    1                         P NN ¼ s N ¼ E ½ N ð t Þ  ¼   S NN ð w Þ · d w watts                                                2 p   1              2 The variance s N ¼ R NN ð 0 Þ is the mean instantaneous power P NN over all frequenciesatany time t .Information Theory                                                                 6 -13  Effect of Linear Transformations on Autocorrelation and Power Spectral Density When  h ( t )isthe impulse response function of atime-invariant linear system L ,and H ( w ) ¼ F [ h ( t )] is its  transfer function, let the input noise signal N ( t )havethe autocorrelation function R NN( t )and psdf S NN( w ). Then we denote the output noise signal by Y ( t ) ¼ L [ N ( t )]. The Fourier transforms Y ( w )   [ F [ Y ( t )] and N ( w )   [ F [ N ( t )] do not exist, but they are not needed. The output Y ( t )ofalinear system is ws wheneverthe  input N ( t )isws[Gardner,1990, p. 195; or Peebles, 1987, p. 215]. The output psdf S YY( w )and autocorrelation function R YY( t )are given by the following,respectively,                                     2                      1                     S YY ð w Þ¼j H ð w Þj S NN ð w Þ ; R YY ð t Þ¼F ½ S YY ð w Þ  ð 6 : 22Þ  [see Gardner,1990, p. 223]. The output noise power is                                Z 1                Z 1                   2          1                  1            2                 s Y ¼ P YY ¼       S YY ð w Þ dw ¼    j H ð w Þj S NN ð w Þ dw    ð 6 : 23Þ                             2 p   1            2 p   1  White, Gaussian, and Pink Noise Models White noise [Brown,1983; Gardner,1990, p. 234; or Peebles, 1987] is atheoretical model W ( t )ofnoise that is  ws withzeromean. It has aconstant powerlevelwith n o overall frequencies (analogous to white light), so its psdf is S WW( w ) ¼ n o W/(rad/s),   1 , w , 1 .The inverse Fourier transform of this is the impulse function R WW( t ) ¼ ( n o ) d ( t ), which is zero for all offsets except t ¼ 0. Therefore, white noise W ( t )isaprocess that is uncorrelated over time, i.e., E [ W ( t 1 ) W ( t 2 )] ¼ 0for t 1 not equal to t 2 .Figure6.4(a) shows the autocorrelation and psdf for white noise wherethe offset is s ¼ t .AGaussian white noise is white noise such that the probability  distribution of each random variable W t ¼ W ( t )isGaussian. When two Gaussian random variables W 1 and W 2 are uncorrelated,i.e., E [ W 1 W 2 ] ¼ 0, they are independent [Gardner,1990, p. 37]. We use Gaussian models because of the central limit theorem states that the sum of anumber of random variables is approximately Gaussian.   Actual circuits attenuate signals abovecutofffrequencies, and the power must be ﬁnite. However,for white  noise, P WW ¼ R NN(0) ¼ 1 ,soweoften truncate the white noise spectral density(psdf)atcutoffs   w c to w c . The result is known as band limited white noise, P ( t ), and is usually takentobeGaussian because linear ﬁltering of anywhite noise (through the effect of the central limit theorem) tends to make the noise Gaussian                                                              1 [Gardner,1990, p. 241]. Figure6.4(b) shows the sinc function R PP( s ) ¼ F [ S PP( w )] for pink noise. Random variables P 1 and P 2 at times t 1 and t 2 are correlated only for t 1 and t 2 close.                  FIGURE 6.4 Powertransform pairs for white and band limited white noise.6 -14                                  Broadcasting and Optical Communication Technology  Thermal   Noise  as Gaussian  White  Noise Brown observed in 1828 that pollen and dust particles moved randomly when suspended in liquid. In 1906, Einstein analyzed such motion based on the random walk model. Perrin conﬁrmed in 1908 that the thermal activityofmolecules in aliquid caused irregular bombardment of the much larger particles. It was predicted that charges bound to thermally vibrating molecules would generate electromotive force(emf)atthe open terminals of aconductor,and that this placed alimit on the sensitivityofgalvanometers. Thermal noise (also called Johnson noise)was ﬁrst observedbyJ.B. Johnson at Bell Laboratories in 1927. Figure6.5 displays white noise as seen in the laboratoryonanoscilloscope.   The voltage N ( t )generated thermally between twopoints in an open circuit conductor is the sum of an extremely large number of superimposed, independent electronically,and ionically induced microvoltages at  all frequencies up to f c ¼ 6,000 GHz at room temperature [Gardner 1990, p. 235] near infrared. The mean                                           10 relaxation time of freeelectrons is 1/f c ¼ 0.5 · 10 / Ts,thereforeatroom temperature of T ¼ 290K, it is 0.17 ps (1 picosecond ¼ 10  12 s). The values of N ( t )atdifferent times are uncorrelated for time differences  (offsets) greater than t c ¼ 1/f c .The expected value of N ( t )iszero. The powerissomewhat constant across a broad spectrum, and we cannot sample signals at picosecond periods, so we model Johnson noise N ( t )with Gaussian white noise W ( t ). Although m ¼ E [ W ( t )] ¼ 0, the average powerispositive at temperatures above 0K,        2 and is s W ¼ R WW ð 0 Þ (see the right side of Equation (6.21)). Adisadvantage of the white noise model is its                          2 inﬁnite power,i.e., R WW ð 0 Þ¼s W ¼ 1 ,but it is valid if band limited to B Hz, in which case its power is ﬁnite.   In 1927, Nyquist [1928] theoretically derivedthermal noise power in aresistor to be                                  P WW ð B Þ¼4 kTRB ð wattsÞð6                        : 24Þ  where R is resistance (ohms), B is the frequency bandwidth of measurementinHz(all emf ﬂuctuations outside  of B are ignored), P WW( B )isthe mean power over B (see Equation (6.21)), and Boltzmann’s constant is k ¼ 1.38 · 310   23 J/K [Ott, 1988; Gardner,1990, p. 288; or Peebles, 1987, p. 227]. Under external emf, the thermally induced collisions are the main source of resistance in conductors (electrons pulled into motion by                                                                      1/2 an external emf at 0K meet no resistance). The rms voltage is W rms ¼ s W ¼ [(4kTRB)] Voverabandwidth of B Hz.                                                                      34   Planck’s radiation lawis S NN ð w Þ¼ð 2 h j f jÞ= ½ expð h j f j = kTÞ 1   ,where h ¼ 6.63 · 10 J/s is Planck’sconstant, and f is the frequency [Gardner,1990, p. 234]. For j f j much smaller than kT/h ¼ 6.04 · 1012 Hz< 6,000 GHz, the  exponential above can be approximated by expð h j f j = kTÞ¼1 þ h j f j = kT.The denominator of S NN( w )becomes   FIGURE 6.5 Thermal noise in aresistor.(Source:H.W.Ott, Noise Reduction Techniques in Electronic Systems,2nd ed., NewYork: Wiley-Interscience, 1988, p. 203. With permission.)Information Theory                                                                 6 -15                               FIGURE 6.6 Thermal noise in aresistor.  h j f j = kt,so S NN ð w Þ¼ð 2 h j f jÞ= ð h j f j = kTÞ¼2 kT W = Hz in a1- O resistor.OveraresistanceofRO and abandwidth of B Hz (positivefrequencies) yields the total power P WW( B ) ¼ 2 BRSNN( w ) ¼ 4 kTRB Woverthe two-sided frequency spectrum. This is Nyquist’s result.   Thermal noise is the same in a1000-O carbon resistor as it is in a1000-O tantalum thin-ﬁlm resistor [Ott, 1988]. While the intrinsic noise may never be less, it maybehigher because of other superimposed noise. We model the thermal noise in aresistor by an internal source (generator), as shown in Figure6.6. Capacitance cannot be ignored at high f ,but pure reactance ( C or L )cannot dissipate energy, and thereforecannot generate  thermal noise. The white noise model W ( t )for thermal noise N ( t )has aconstant psdf S WW( w ) ¼ n o W/(rad/s) for   1 , w , 1 .ByEquation (6.21), the white noise mean poweroverthe frequency bandwidth B is                                1 Z 2 p B                    P WW ð B Þ¼       S WW ð w Þ d w ¼ n o ð 4 p B = 2 p Þ¼2 n o B ð 6 : 25Þ                              2 p   2 p B   Solving for the constant n o ,weobtain n o ¼ P WW( B )/2B ,which we put into Equation (6.20) to obtain the spectral densityasafunction of temperature and resistance using Nyquist’s result above.             S WW ð w Þ¼n o ¼ P WW ð B Þ = 4 p B ¼ 4 kTR2 p B = 4 p B ¼ 2 kTR watts= ð rad= s Þð6 : 26Þ  Some Examples The parasitic capacitanceinthe terminals of aresistor maycause aroll-off of approximately 6dB/octavein actual resistors [Brown,1983, p. 139]. At 290K (room temperature), we have2kT ¼ 2 · 1.38 · 10  23 · 290 ¼         20                                           6                    14 0.8 · 10 W/Hz due to each ohm [Ott, 1988]. For R ¼ 1MO (10 O ), S WW( w ) ¼ 0.8 · 10 .Over aband of   8                                      14  8           6 10 Hz, we have P WW( B ) ¼ S WW( w ) B ¼ 0.8 · 10 · 10 ¼ 0.8 · 10 W ¼ 0.8 m WbyEquation (6.24) and Equation (6.26). In practice, parasitic capacitancecauses thermal noise to be bandlimited (bandlimited white noise). Nowconsider Figure6.6(b), and let the temperature be 300K, R ¼ 106 O , C ¼ 1pf(1picofarad ¼ 10  12 farads), and assume L is 0H. By Equation (6.21), the thermal noise power is                                               23        6            17              S WW ð w Þ¼2 kTR ¼ 2 · 1 : 38 · 10 · 300 · 10 ¼ 828 · 10 W = Hz                                  6                                12 The power across abandwidth B ¼ 10 is P WW( B ) ¼ S WW( w )B ¼ 8280 · 10 W, so the rms voltage is               1/2 W rms ¼ [ P WW( B )] ¼ 91 m V.6 -16                                  Broadcasting and Optical Communication Technology    Nowlet  Y ( t )bethe output voltage across the capacitor.The transfer function can be seen to be H ( w ) ¼ { I ( w )(1/jwC)}/{I ( w )[R þ (1/jwC)]} ¼ (1/jwC)/[R þ 1/jwC] ¼ 1/[1þ j wRC ](where I ( w )isthe Fourier transform of the current). The output psdf (see Equation (6.22)) is                                    2                   2  2 2                   S YY ð w Þ¼j H ð w Þj S WW ð w Þ¼ð 1 = ½ 1 þ w R C  ÞS WW ð w Þ                          2 2 2 Integrating S YY( w ) ¼ (1/[1þ w R C ])S WW( w )overall radian frequencies w ¼ 2 p f (see Equation (6.21)), we obtain the antiderivative (828 · 10  17)(1/RC)atan(RCw)/2p .Upon substituting the limits w ¼ ^ 1 ,this becomes     828 · 10  17[ p /2þ p /2]/2p RC ¼ 414 · 10  17(1/2RC) ¼ 207 · 10  17 · 106 ¼ 2070 · 10  12 W/Hz.        2       2                         12                             1/2 Then  s Y ¼ E ½ Y ð t Þ  ¼P YY ð 1 ; 1 Þ¼2070 · 10 W, so Y rms( t ) ¼ s Y ¼ [ P YY(   1 , 1 )] ¼ 45.5 m V. The                                               6 half-power (cut-off) radian frequency is w c ¼ 1/RC ¼ 10 rad/s, or f c ¼ w c /2p ¼ 159.2 kHz. Approximating                                             6      6 S YY( w )bythe rectangular spectrum S YY( w ) ¼ n o ,   10 , w , 10 rad/s (0 elsewhere), we havethat R YY( t ) ¼ ( w c / p )sinc(w c t ), which has the ﬁrst zeros at j w c t j¼p ,that is j t j¼1 = ð 2 f c Þ (see Figure6.4(b)). We approximate the autocorrelation by R YY( t ) ¼ 0for j s j > 1 = 2 f c .  Measuring   Thermal   Noise  In Figure6.7, the thermal noise from anoisy resistor R is to be measured, where R L is the measurement load. The incremental noise power in R over an incremental frequency band of width df is P WW( df) ¼ 4 kTRdf W, by  Equation (6.24). P YY( df)isthe integral of S YY( w )over df by Equations (6.21), where              2 S YY ð w Þ¼j H ð w Þj S WW ð w Þ ,byEquation (6.22). In this case, the transfer function H ( w )isnonreactiveand does not depend upon the radian frequency (we can factor it out of the integral). Thus,                            Z df                                     2                        2                 P YY ð df Þ¼   j H ð f Þj ð 2 kTRÞ d f ¼fR L = R þ R L Þ gð4 kTRdf Þ                               df   To maximize  the power measured, let R L ¼ R .The incremental available power measured is then             2     2 P YY( df) ¼ 4 kTR df/(4R ) ¼ kTdf [Ott, 1988, p. 201; Gardner,1990, p. 288; or Peebles, 1987, p. 227]. Therefore, we havethe result that incremental available poweroverbandwidth df depends only on the temperature T .                           P YY ð df Þ¼kTdf ð output power over df Þð6                : 27Þ                             FIGURE 6.7 Measuring thermal noise voltage.Information Theory                                                                 6 -17    In 1906, AlbertEinstein used statistical mechanics to postulate that the mean kinetic energyper degreeof freedom of aparticle, (1/2)mE[ v 2 ( t )], is equal to (1/2)kT,where: m is the mass of the particle, v ( t )isits instantaneous velocityinasingle dimension, k is Boltzmann’s constant, and T is the temperatureinkelvin. Ashunt capacitor C is charged by the thermal noise in the resistor (see Figure6.6(b), where L is assumed to be zero). The average potential energystored is (1/2)CE[ W ( t ) 2 ]. Equating this to 1/2kT and solving,weobtain the mean square power                                    E ½ W ð t Þ 2  ¼kT= C                          ð 6 : 28Þ  Forexample, let T ¼ 300K and C ¼ 50 pf, and recall that k ¼ 1.38 · 10  23 J/K. Then E [ W ( t ) 2 ] ¼ kT/ C ¼ 82.8 · 10  12,sothat the input rms voltage is { E [ W ( t ) 2 ]}1/2 ¼ 9.09 m V.  Effective Noise and Antenna Noise  Let twoseries resistors R 1 and R 2 haverespective temperaturesof T 1 and T 2 .The total noise poweroveran incremental frequency band df is P Total( df) ¼ P 11( df) þ P 22( df) ¼ 4 kT1 R 1 dfþ 4 kT2 R 2 df ¼ 4 k ( T 1 R 1 þ T 2 R 2 ) df. By putting                                T E ¼ðT 1 R 1 þ T 2 R 2 Þ = ð R 1 þ R 2 Þð6           : 29Þ  we can write P Totalð df Þ¼4 kTE ð R 1 þ R 2 Þ df . T E is called the effective noise temperature [Gardner,1990, p. 289; or Peebles, 1987, p. 228]. An antenna receives noise from various sources of electromagnetic radiation, such as radio transmissions and harmonics, switching equipment (such as computers, electrical motor controllers), thermal (blackbody) radiation of the atmosphere and other matter,solar radiation, stellar radiation, and galaxial radiation (the ambient noise of the universe). To account for noise at the antenna output, we model  the noise withanequivalent thermal noise using an effective noise temperature T E .The incremental available power (output) over an incremental frequency band df is P YY ( df) ¼ kTE df,fromEquation (6.27). T E is often called antenna temperature,denoted by T A .Althoughitvaries with the frequency band, it is usually virtually constant over asmall bandwidth.  Noise Factor and Noise Ratio In reference to Figure6.8(a), we deﬁne the noise factor F ¼ ( noise power output of actual device)/ ( noise power output of ideal device), where(noise power output of ideal device) ¼ (power output due to                           FIGURE 6.8 Equivalent input noise and noise factor.6 -18                                  Broadcasting and Optical Communication Technology  thermal noise source). The noise source is taken to be anoisy resistor R at atemperature T ,and all output  noise measurements must be takenoveraresistive load R L (reactance is ignored). Letting P WW ( B ) ¼ 4 kTRB be the open circuit thermal noise powerofthe source resistor over afrequency bandwidth B ,and noting that the                                                                    2               2 gain of the device is G ,the output powerdue to the resistive noise source becomes G P WW ð B Þ¼4 kTRBG = R L . Nowlet Y ( t )bethe output voltage measured at the output across R L .Then the noise factor is                                      2                                2                  F ¼ðP YY ð B Þ = R L Þ = ð G P WW ð B Þ = R L Þ¼ð P YY ð B ÞÞ= ð 4 kTRBG Þð6 : 30Þ   F is seen to be independent of R L ,but not R .Tocomparetwo noise factors, the same sourcemust be used. In the ideal noiseless case, F ¼ 1, but as the noise level in the device increases, F increases. Because this is apower ratio,wemay take the logarithm, called the noise ratio,which is                                                                       2                  N F ¼ 10 log10ð F Þ¼10 log10ð P YY ð B ÞÞ   10 log10ð 4 kTRBG Þð6  : 31Þ     The noise power output P YY( B )ofanactual device is asuperposition of the ampliﬁed source thermal noise  2                                     2 G P WW( B )and the device noise, i.e., P YY( B ) ¼ G P WW( B ) þ (device noise). The output noise across R L can be measured by putting asingle frequency (in the passband) source generator S ( t )asinput. First, S ( t )isturned  off, the output rms voltage Y ( t )ismeasured, and the output power P Y(W) ( B )isrecorded. This is the sum of the thermal available powerand the device noise. Next, S ( t )isturned on and adjusted until the output power  doubles, i.e., until the output power P Y ( W ) ( B ) þ P Y ( S ) ( B ) ¼ 2 P Y ( W ) ( B ). This P SS ( B )isrecorded. Solving                                                   2 for P Y ( S ) ( B ) ¼ P Y ( W ) ( B ), we substitute this in F ¼ P Y ( W ) ( B )/(G P WW( B )) to obtain                             2             2          2             F ¼ P Y ð S Þ ð B Þ = ð G P WW ð B ÞÞ ¼ðG P SSð B ÞÞ= ð G 4 kTRBÞ¼P SSð B Þ = 4 kTRB ð 6 : 32Þ    Abetter wayistoinput white noise W ( t )inplaceof S ( t )(anoise diode may be used). The disadvantages of noise factors are: (1) when the device has low noise relativetothermal noise, the noise factor has value close to 1; (2) alow resistance causes highvalues; and (3) increasing the sourceresistancedecreases the noise factor while increasing the total noise in the circuit [Ott, 1988, p. 216]. Therefore,the accuracyisnot adequate. For cascaded devices, the noise factors can be conveniently computed [Buckingham, 1985, p. 67; or Ott, 1988, p. 228].  Equivalent  Input  Noise Shot noise (see below) and other noise can be modeled by the equivalent thermal noise that would be generated in an input resistor by increased temperature. Recall the (maximum) incremental available power  (output) in afrequency bandwidth df is P WW ( df ) ¼ kTdf from Equation (6.27). Figure 6.8(b) presents the following situation. Let the resistor be the noise sourceattemperature T o with thermal noise W ( t ). Then       2 E [ W ( t ) ] ¼ 4 kTo Rdf,byEquation (6.24) (Nyquist’sresult). Let the open circuit output noise powerat R L be      2 E [ Y ( t ) ]. The incremental available noise power P YY ( df)atthe output ( R L ¼ R )can be considered to be due to the resistor R having ahigher temperature and an ideal (noiseless) device, usually an ampliﬁer.Wemust ﬁnd a                                                     2 temperature T e at which apseudothermal noise power E [ W e ( t ) ] ¼ 4 kTe Rdf yields the extra ‘‘input’’noise power. Let V ( t ) ¼ W ( t ) þ W e ( t ). Then P VV ( df ) ¼ 4 kTo Rdfþ 4 kTe Rdf ¼ 4 k ( T o þ T e ) Rdf W, from Equation (6.24). T e is called the equivalent input noise temperature.Itisrelated to the noise factor F by T e ¼ 290(F   1). In cascaded ampliﬁers with gains G 1 , G 2 , ... and equivalent input noise temperatures T e1, T e2, ... ,the total equivalent input noise temperature is                          T e ð TotalÞ ¼ T e1 þ T e2= G 1 þ T e3= G 1 G 2 þ ...     ð 6 : 33Þ  [Gardner,1990, p. 289].Information Theory                                                                 6 -19  Other Electrical Noise Thermal noise and shot noise, which can be modeled by thermal noise withequivalent input noise, are the main noise sources. Other noises are discussed in the following paragraphs. Shot Noise In aconductor under an external emf, thereisanaverage ﬂowofelectrons, holes, photons, etc. In addition to this induced net ﬂowand thermal noise, thereisanother effect. The potential differs across the boundaries of metallic grains and particles of impurities, and when the kinetic energyofelectrons exceeds this potential, electrons jump across the barrier.This summed random ﬂowisknown as shot noise [Gardner,1990, p. 239;                                                                    1/2                 19 Ott, 1988, p. 208]. The shot effect was analyzed by Schottky in 1918 as I sh ¼ (2qIdc B ) ,where q ¼ 1.6 · 10 coulombs per electron, I dc ¼ average dc current in amperes, and B ¼ noise bandwidth (Hz). Partition Noise Partition noise is caused by aparting of the ﬂow of electrons to different electrodes into streamsofrandomly varying density. Suppose that electrons from some sourceSﬂowtodestination electrodes A and B .Let n ( A ) and n ( B )bethe average numbers of electrons per second that go to nodes A and B respectively, so that n ( S ) ¼ n ( A ) þ n ( B )isthe average total number of electrons emitted per second. It is asuccess when an electron goes to A ,and the probabilityofsuccess on asingle trial is p ,where                              p ¼ n ð A Þ = n ð S Þ ; 1   p ¼ n ð B Þ = n ð S Þð6     : 34Þ  The current to the respectivedestinations is I ( A ) ¼ n ( A ) q , I ( B ) ¼ n ( B ) q ,where q is the charge of an electron, so that I ( A )/I ( S ) ¼ p and I ( B )/I ( S ) ¼ 1   p .Using the binomial model, the average numbers of successes are E [ n ( A )] ¼ n ( S ) p and E [ n ( B )] ¼ n ( S )(1  p ). The varianceisVar(n ( A )) ¼ n ( S ) p (1  p ) ¼ Var(n ( B )) (from the binomial formula for variance). Therefore,substitution yields                 Varð I ð A ÞÞ ¼ q 2 ½ n ð S Þ p ð 1   p Þ  ¼ q 2 n ð S ÞfI ð A Þ I ð B Þ = ½ I ð A ÞþI ð B Þ g ð 6 : 35Þ    Partition noise applies to pentodes, wherethe sourceisthe cathode, A is the anode (success), and B is the grid. For transistors, the source is the emitter, A is the collector,and B represents recombination in the base. In photo devices, aphotoelectron is absorbed, and either an electron is emitted (a success) or not. Even a partially silvered mirror can be considered to be apartitioner;the passing of aphoton is asuccess and reﬂection is afailure.While the binomial model applies to partitions with destinations A and B ,multinomial models are analogous for morethan two destinations. 1/f, Burst, and Contact Noise In 1925, J.B. Johnson ﬁrst noticed that noise across thermionic gates exceeded the expected shot noise at lower frequencies. This added noise, abovethermal noise, is characterized by aspectral distribution that has equal energyineach octave. This leads to astraight line on alog-log graph with aslope of   1. The corner frequency whereitintercepts the thermal noise is usually in the neighborhood of 1KHz, but improvements in semiconductor processing cleanliness havepushed it down towards 10 Hz. The physical mechanism causing 1/f noise is currently attributed to one of twocompeting theories: (1) surface defects trapping carriers, and (2) abulk phenomenon involving ﬁlaments of current varying within asemiconductor channel [Lundberg p. 3]. The surface defect theoryexplains the current problem in semiconductor miniaturization: 1/f noise is increasing as the size of transistors become smaller.Sincethe ratio of surface area to volume increases as size decreases,the surfacedefect process theoryexplains that 1/f noise is increasing in more densely populated semiconductors.   The problem with a1/ f spectra is the inverse of the ultraviolet catastrophe in physics, whereintegrating to inﬁnityyields an unbounded energy. Here, integrating to zero frequency leads to unbounded energy. One popular model is the Lorentzian spectra, or the response of alow-pass ﬁlter,whereatsome very low frequency the energybecomesconstant per unit bandwidth for all lowerfrequenciesand decreases abovethat point as 1/f [Poore, 2001, pp.1–2]. For some physical processes thereisoften ahighfrequency abovewhich the 1/f spectra6 -20                                  Broadcasting and Optical Communication Technology  falls offas1/f^^2 [Milotti, p. 4]. The psdf of the extra noise, called ﬂicker noise, is                                    S ð f Þ¼I 2 = a f ; f > 0                       ð 6 : 36Þ  where I is the dc current ﬂowing throughthe device and f is the positive frequency.Empirical values of (a) are approximately 0.5 to 1.6 for different sources. These sources vary, but include the size irregularity of the cathode surface macroregions, impurities in the conducting channel, and generation and recombinationnoise in transistors. In the early days of transistors, this generation-recombination was of great concern because the materials werenot highinpurity. Flicker noise occurs in thin layers of metallic or semiconducting material, solid-state devices, carbon resistors, and vacuum tubes [Buckingham, 1985, p. 143]. Flickernoise maybehigh at low frequencies.   Contact noise is caused by ﬂuctuating conductivitydue to imperfect contact between twosurfaces, especially in switches and relays.   Burst noise is also called popcornnoise.Audio ampliﬁers sound like popcorn popping in afrying pan background (thermal noise). Its characteristic is 1/f n (usually n ¼ 2), so its power densityfalls offrapidly, where f is frequency.Itmay be problematic at low frequencies. The cause is manufacturing defects in the junction of transistors (usually ametallic impurity). Barkhousen  and Other Noise Barkhousen noise is due to the variations in size and orientation of the small regions of ferromagnetic material and is especially noticeable in the steeply rising region of the hysteresis loop.There is also secondaryemission, photo,and collision ionization, etc.  Measurement    and  Quantization   Noise Measurement  Error  The measurement X t of asignal X ( t )atany t results in ameasured value X t ¼ x that contains error,and so is not equal to the true value X t ¼ x T .The probability is higher that the magnitude of e ¼ ( x   x T )iscloser to zero. The bell-shaped Gaussian probabilitydensity f ( e ) ¼ [1/(2p s 2]1/2exp(  e 2 /2p s )ﬁts the error well. This noise                                                                       2 process is stationaryovertime. The expected value is m e ¼ 0, the mean-square error is s e ,and the rms error is                                 2 s e .Its instantaneous powerattime t is s e .Tosee this, the error signal e ( t ) ¼ ( x   x T )has instantaneous power per O of                                                          2                             P i ¼ e ð t Þ i ð t Þ¼e ð t Þ½e ð t Þ = R  ¼e ð t Þð6   : 37Þ  where R ¼ 1 O and i ( t )isthe current. The average poweristhe summed instantaneous power over aperiod of time T ,divided by the time, takeninthe limit as T ! 1 ,i.e.,                                                 Z T                                                    2                                P ave ¼ lim ð 1 = T Þ e ð t Þ d t                                      T ! 1       0  This average power can be determined by sampling on known signal values and then computing the sample variance (assuming ergodicity: see Gardner [1990, p. 163]). The error and signal are probabilistically independent, unless the error depends on the values of X .The signal-to-noise power ratio is computed by  S/N ¼ P signal/ P ave . Quantization Noise  Quantization noise is due to the digitization of an exact signal value v t ¼ v ( t )capturedatsampling time tbyan                                                                                       n A/D converter.The binaryrepresentation is b n   1 b n   2 ... b 1 b 0 (an n -bit word). The n -bit digitization has 2 different values possible, from zero to 2 n   1. Let the voltage range be R .The resolution is dv ¼ R /2n .Any  voltage v t is coded into the nearest lowerbinaryvalue x b ,wherethe error e ¼ x t   x b satisﬁes 0 # e # dv.Information Theory                                                                 6 -21  Thus, the errors e are distributed equally over the interval [0, dv]insuch away that implies the uniform  distribution on [0, dv]. The expected value of e ¼ e t ¼ e ( t )atany time is m e ¼ dv/2, and the variance is  2    2                                                                2 m e ¼ dv = 12 (the variance of auniform distribution on an interval [ a , b ]is s ¼ ( b   a ) /12). Therefore, the noise is ws and the powerofquantization noise is                        Z dv                   2                2                  s e ¼   ð e   dv= 2 Þ ð 1 = dvÞ d e                        0                                                          ð 6 : 38Þ                                3     dv       3       3           2                  d e ¼ðe   dv= 2 Þ = 3 dvj 0 ¼½ð dvÞ þðdvÞ   = 24dv ¼ dv = 12  We can ﬁnd the signal-to-noise voltage ratio for the total range R via R /(dv/(12)1/2) ¼ 2 n dv/(dv/(12)1/2) ¼ 2 n    1/2                                        2 n (12) .The power ratio is the square of this, which is (2 )(12). In decibels this becomes ( S/N) dB ¼ 10 log10   2 n (2  ·12) ¼ 10 log10 (12) þ 20n log10 (2) ¼ 10.8 þ 6.02n .Thus, the quantization S/N powerratio depends directly upon the number of bits n in that the higher S/N power ratio is better as is expected.  Coping with Noise External interference is ubiquitous. Intrinsic noise is present up to the incremental available power at temperatures above absolute zero, and other intrinsic noises depend on material purityand connection integrity.Processing error is always introduced in some form. External Sources Standarddefenses are:    1. 100% shielding of lines and circuits: thin conductors will stop electrostaticﬁelds, but magnetic ﬁelds       require thicker shielding and low reluctancemetals.    2. Twisted wire pairs are effective against magnetic ﬁelds combined with highcommon mode rejection       balanced inputs.    3. Coaxial cables with multiple shields for radio frequencies.    4. Shortlines and leads.    5. Multilayered PCBs with internal groundplanes each side of buried signal lines.    6. Digital regeneration at waypoints of digital signals.    7. Matched ﬁltering of signals.    8. Correlation of received signals with multipaths.    9. Adaptivenotch ﬁltering to eliminate interferenceatknown frequencies; e.g., the second harmonic of       60-Hz ac powerlines mayinterferewith biological microvoltage measurements, but could be       eliminated viaadaptivenotch ﬁltering.   10. Star grounding to prevent ground loops.   11. Lowpass ﬁlters on powerleads between circuits.   12. Orienting magnetic components for minimum pickup.   Ferrite beads can dampen interference [Barnes, 1987]. They are used on power leads providing alow resistance to DC current while presenting highimpedancetohighfrequencies. The core losses lead to an appreciable portion of the AC impedancebeing resistive, thereforeavoidingresonances, digital signal processing, spectral shaping ﬁlters [Brown,1983], and frequency-shift ﬁlters [Gardner,1990, p. 400] that can be used to lowernoise power. Kalman ﬁltering is apowerful estimation method, and frequency-shift ﬁltering is anewer technique for discriminating against both measurement error (e.g., in system identiﬁcation applications) and extrinsic sources of both noise and interference [Gardner,1990, p. 400]. Intrinsic Sources Strategies for minimizing intrinsic noise are: (1) small bandwidth B ,(2) small resistances R ,(3) low temperature T (higher temperaturescan be devastating), (4) low voltage and currents (CMOS transistors), (5) modern materials of highpurity, (6) the latest generation active components with the lowest 1/f noise, (7) wire6 -22                                  Broadcasting and Optical Communication Technology  wound resistors (thermal noise is the same, but other noise will be less), (8) fewer and better connections (of gold), (9) smaller circuits of lowerpower,(10) shunt capacitors to reduce noise bandwidth, (11) averagingto gain squareroot of N improvement in signal to noise ratio,and (12) using selected JFET devices. Currently, progress in the purityofintegrated circuit materials is leading to ayearly improvement in 1/f and other extra noises. Better design and materials are the keys to lowernoise.  Processing Sources Processing errors can be reducedbyusing higher resolution of analog-to-digital converters, i.e., morebits to represent each value. This lowersthe quantization error power. Measurement error can be reduced while using the same instruments by taking multiple measurements and averaging.Other estimation/correlation can yield better values (e.g., the Global Positioning System location determination can be reduced from meters to afew centimeters by amultiple measurement estimation).  Deﬁning   Terms Autocorrelation: Afunction associated with arandom signal X ( t )that is deﬁned on pairs of time instants       t 1 and t 2 ,and which the value is the expected value of the product of the random variables X ( t 1 )and      X ( t 2 ), i.e., R XX( t 1 , t 2 ) ¼ E [ X ( t 1 ) X ( t 2 )]. For weakly stationaryrandom signals, it depends only on the offset      t ¼ t 2   t 1 ,sowewrite R XX( t ) ¼ E [ X ( t ) X ( t þ t )]. Noise:  Asignal N ( t )inwhich the value at anytime t is randomly selected by events beyond our control. At       anytime instant t , N ( t )isarandom variable N t with aprobabilitydistribution that determines the      relativefrequencies at which N t assumes values. The statistics of the family of random variables { N t }may      be constant (stationary) over time (in most cases) or mayvary. Power spectral density: The Fourier transform of the power X 2 ( t )does not necessarily exist, but it does          2      for X T ð t Þ = 2 T ð X T ð t Þ¼0for j t j > T ; ¼ X ð t Þ elsewhere), for anyT. 0. Letting T ! 1 ,the expected value                               2             2      of the Fourier transforms E ½ F ½ X T ð t Þ = 2 T  ¼F ½ E ½ X T ð t Þ = 2 T goes to the limit of the average power in X ( t )      over   T to T ,known as the power spectral densityfunction S xx(w ) .Summed up over all frequencies, it      gives the total power in the signal X ( t ). Random   process: Asignal that is either anoise, an interfering signal s ( t ), or asum of these such as       X ð t Þ¼s 1 ð t Þþ...þ s m ð t ÞþN 1 ð t Þþ... þ N n ð t Þ . Realization: Atrajectory{( t , x t ): X ( t ) ¼ x t }determined by the actual outcomes { x t }ofvalues from a      random signal X ( t ), where X ( t ) ¼ x t at each instant t .Atrajectoryisalso called a sample function of X ( t ). Weakly  stationary (ws) random process (signal): Arandom signal whose ﬁrst- and second-order      statistics remainstationary(ﬁxed) over time.   References J.R. Barnes, Electronic System Design: Interference and Noise Control,Englewood Cliffs, N.J.: Prentice-Hall,      1987. R.G. Brown, Introduction to Random Signal Analysis and Kalman Filtering,New York: Wiley,1983. M.J.Buckingham, Noise in Electronic Devices and Systems,New York: Halstead Press, 1985. W.A. Gardner, Introduction to Random Processes,2nd ed., NewYork: McGraw-Hill, 1990. J.B. Johnson, ‘‘Thermal agitation of electricityinconductors,’’ Phys. Rev.,vol. 29, pp.367–368, 1927. J.B. Johnson, ‘‘Thermal agitation of electricityinconductors,’’ Phys. Rev.,vol. 32, pp.97–109, 1928. K.H. Lundberg,‘‘Noise SourcesinBulk CMOS,’’ Unpublished paper,p.3http://web.mit.edu/klund/www/      CMOSnoise.pdf, 2002. E. Milotti, ‘‘1/f Noise: APedagoglical Review,’’ H. Nyquist, ‘‘Thermal agitation of electric charge in conductors,’’ Phys. Rev.,vol. 32, pp.110–113, 1928. H.W.Ott, Noise Reduction Techniques in Electronic Systems,2nd ed., NewYork: Wiley-Interscience, 1988.Information Theory                                                                 6 -23  P.Z. Peebles, Jr., Probability,Random Variables, and Random Signal Principles,2nd ed., NewYork: McGraw-      Hill, 1987. R.  Poore, ‘‘Phase Noise and  Jitter,’’Agilent Technologies, http://eesof.tm.agilent.com/pdf/jitter_      background.pdf, 2001.  Further Information The IEEE Individual Learning Program, ‘‘Random Signal Analysis with Random Processes and Kalman Filtering,’’prepared by Carl G. Looney (IEEE Educational Activities Board, PO Box1331, Piscataway,NJ 08855–1331, 1989) contains agentle introduction to estimation and Kalman ﬁltering.   Another recommended sourceisbyH.M. Denny, ‘‘Getting Rid of Interference’’, IEEE Video Conference, Educational Activities Dept., Piscataway, NJ,08855–1331, 1992.  6.3     Stochastic Processes Carl G. Looney Introduction to Random     Variables A random variable (rv) A is speciﬁed by its probability density function (pdf)                       f A ð a Þ¼lime ! 0 ð 1 = e Þ P ½ a  ðe = 2 Þ < A < a þðe = 2 Þ    In other words, the rectangular area e · f A ( a )approximates the probability P ½ðA < a þðe = 2 Þ   P ½ a  ðe = 2 Þ 5 A   : The joint pdf of two rv’s A and B is speciﬁed by                               2          f ABð a ; b Þ¼lime ! 0 ð 1 = e Þ P ½ a   e < A < a þðe = 2 Þ and b   e < B < b þðe = 2 Þ   Asimilar deﬁnition holds for anyﬁnite number of rv’s.    The expected value E [ A ], or mean m A ,ofarv A is the ﬁrst moment of the pdf, and the variance of A is the second centralized moment, deﬁned respectively by                                              Z 1                                m A ¼ E ½ A      afA ð a Þ d a                    ð 6 : 39aÞ                                                1                                               Z 1                          2             2                2                         s A ¼ E ½ðA   m A Þ     ð a   m A Þ f A ð a Þ d a        ð 6 : 39bÞ                                                1  The squareroot of the variance is the standard deviation,which is also called the root mean square (rms) error. The covariance of tworv’s A and B is the second-order centralized joint moment                                         Z 1 Z 1              s AB ¼ E ½ðA   m A ÞðB   m B Þ     ð a   m A Þðb   m B Þ f ABð a ; b Þ d a d b ð 6 : 40Þ                                            1   1  The noncentralized second moments are the mean-square value and the correlation,respectively,             Z 1                                  Z 1  Z 1       2         2           2    2   E ½ A  ¼     a f A ð a Þ d a ¼ s A þ ; m A ; E ½ AB ¼  abfABð a ; b Þ d a d b ¼ s AB þ m A m B               1                                     1   16 -24                                  Broadcasting and Optical Communication Technology  Aset of rv’s A, B, and C is deﬁned to be independent whenever their joint pdf factors as                                 f ABCð a ; b ; c Þ¼f A ð a Þ f B ð b Þ f C ð c Þð6   : 41Þ  for all a, b, and c, and similarly for anyﬁnite set of rv’s.Aweak independence holds when the second moment  of the joint pdf, the correlation, factors as E [ AB] ¼ E [ A ] E [ B ], so that s AB ¼ 0, in which case the rv’s are said to be uncorrelated. The covariance of A and B is ameasure of how often A and B varytogether (have the same sign), how often they varyoppositely (different signs), and by how much, on the average over trials of outcomes. To standardize so that units do not inﬂuence the measureofdependence, we use the correlation coefﬁcient                                      r AB   s AB= s A s B    The accuracy of approximating arv A as alinear function of another rv B, A < cB þ d, for real coefﬁcients c and d, is found by minimizing the mean-square error e ¼ E {[A –(cB þ d )]2 }. Upon squaring and taking the                                  2        2 expected values, we can obtain e min ¼ s A ð 1  jr ABj Þ ,which shows j r ABj to be ameasureofthe degree of linear relationship between A and B .Because e min $ 0, this shows that j r ABj # 1, which demonstrates the Cauchy-Schwarz inequality                                           no1          = 2                                j E ½ AB j < E ½ A 2   E ½ B 2                     ð 6 : 42Þ   When j r ABj¼1, then knowledge of one of A or B completely determines the other ( c 6¼ 0), and so A and B are completely dependent, while j r ABj¼0indicates there is no linear relationship,i.e., that A and B are uncorrelated.   An important result is the fundamental theorem of expectation:if g (·)isany real function, then the expected value of the rv B ¼ g ( A )isgiven by                                               Z 1                             E ½ B  ¼E ½ g ð A Þ  ¼ g ð a Þ f A ð a Þ d a          ð 6 : 43Þ                                                 1  Stochastic  Processes  A stochastic (or random) process is acollection of random variables { X t : t [ T }, indexed on an ordered set T that is usually asubset of the real numbers or integers. Examples are the Dow-Jones averages D ( t )ateach time t, the pressure R ( x )inapipe at distance x, or anoise voltage N ( t )attime t .Aprocess is thus a random function  X ( t )of t whose value at each t is drawn randomly from arange of outcomes for the rv X t ¼ X ( t )according to aprobabilitydistribution for X t .Atrajectory{x t : t [ T }ofoutcomes over all t [ T, where X t ¼ x t is the realized value at each t, is called a sample function (or realization)ofthe process. Astochastic process X ( t )has  mean value E [ X ( t )] ¼ m ( t )attime t ,and autocorrelation function R XX ð t ; t þ t Þ¼E ½ X ð t Þ X ð t þ t Þ  at times t and t þ t ,the correlation of tworv’sattwo times offset by t .When m ( t ) ¼ 0for all t ,the autocorrelation  function equals the autocovariance function C XX ð t ; t þ t Þ¼E ½ðX ð t Þ m ð t ÞÞðX ð t þ t Þ m ð t þ t ÞÞ .   Aprocess X ( t )iscompletely determined by its joint pdf’s f X ð t ð 1 ÞÞ...X ð t ð n ÞÞð x ð t 1 Þ ; ... ; x ð t n ÞÞ for all time combinations t 1 , ... , t n and all positive integers n (where t ( j ) ¼ t j ). When the rv’s X ( t )are iid (independent, identically distributed), then knowledge of one pdf yields the knowledge of all joint pdf’s.This is because we can construct the joint pdf by factorization, per Equation (6.41).  Classiﬁcations  of Stochastic  Processes The ordered set T can be continuous or discrete, and the values that X ( t )assumes at each t mayalso be continuous or discrete, as shown in Table 6.1.Information Theory                                                                 6 -25               TABLE 6.1 Continuous/Discrete Classiﬁcation of Stochastic Processes                                               X Values                 T Values         Continuous                 Discrete               Continuous  Continuous stochastic processes Discrete valued stochastic processes              Discrete    Continuous random sequences Discrete valued random sequences    In another classiﬁcation, astochastic process X ( t )is deterministic whenever an entiresample function can be  determined from an initial segment f x t : t < t 1 g of X ð t Þ .Otherwise, it is nondeterministic [Brown,1983, p. 79; or Gardner,1990, p. 304].  Stationarity of Processes Astochastic process is nth order ( strongly) stationary wheneverall joint pdf’s of n and fewer rv’s are  independent of all translations of times t 1 , ... , t n to times t þ t 1 , ... , t þ t n .The case of n ¼ 2isvery useful. Another type of process is called weakly stationary (ws), or wide-sense stationary ,and is deﬁned to haveﬁrst- and second-order moments that are independent of time. These satisfy (1) m ( t ) ¼ m (constant)  for all t ,and (2) R XX ð t ; t þ t Þ¼R XXð t þ s ; t þ s þ t Þ for all values of s .For s ¼ t, this yields R XXð t ; t þ t Þ¼R XXð 0 ; 0 þ t Þ ,which is abbreviated to R XX( t ). X ( t )is uncorrelated whenever C XX( t ) ¼ 0for t not zero[we say X ( t )has no memory ]. If X ( t )iscorrelated, then X ( t 1 )depends on values X ( t )for t 6¼ t 1 [ X ( t ) has memory ].    Some properties of autocorrelation functions for ws processes follow. First, R XX ð t Þj < R XXð 0 Þ ;   1 5t51 ,                                         2                   2      2 as can be seen fromEquation (6.42) with j R XXð t Þj ¼ E ½ X ð 0 Þ X ð t Þ  < E ½ X ð 0 Þ   E ½ X ð t Þ  ¼R XX ð 0 Þ R XXð t Þ .Next, R XX( t )isrealand even, i.e., R XXð t Þ¼R XXð t Þ ,which is evident from substituting s ¼ t – t in E ½ X ð s Þ X ð s þ t Þ  and using time independence. If X ( t )has aperiodic component, then R XX( t )will have that same periodic component, which follows from the deﬁnition. Finally,if X ( t )has anonzeromean mand no periodic                                                                              2    2 components, then the variance goes to zero(the memoryfades) and so limt ! 1 R XXð t Þ!0 þ m ¼ m .  Gaussian and Markov Processes  Aprocess X ( t )isdeﬁned to be Gaussian if for everypossible ﬁnite set f t 1 ; ... ; t n g of times, the rv’s X ð t 1 Þ ; ... ; X ð t n Þ are jointly Gaussian,which means that everylinear combination Z ¼ a 1 X ð t 1 Þþ... þ a n X ð t n Þ is aGaussian rv,deﬁned by the Gaussian pdf                                  hip     ﬃﬃﬃﬃﬃ                                                          2   2                         f Z ð z Þ¼ 1 = ð s Z 2 p Þ expf ðz   m Z Þ = 2 s Z gð6      : 44Þ   In case the n rv’s are linearly independent,i.e., Z ¼ 0only if a 1 ¼ ... ¼ a n ¼ 0, the joint pdf has the Gaussian form [see Gardner,1990, pp.39–40]                                              n = 2 1 = 2          t   1            f X ð t ð 1 ÞÞ   X ð t ð n ÞÞð x 1 ; ... ; x n Þ¼ 1 = ð 2 p Þ jjC expf ðx   mmmmm Þ C ð x   mmmmm Þg ð 6 : 45Þ                                       t where x ¼ ( x 1 , ... , x n )isacolumn vector, x is its transpose, m ¼ ( m 1 , ... , m n )isthe mean vector, C is the covariance matrix                                         *+2                                          s 1 ::  s ln                                   C ¼    ::                                       ð 6 : 46Þ                                                   2                                         s ln ::  s n6 -26                                  Broadcasting and Optical Communication Technology   and j C j is the determinant of C .If X ( t 1 ),... , X ( t n )are linearly dependent, then the joint pdf takes on aform similar to Equation (6.45), but contains impulses [Gardner,1990, p. 40].   Aweakly stationaryGaussian process is strongly stationarytoall orders n: all Gaussian joint pdf’s are completely determined by their ﬁrst and second moments by Equation (6.45), and those moments are time independent by weak stationarity,and so all joint pdf’s are also.Everysecond-order strongly stationary stochastic process X ( t )isalso weakly stationarybecause the time translation independence of the joint pdf’s determines the ﬁrst and second moments to havethe same property.However,non-Gaussian weakly stationaryprocesses need not be strongly second-order stationary.   Rather than with pdf’s,aprocess X ( t )may be speciﬁed in terms of conditional pdf’s         f X ð t ð 1 ÞÞ...X ð t ð n ÞÞð x 1 ; ... ; x n Þ¼f X ð t ð n ÞÞjX ð t ð n   1 ÞÞð x n j x n   1 Þ · ... ·fX ð t ð 2 ÞÞjX ð t ð 1 ÞÞð x 2 j x 1 Þ f X ð t ð 1 ÞÞð x 1 Þ   by successiveapplications of Bayes’law,for t 1 < t 2 < ... < t n .The conditional pdf’s satisfy                                   f A j B ð a j b Þ¼f ABð a ; b Þ = f B ð b Þð6       : 47Þ  The conditional factorization property maysatisfy                   f X ð t ð n ÞÞjX ð t ð n   1 ÞÞ...X ð t ð 1 ÞÞð x n j x n   1 ; ... ; x 1 Þ¼f X ð t ð n ÞÞjX ð t ð n   1 ÞÞð x n j x n   1 Þð6 : 48Þ   which indicates that the pdf of the process at anytime t n , given values of the process at anynumber of previous times t n   1 , ... , t 1 ,isthe same as the pdf at t n given the value of the process at the most recent time t n   1 .Such an X ( t )iscalled a ﬁrst-order Markov process,inwhich case we saythe process remembers only the previous value (the previous value has inﬂuence). In general, an nth-order Markov process remembers only the n most recent previous values. Aﬁrst-order Markov process can be fully speciﬁed in terms of its ﬁrst-order conditional pdf’s  f X ( t ) j X ( s ) ( x t ,xs )and its unconditional ﬁrst-order pdf at some initial time t 0 ; f X ð t ð 0 ÞÞð x 0 Þ .  Examples   of Stochastic Processes Figure6.9 shows twosample functions of nonstationaryprocesses.Now consider the discrete time process X ( k ) ¼ A ,for all k $ 0, where A is arv(a random initial condition)that assumes avalue 1or–1with respective probabilities p and 1   p at k ¼ 0. This value does not change, once the initial random drawisdone at k ¼ 0. This stochastic sequence has twosample functions only,the constant sequences {   1} and {1}. The expected value of X ( k )atany time k is E ½ X ð k Þ  ¼ E ½ A  ¼p 1 þð1   p Þð 1 Þ¼2 p   1, which is independent of k .The autocorrelation function is, by deﬁnition, E ½ X ð k Þ X ð k þ m Þ  ¼ E ½ AA ¼E ½ A 2  ¼p 1 2 þð1   p Þð 1 Þ 2 ¼ 1which is also independent of time k .Thus X ( k )isperfectly correlated for all time (the process has inﬁnite memory ). This process is deterministic.   Foranother example, put X ð t Þ¼ð c Þ cosð wt þ F Þ ,where F is the uniform rv on (–p , p ). Then X ( t )isa function of the rv F (as well as t ), so by use of Equation (6.39a), we obtain                            Z p                                                                     p              E ½ X ð t Þ¼c·  cosð wt þ f Þ f F ð f Þ d f ¼ðc = 2 p Þ sinð wt þ f Þj  p ¼ 0                             pInformation Theory                                                                 6 -27                            FIGURE 6.9 Examples of nonstationaryprocesses. Therefore,the mean does not varywithtime t .The autocorrelation is              R XXð t ; t þ t Þ¼E ½ðc Þ cosð wt þ F Þðc Þ cosð wt þ w t þ F Þ                          ¼  c 2 E ½ cosð wt þ F Þ cosð wt þ w t þ F Þ                               Z p                            2                         ¼  c     cosð wt þ f Þ cosð wt þ w t þ F Þ f F ð f Þ d f                                 p                                 Z p                         ¼ðc 2 = 2 Þ f cosð 2 wt þ 2 f þ w t Þþcosð w t Þgð1 = 2 p Þ d f                                     p                         ¼ðc 2 = 4 p Þ · f sinð Y þ 2 p Þþcosð w t Þ · 2 p g                         ¼ðc 2 = 4 p Þ · f cosð w t Þ · 2 p g¼ð c 2 = 2 Þ cosð w t Þ  [using cosð x Þ cosð y Þ¼1 = 2 f cosð x þ y Þþcosð x   y Þg and letting Y ¼ 2 wt þ 2 F þ wt  .Therefore, X ( t )isws. The autocorrelation is periodic in the offset variable t .    Nowconsider the example X ( t ) ¼ A cos(2p f 0 t )for each t ,where f 0 is aconstant frequency,and the amplitude A is arandom initial condition as given above.There are only twosample functions here: (1) x ( t ) ¼  cos(2p f 0 t )and (2) x ( t ) ¼ –cos(2p f 0 t ). Arelated example is X ( t ) ¼ A cos(2 p f 0 t þ F ), where A is given above, the phase F is the uniform random variable on [0, p ], and A and F are independent. Again, F and A do not6 -28                                  Broadcasting and Optical Communication Technology  depend on time (initial random conditions). Thus, the sample functions for X ( t )are x ( t ) ¼ ^ cos(2 p ft þ f ), where F ¼ f is the value assumed initially.There are inﬁnitely manysample functions because of the phase. Equation (6.39b) and the independence of A and F yield                                                         Z p          E ½ X ð t Þ  ¼ E ½ A cosð 2 p ft þ F Þ  ¼ E ½ A   E ½ g ð F Þ  ¼ m A cosð 2 p ft þ f Þð1 = p Þ d f                                                          0                                      p                 ¼ðm A = p Þ sinð 2 p t þ f Þjf ¼ 0 ¼ðm A = p Þ½sinð 2 p t þ p Þ sinð 2 p t Þ                   ¼ðm A = p Þ½sinð 2 p t Þ sinð 2 p t Þ  ¼ð  2 m A = p Þ sinð 2 p t Þ  which is dependent upon time. Thus, X ( t )isnot ws.   Next, let X ð t Þ¼½ a þ S ð t Þ cos½ 2 p ft þ F   ,where the signal S ( t )isanondeterministic stochastic process. This is an amplitude-modulated sine wave carrier.The carrier cos½ 2 p ft þ F   has random initial condition F and is deterministic. Because S ( t )isnondeterministic, X ( t )isalso.The expected value E ½ X ð t Þ  ¼ E ½ a þ S ð t Þ E ½ cosð 2 p ft þ F Þ  can be found as above by independenceof S ( t )and F .    Finally,let X ( t )beuncorrelated ð E ½ X ð t Þ X ð t þ t Þ  ¼ 0for t not zero) such that each rv X ( t ) ¼ X t is Gaussian with zeromean and variance s 2 ( t ) ¼ t, for all t , 0. Anyrealized sample function x ( t )of X ( t ) cannot be predicted in anyaverage sense based on past values (uncorrelated Gaussian random variables are independent). The variancegrows in an unbounded manner over time, so X ( t )isneither stationarynor deterministic. This is called the Wiener process.                                                             2   Auseful model of awsprocess is that for which m ¼ 0and R XXð t Þ¼s X expð a j t jÞ.Ifthis process is also Gaussian, then it is strongly stationaryand all of its joint pdf’s are fully speciﬁed by R XX( t ). In this case it is also aﬁrst-order Markovprocess and is called the Ornstein-Uhlenbeck process [Gardner,1990, p. 102]. Unlike  white noise, manyreal-world ws stochastic processes are correlated ðjR XX ð t ; t þ t Þj4 0 Þ for j t j 4 0. The autocorrelation either goes to zeroas t goes to inﬁnity, or else it has periodic or other nondecaying memory. We consider ws processes henceforth (for nonstationaryprocesses, see Gardner,1990). We will also assume without loss of generality that m ¼ 0.  Linear Filtering of Weakly   Stationary Processes Let the ws stochastic process X ( t )bethe input to alinear time-invariant stable ﬁlter withimpulse response function h ( t ). The output of the ﬁlter is also awsstochastic process and is given by the convolution                                               Z 1                          Y ð t Þ¼h ð t Þ X ð t Þ¼ h ð s Þ X ð t   s Þ d s         ð 6 : 49Þ                                                  1  The mean of the output process is obtained by using the linearityofthe expectation operator [see Gardner, 1990, p. 32]                                   Z 1                 Z 1               m Y ¼ E ½ Y ð t Þ  ¼ E h ð s Þ X ð t   s Þ d s ¼ h ð s Þ E ½ X ð t   s Þ d s                                     1                    1                     Z 1               Z 1                                         ð 6 : 50Þ                   ¼     h ð s Þ m X d s ¼ m X h ð s Þ d s ¼ m X ·Hð 0 Þ                        1                 1              R              1       j2p ft where H ð f Þ¼   1 h ð t Þ e d t is the ﬁlter transfer function and H (0) is the dc gain of the ﬁlter.Information Theory                                                                 6 -29    The autocorrelation of the output process, obtained by using the linearityof E [·], is                                      Z 1             Z 1       R YY ð t Þ¼E ½ Y ð t Þ Y ð t þ t Þ  ¼ E h ð v Þ X ð t   v Þ d v h ð u Þ X ð t þ t   u Þ d u                                     1                  1                Z 1 Z 1              ¼          E ½ X ð t   v Þ X ð t þ t   u Þ h ð v Þ h ð u Þ d v d u                    1   1                Z 1   Z 1                                    ¼           R XXð t   u þ v Þ h ð u Þ d u h ð v Þ d v                    1    1                                                         ð 6 : 51Þ                Z 1   Z 1                                         ¼           R XXð½t  ð  v Þ    u Þ h ð u Þ d u h ð v Þ d v                    1    1                Z 1                                     ¼       R XXð t þ v Þ * h ð t þ v Þ h ð v Þ d v                    1               ¼½R XXð t Þ * h ð t Þ  * h ð t Þ¼R XXð t Þ * ½ h ð t Þ * h ð t Þ  ¼ R XXð t Þ * r h ð t Þ              R 1                                                                2 where r h ð t Þ¼   1 h ð t þ u Þ h ð u Þ d u .However, r h ( t )has Fourier transform H ð f Þ H * ð f Þ¼j H ð f Þj ,because the Fourier transform of the convolution of twofunctions is the product of their Fourier transforms, and the Fourier transform of h (–t )isthe complex conjugate H *(f )ofthe Fourier transform H ( f )of h ( t ). Thus,  the Fourier transform of R YY( t ), denoted by F f R YY ð t Þg,is                                                                                     2     F f R YY ð t Þg ¼ F ½ R XXð t Þ * h ð t Þ * h ð t Þg ¼ F f R XXð t Þg ·Hð f Þ H * ð f Þ¼F f R XXð t Þg · j H ð f Þj  Upon deﬁning the functions                           S XXð f Þ   F f R XXð t Þg; S YY ð f Þ   F f R YY ð t Þg ð 6 : 52Þ  we can also determine R YY( t )via the two steps                                                         2                                 S YY ð f Þ¼S XXð f Þ · j H ð f Þj                 ð 6 : 53Þ                                                 Z 1                                     1                     j2p f t                        R YY ð t Þ¼F f S YY ð f Þg ¼ S YY ð f Þ e d f              ð 6 : 54Þ                                                   1    Equations (6.52) deﬁne the power spectraldensity functions (psdf’s) S XX( f )for X ( t )and S YY( f )for Y ( t ). Thus, R XX( t )and S XX( f )are Fourier transform pairs, as are R YY( t )and S YY( f )(see Equation (6.58)). Further, the psdf S XX( f )of X ( t )isapowerspectrum (in an average sense). If X ( t )isthe voltage dropped across a1- O             2                                                                       2 resistor,then X ( t )isthe instantaneous power dissipation in the resistance.Consequently, R XX(0) ¼ E [ X ( t )] is the expected power dissipation over all frequencies, i.e., by Equation (6.54) with t ¼ 0, we have                                           Z                                  R XXð 0 Þ¼   S XXð f Þ d f                                              1    We want to show that when we pass X ( t )through anarrow bandpass ﬁlter withabandwidth d centered at  the frequency ^ f 0 ,the expected power at the output terminals, divided by the bandwidth d ,is S XX( f 0 )inthe limit as d ! 0. This shows that S XX( f )isadensity function (whose area is the total expected power over all frequencies, just as the area under apdf is the total probability). This result that R XX( t )and S XX( f )are a Fourier transform pair is known as the Wiener-Khinchin relation [see Gardner,1990, p. 230].6 -30                                  Broadcasting and Optical Communication Technology    To verify this relation, let H ( f )bethe transfer function of an ideal bandpass ﬁlter,where                      H ð f Þ¼1 ; j f   f 0 j 5d= 2 ; H ð f Þ¼0 ; otherwise  Let Y(t )bethe output of the ﬁlter.Then Equation (6.54) and Equation (6.53) provide                                     Z 1             Z 1                    2                                                2                 E ½ Y ð t Þ  ¼ R YY ð 0 Þ¼ S YY ð f Þ d f ¼ S XXð f ÞjH ð f Þj d f                                        1               1                           Z f 0 þ d = 2     Z   f 0 þ d = 2                         ¼        S XXð f Þ d f þ    S XXð f Þ d f                             f 0   d = 2         f 0   d = 2  Dividing by 2 d and taking the limit as d ! 0yields (1/2)S XX( f 0 ) þ (1/2)S XX(   f 0 ), which becomes S XX( f 0 ) when we use the fact that psdf’s are even and real functions (because they are the Fourier transforms of autocorrelation functions, which are even and real).    Forexample, let X ( t )bewhite noise, with S XX( f ) ¼ N 0 ,being put throughaﬁrst-order linear time- invariant system withrespectiveimpulse response and transfer functions              h ð t Þ¼expf a t g ; t > 0 ; h ð t Þ¼0 ; t < 0 H ð f Þ¼1 = ½ a þ j2p f   ; all f  The temporal correlation of h ( t )with itself is r h ( t ) ¼ (1/2a )exp{–a j t j }, so the power transfer function is j H ð f Þj2 ¼ 1 = ½ a 2 þð2 p f Þ 2   .The autocorrelation for the input X ( t )is                                       Z 1                                              j2p f t                            R XXð t Þ¼    N 0 e  d f ¼ N 0 d ð t Þ                                         1  which is an impulse. It follows (see Equation (6.22)) that the output Y ( t )has respectiveautocorrelation and psdf                                     a j t j         a j t j            2        2        R YY ð t Þ¼½ N 0 d ð t Þ  * ½ð1 = 2 a Þ e  ¼ð N 0 = 2 a Þ e ; S YY ð f Þ¼N 0 = ½ a þð2 p f Þ    The output expected power E [ Y 2 ( t )] can be found from either one of                                                         Z 1                2                                2            E ½ Y ð t Þ  ¼ R YY ð 0 Þ¼N 0 = 2 a or E ½ Y ð t Þ  ¼ S YY ð f Þ d f ¼ N 0 = 2 a                                                           1    If the input X ( t )toalinear system is Gaussian, then the output will also be Gaussian [see Brown,1983; or Gardner,1990]. Thus, the output of aﬁrst-order linear time-invariant system driven by Gaussian white noise is the Ornstein–Uhlenbeck process, which is also aﬁrst-order Markov process.    Foranother example, let X ð t Þ¼A cosð w o t þ Y Þ ,wherethe random amplitude A has zero mean, the random  phase Y is uniform on [–p , p ], and A and Y are independent. As before, we obtain          2                                                2 R XXð t Þ¼s A cosð w o t Þ ,from which it follows that S XXð f Þ¼ð s A = 2 Þ½d ð f   w o = 2 p Þþd ð f þ w o = 2 p Þ . These impulses in the psdf, called spectrallines,represent positive amounts of poweratdiscrete frequencies.  Cross-Correlation   of Processes The cross-correlation function for two random processes X ( t )and Y ( t )isdeﬁned via                               R XY ð t ; t þ t Þ   E ½ X ð t Þ Y ð t þ t Þ         ð 6 : 55Þ  Let both processes be ws with zero means, so the covariance coincides with the correlation function. We say  that two ws processes X ( t )and Y ( t )are jointly ws whenever R XY ð t ; t þ t Þ¼R XY ð t Þ .Incase Y ( t )isthe output ofInformation Theory                                                                 6 -31   aﬁlter with impulse response h ( t ),we can ﬁnd the cross-correlation R XY( t )between the input and output via                                                   Z 1                 R XY ð t Þ¼E ½ X ð t Þ Y ð t þ t Þ  ¼ E ½ X ð t Þ h ð u Þ X ð t þ t   u Þ d u                                                       1                          Z 1                        ¼     h ð u Þ E ½ X ð t Þ X ð t þ t   u Þ d u              ð 6 : 56Þ                             1                          Z 1                        ¼     h ð u Þ R XXð t   u Þ d u ¼ R XXð t Þ h ð t Þ                             1                                                                       2   Cross-correlation functions of ws processes satisfy (1) R XY ð t Þ¼R YXð t Þ ,(2) j R XY ð t Þj < R XXð 0 Þ R YY ð 0 Þ ,and (3) j R XY ð t Þj < ð 1 = 2 Þ½R XX ð 0 ÞþR YY ð 0 Þ .The ﬁrst follows from the deﬁnition, while the second comes from expanding E ½fY ð t þ t Þ a X ð t Þg2   > 0. The third comesfromthe fact that the geometric mean cannot exceed the arithmetic mean [see Peebles, 1987, p. 154].   Taking the Fourier transform of the leftmost and rightmost sides of Equation (6.56) yields                                    S XY ð f Þ¼S XXð f Þ H ð f Þð6                    : 57Þ  The Fourier transform of the cross-correlation function is the cross-spectraldensity function                                         Z 1                                                      j2p f t                               S XY ð f Þ¼  R XY ð t Þ e d t                       ð 6 : 58Þ                                            1  According to Gardner [1990, p. 228], this is a spectral correlation density function that does not represent power in anysense.   Equation (6.57) suggests amethod for identifying alinear time-invariant system. If the system is subjected to awsinput X ( t )and the power spectral densityof X ( t )and the cross-spectral densityof X ( t )and the output Y ( t )are measured, then the ratio yields the system transfer function                                   H ð f Þ¼S XY ð f Þ = S XXð f Þð6                   : 59Þ  In fact, it can be shown that this method gives the best linear time-invariant model of the (possibly time varying and nonlinear) system in the sense that the time-averaged mean-square error between the outputs of the actual system and of the model, when both are subjected to the same input, is minimized [see Gardner, 1990, pp.282–286].   As an application, suppose that an undersea sonar-based device is to ﬁnd the range to atarget, as shown in Figure6.10, by transmitting asonar signal X ( t )and receiving the reﬂected signal Y ( t ). If n is the velocity of the  sonar signal, and t o is the offset that maximizes the cross-correlation R XY( t ), then the range (distance) d can be determined from d ¼ nto /2 (note that the signal travels twice the range d ).  Coherence  When  X ( t )and Y ( t )havenospectral lines at f, the ﬁnite spectral correlation S XY( f )isactually aspectral covarianceand the two associated variancesare S XX( f )and S YY( f ). We can normalize S XY( f )toobtain a spectral correlation coefﬁcient Y XY( f )deﬁned by                                    2           2                            Y XY ð f Þ ¼jS XY ð f Þj = S XXð f Þ S YY ð f Þð6        : 60Þ  We call Y XY( f )the coherence function.Itisameasureofthe power correlation of X ( t )and Y ( t )ateach frequency f .When Y ( t ) ¼ X ( t ) * h ( t ), it has amaximum: by Equation (6.53), Equation (6.59), and6 -32                                  Broadcasting and Optical Communication Technology                                 FIGURE 6.10 Asonar range ﬁnder.                       2              2                     2 Equation (6.60), j Y XY ð f Þj ¼jS XXð f Þ ·Hð f Þj = ½ S XXð f Þ ·SXXð f Þ · j H ð f Þj  ¼1. In the general case we have                                                         1 = 2                               j S XY ð f Þj < ½ S XXð f Þ S YY ð f Þ              ð 6 : 61Þ  Upon  minimizing the mean-square error e ¼ E ½ðY ð t Þ X ð t Þ h ð t ÞÞ2   over all possible impulse response  functions h ( t ), the optimal one, h o ( t ), has transfer function                                  H o ð f Þ¼S XY ð f Þ = S XXð f Þð6                  : 62Þ  Further,the resultantminimum value is given by                                    Z 1                                                         2                            e min ¼    S YY ð f Þ½1  jY XY ð f Þj   d f                                      1  [see Gardner,1990, pp.434–436; or Bendat and Piersol, 1986]. At frequencies f where j Y XY ð f Þj < 1 ; e min < 0.               2                                                                  2 Thus 1  jY XY ð f Þj is the mean-squareproportion of Y ( t )not accountedfor by X ( t ), while j Y XY ð f Þj is the proportion due to X ( t ). When Y ð t Þ¼X ð t Þ h ð t Þ ; e min ¼ 0   The optimum system H o ( f )ofEquation (6.62) is known as the Wiener ﬁlter for minimum mean-square error estimation of one process Y ( t )using aﬁlteredversion of another process X ( t )[see Gardner,1990; or Peebles, 1987, p. 262].  Ergodicity When the time average                                              Z T = 2                                limT ! 1 ð 1 = T Þ X ð t Þ d t                                                 T = 2  exists and equals the corresponding expected value E [ X ( t )], then the process X ( t )issaid to possess an ergodic property associated with the mean.There are ergodic properties associated with the mean, autocorrelation (and powerspectral density), and all ﬁnite-order joint moments, as well as ﬁnite-order joint pdf’s.Ifaprocess has all possible ergodic properties, it is said to be an ergodic process.Information Theory                                                                 6 -33     Let Y ð t Þ¼g ½ X ð t þ t 1 Þ ; ... ; X ð t þ t n Þ ,where g [·]isany nonrandom real function, so that Y ( t )isa function of aﬁnite number of time samples of astrongly stationaryprocess. Forexample, let (1)  Y ð t Þ¼X ð t þ t 1 Þ X ð t þ t 2 Þ , E ½ Y ð t Þ  ¼ R XXð t 1   t 2 Þ and (2) Y ð t Þ¼1if X ð t Þ < x ; Y ð t Þ¼0, otherwise, so that                                                                    Z 1          E ½ Y ð t Þ  ¼ 1 ·Pð X ð t Þ < x Þþ0 ·Pð X ð t Þ < x Þ¼P ð X ð t Þ < x Þ¼ f X ð t Þ ð z Þ d z                                                                      1    We want to knowunder what conditions the mean-square error between the time average                                               Z T = 2                                hiY ð t Þ T   ð 1 = T Þ Y ð t Þ d t                                                 T = 2  and the expected value E [ Y ( t )] will converge to zero. It can be shown that anecessaryand sufﬁcient condition for the mean-square ergodic property                                                        2                              limT ! 1 E ½fhY ð t Þi   E ½ Y ð t Þ g ¼ 0           ð 6 : 63Þ  to hold is that                                            Z T                              limT ! 1 ð 1 = T Þ C YY ð t Þ d t ¼ 0                ð 6 : 64Þ                                             0    For example, if C YY( t ) ! 0as t ! 1 ,then Equation (6.64) will hold, and thus Equation (6.63) will also, where C YY( t )isthe covariance function of Y ( t ). As long as the two sets of rv’s f X ð t þ t 1 Þ ; ... ; X ð t þ t n Þg and f X ð t þ t 1 þ t Þ ; ... ; X ð t þ t n þ t Þg become independent of each other as t ! 1 ,the above condition holds, so Equation (6.63) holds [see Gardner,1990, pp.163–174].    In practice, if X ( t )exhibits ergodicityassociated with the autocorrelation, then we can estimate R XX( t )using the time average                                               Z T = 2                        h X ð t Þ X ð t þ t ÞiT   ð 1 = T Þ X ð t Þ X ð t þ t Þ dt ð 6 : 65Þ                                                  T = 2                                                                2 In this case the mean-square estimation error E ½fhX ð t Þ X ð t þ t ÞiT   R XXð t Þg   will converge to zeroas T increases to inﬁnity, and the powerspectral density S XX( f )can also be estimated viatime averaging [see Gardner,1990, pp.230–231].  Deﬁning   Terms  Autocorrelation function: Afunction R XXð t ; t þ t Þ¼E ½ X ð t Þ X ð t þ t Þ  that measures the degreetowhich      anytwo rv’s X ( t )and X ( t þ t ), at times t and t þ t ,are correlated. Coherence function:  Afunction of frequency fthat provides the degreeofcorrelation of twostochastic      processes at each fbythe ratio of their cross-spectral density function to the product of their power      spectral density functions. Power spectral density function: The Fourier transform of the autocorrelation function of astochastic       process X ( t ), denoted by S XX( f ). The areaunder its curve between f 1 and f 2 represents the total power      over all t in X ( t )inthe band of frequencies f 1 to f 2 .Its dimension is watts per Hz. Sample function: Areal-valued function x ( t )of t whereateach time t the value x ( t )atthe argument t was       determined by the outcome of arv X t ¼ x ð t Þ . Stochastic process: Acollection of rv’s f X t : t 2 T g ,where T is an ordered set such as the real numbers or      integers [ X ( t )isalso called arandom function, on the domain T ].6 -34                                  Broadcasting and Optical Communication Technology   Time  average: Anyrealfunction g ( t )oftime has average value g ave on the interval [ a,b]such that the      rectangular area g ave ( b–a )isequal to the areaunder the curve between a and b, i.e.,                    R b      g ave ¼½1 = ð b   a Þ  a g ð t Þ d t .The time average of asample function x ( t )isthe limit of its average      value over [0,T ]as T goes to inﬁnity. Weakly  stationary: The property of astochastic process X ( t )whose mean E [ X ( t )] ¼ m ( t )isaﬁxed       constant m over all time t, and whose autocorrelation is also independent of time in that R XXð t ; t þ t Þ¼      R XXð s þ t ; s þ t þ t Þ for any s. Thus, R XXð t ; t þ t Þ¼R XXð 0 ; t Þ¼R XXð t Þ .  Acknowledgment The author is grateful to William Gardner of the UniversityofCalifornia, Davis for making substantial suggestions.  References J.S. Bendat and A.G. Piersol, Random Data: Analysis and Measurement, 2nd ed., NewYork: Wiley-Inter-      science,1986. R.G. Brown, Introduction to Random Signal Analysis and Kalman Filtering, NewYork: Wiley,1983. W.A. Gardner, Introduction to Random Processes, 2nd ed., NewYork: McGraw-Hill, 1990. P.Z. Peebles, Jr., Probability,Random Variables, and Random Signal Principles, 2nd ed., NewYork:      McGraw-Hill, 1987.  Further  Information The IEEE Individual Learning Package, Random Signal Analysis with Random Processes and Kalman Filtering, preparedfor the IEEE in 1989 by Carl G. Looney,IEEE Educational Activities Board, PO Box1331, Piscataway, NJ 08855–1331.   R. Iranpour and P. Chacon, Basic Stochastic Processes: The Mark KacLectures, NewYork: Macmillan, 1988.   A. Papoulis, Probability,Random Variables, and Stochastic Processes, 3rd ed., NewYork: Macmillan, 1991.  6.4     The   Sampling    Theorem Robert   J. Marks  II  Most signals originating from physical phenomena are analog.Most computational engines, on the other hand, are digital. Transforming from analog to digital is straightforward: we simply sample. Regaining the original signal from these samples and assessing the information lost in the sampling process are the fundamental questions addressed by the sampling theorem.   The fundamental result of the sampling theorem is, remarkably,that abandlimited signal is uniquely speciﬁed by its sufﬁciently close equally spaced samples. Indeed, the sampling theorem illustrates howthe original signal can be regained from knowledge of the samples and the sampling rate at which they weretaken.   Popularization of the sampling theorem is credited to Shannon [1948] who,in1948, used it to showthe equivalenceofthe information content of abandlimited signal and asequence of discrete numbers. Shannon was aware of the pioneering work of Whittaker [1915] and Whittaker’sson [1929] in formulating the sampling theorem. Kotel’nikov’s[1933] independent discoveryinthe then Soviet Union deservesmention. Higgins [1985] credits Borel [1897] with ﬁrst recognizing that asignal could be recovered from its samples.   Surveysofsampling theoryare in the widely cited paper of Jerri [1977] and in twobooks by the author [1991, 1993]. Marvasti [1987] has written abook devotedtononuniform sampling.  The  Cardinal  Series If asignal has ﬁnite energy, the minimum sampling rate is equal to twosamples per period of the highest frequency component of the signal. Speciﬁcally,ifthe highest frequency component of the signal is B Hz, thenInformation Theory                                                                 6 -35  the signal, x ( t ), can be recovered from the samples by                                                                               1 X 1    n  sin½ p ð 2 Bt   n Þ                             x ð t Þ¼     x                                         ð 6 : 66Þ                                   p n ¼ 1 2 B     2 Bt   n  The frequency B is also referred to as the signal’sbandwidth and, if B is ﬁnite, x ( t )issaid to be bandlimited. The signal, x ( t ), is herebeing sampled at arate of 2 B samples per second. If sampling weredone at alower rate, the replications would overlap and the information about X ( o )[and thus x ( t )] is irretrievably lost. Undersampling results in aliased data. The minimum sampling rate at which aliasing does not occur is referred to as the Nyquist rate which, in our example, is 2 B .Equation (6.66) was dubbed the cardinal series by the junior Whittaker [1929].   Asignal is bandlimited in the low-pass sense if there is a   B . 0such that                                                   o                                  X ð o Þ¼X ð o Þ P                                ð 6 : 67Þ                                                  4 p B  wherethe gate function P ( j )isone for j # 1/2 and is otherwise zero,and                                          Z 1                                  X ð o Þ¼    x ð t Þ e   j o t d t                ð 6 : 68Þ                                             1  is the Fourier transform of x ( t ). That is, the spectrum is identically zero for j o j 4 2 p B .The B parameter is referred to as the signal’sbandwidth. The inverse Fourier transform is                                        1 Z 1                                 x ð t Þ¼      x ð t Þ e j o t d o                 ð 6 : 69Þ                                        2 p   1    The sampling theorem reduces the normally continuum inﬁnityofordered pairs required to specify afunction to acountable—althoughstill inﬁnite—set. Remarkably,these elements are obtained directly by sampling.   Howcan the cardinal series interpolate uniquely the bandlimited signal from which the samples were taken? Could not the same samples be generated from another bandlimited signal? The answer is no. Band-limited functions are smooth. Anybehavior deviating from smooth would result in high- frequency components which in turn invalidates the required property of being bandlimited. The smoothness of the signal between samples precludes arbitraryvariation of the signal there.   Let’s examine the cardinal series moreclosely. FIGURE 6.11 Illustration of the interpolation that results Evaluate Equation (6.74) at t ¼ m /2B .Since sinc(n )is from the Cardinal series. Asinc function, weighted by the one for n ¼ 0and is otherwise zero, only the sample sample, is placed at each sample bottom. The sum of the                                              sincs exactly generates the original bandlimited function at t ¼ m /2B contributes to the interpolation at that                                              from which the samples were taken. point. This is illustrated in Figure 6.11, wherethe reconstruction of asignal fromits samples using the cardinal series is shown. The value of x ( t )atapoint other than asample location [e.g., t ¼ ( m þ 1/2)/2B ] is determined by all of the sample values.  Proof of the Sampling Theorem Borel [1897] and Shannon [1948] both discussed the sampling theorem as the Fourier transform dual of the Fourier series. Let x ( t )haveabandwidth of B .Consider the periodic signal6 -36                                  Broadcasting and Optical Communication Technology                                          X 1                                Y ð o Þ¼     X ð o   4 p nBÞð6                       : 70Þ                                        n ¼ 1  The function Y ( o )isaperiodic function with period 4 p B .From Equation (6.67) X ( o )iszerofor o . 2 p B and is thus ﬁnite in extent. The terms in Equation (6.70) therefore do not overlap.Periodic functions can be expressed as aFourier series.                                                               X 1         j n o                                Y ð o Þ¼     a n exp                               ð 6 : 71Þ                                        n ¼ 1       2 B  wherethe Fourier series coefﬁcients are                                                 1  Z 2 p B        j n o                             a n ¼          Y ð o Þ exp  d o                                  4 p B   2 p B      2 B  or                                                                                         1    n                                      a ¼     x                                    ð 6 : 72Þ                                       n   2 B  2 B  wherewehaveused  the inverse Fourier transform in Equation (6.69). Substituting into the Fourier series in Equation (6.71) gives                                                                                            1  X 1    n        j n o                             Y ð o Þ¼       x     exp                              ð 6 : 73Þ                                    2 B n ¼ 1 2 B       2 B  Sinceaperiod of Y ( o )is X ( o ), we can get back the original spectrum by                                                                                                      o                                  X ð o Þ¼Y ð o Þ P                                                  4 p B  Substitute Equation (6.73) and inverse transforming gives, using Equation (6.69),                                                                                      1 Z 2 p B X 1   n        jno                       x ð t Þ¼             x     exp       e j o t d o                             4 p B   2 p B n ¼ 1 2 B   2 B  or                                                                                 X 1    n                              x ð t Þ¼   x     sin c ð 2 Bt   n Þð6                  : 74Þ                                    n ¼ 1  2 B  where                                               sinp t                                      sin c ð t Þ¼                                                p t  is the inverse Fourier transform of P ( o /2p ). Equation (6.74) is, of course, the cardinal series.   The sampling theorem generally converges uniformly,inthe sense that                                                   2                                  lim j x ð t Þ x N ð t Þj ¼ 0                                  N ! 1Information Theory                                                                 6 -37  wherethe truncated cardinal series is                                                                                   X N    n                             x  ð t Þ¼    x     sin c ð 2 Bt   n Þð6                 : 75Þ                              N             2 B                                     n ¼ N  Sufﬁcient conditions for uniform convergence are [Marks, 1991]    1. the signal, x ( t ), has ﬁnite energy, E ,                                       Z 1                                  E ¼      j x ð t Þj2 d t < 1                                          1     2. or X ( o )has ﬁnite area,                                       Z 1                                 A ¼      j X ð o Þjd o < 1                                         1    Care must be takeninthe second case, though, when singularities exist at o ¼ ^ 2 p B .Here, sampling may be required to be strictly greater than 2 B .Such is the case, for example, for the signal, x ( t ) ¼ sin(2p Bt). Although the signal is bandlimited, and althoughits Fourier transform has ﬁnite area, all of the samples of x ( t ) takenat t ¼ n /2B are zero. The cardinal series in Equation (6.74) will thus interpolate to zero everywhere. If the sampling rate is abit greater than 2 B ,however,the samples are not zeroand the cardinal series willuniformly converge to the proper answer.  The Time-Bandwidth Product The cardinal series requires knowledge of an inﬁnite number of samples. In practice, only aﬁnite number of samples can be used. If most of the energyofasignal exists in the interval 0 # t # T ,and we sample at the Nyquist rate of 2 B samples per second, then atotal of S ¼ k 2 BTl samples are taken. ( k y l denotes the largest number not exceeding y .) The number S is ameasureofthe degrees of freedom of the signal and is referred to as its time-bandwidth product.A5-min single-track audio recording requiring ﬁdelityupto20,000 Hz, for example, requires aminimum of S ¼ 2 · 20,000 · 5 · 60 ¼ 12 million samples. In practice, audio sampling is performed well above the Nyquist rate.  Sources of Error Exact interpolation using the cardinal series assumes that (1) the values of the samples are known exactly,(2) the sample locations are known exactly,and (3) an inﬁnite number of terms are used in the series. Deviation from these requirements results in interpolation error due to (1) data noise, (2) jitter,and (3) truncation, respectively. The effect of data error on the restoration can be signiﬁcant. Some innocently appearing sampling theorem generalizations, when subjected to performanceanalysis in the presence of data error,are revealed as ill-posed. In other words, abounded error on the data can result in unbounded error on the restoration [Marks, 1991].  Data Noise The source of data noise can be the signal from which samples are taken, or from round-offerror due to ﬁnite sampling precision. If the noise is additive and random, instead of the samples                                                                                          n                                          x                                            2 B6 -38                                  Broadcasting and Optical Communication Technology  we must deal with the samples                                                                                         n        n                                     x      þ x                                        2 B     2 B  where j ( t )isastochastic process. If these noisy samples are used in the cardinal series, the interpolation, instead of simple x ( t ), is                                         x ð t ÞþZ ð t Þ  wherethe interpolation noise is                                                                                 X 1    n                              Z ð t Þ¼   x     sin c ð 2 Bt   n Þ                                    n ¼ 1  2 B  If j ( t )isazeromean process, then so is the interpolation noise. Thus, the noisy interpolation is an unbiased version of x ( t ). More remarkably,if j ( t )isazero-mean (wide-sense) stationaryprocess with uncertainty (variance) s 2 ,then so is Z ( t ). In other words, the uncertainty at the sample point locations is the same as at all points of interpolation [Marks, 1991]. Truncation The truncated cardinal series is in Equation (6.75). Asignal cannot be both bandlimited and of ﬁnite duration. Indeed, abandlimited function cannot be identically zero over anyﬁnite interval. Thus, other than the rare case where an inﬁnite number of the signal’szerocrossings coincide with the sample locations, truncation will result in an error.   The magnitude of this truncation error can be estimated through the use of Parseval’stheorem for the cardinal series that states                                                                                           Z 1              X 1         2                                          2     1        n                               E ¼      j x ð t Þj d t ¼   x                                        1           2 B   1 2 B  The energyofasignal can thus be determined directly from either the signals or the samples. The energy associated with the truncated signal is                                                                                              X N         2                                         1        n                                      E N ¼        x                                              2 B   N 2 B  If E   E N << E ,then the truncation error is small. Jitter Jitter occurs when samples are taken near to but not exactly at the desiredsample locations. Instead of the samples x ( n /2W ), we havethe samples                                                                                    n                                       x       s                                         2 W    n  where s n is the jitter offset of the n th sample. For jitter,the s n ’s are not known.Ifthey were, an appropriate nonuniform sampling theorem [Marks, 1993; Marvasti, 1987] could be used to interpolate the signal.   Using the jittered samples in the cardinal series results in an interpolation that is not an unbiased estimate of x ( t ). Indeed, if the probabilitydensity function of the jitter is the same at all sample locations, the expectedInformation Theory                                                                 6 -39  value of the jittered interpolation is the convolution of x ( t )with the probabilitydensityfunction of the jitter. This bias can be removed by inverse ﬁltering at acostofdecreasing the signal-to-noise ratio of the interpolation [Marks, 1993].  Generalizations of the Sampling Theorem There exist numerous generalizations of the sampling theorem [Marks, 1991; Marks, 1993].    1. Stochastic processes. Awide-sense stationarystochastic process, w ( t ), is said to be bandlimited if its       autocorrelation, R w ( t ), is abandlimited function. The cardinal series                                                                                 X 1    n                              ww^ ð t Þ¼ w     sin c ð 2 Bt   n Þ                                    n ¼ 1  2 B       converges to w ( t )inthe sense that                                    E ½jww^ ð t Þ w ð t Þj2  ¼0       where E denotes expectation.     2. Nonuniformsampling. There exist numerousscenarios wherein interpolation can be performed from      samples that are not spaced uniformly.Marvasti [1987] devotesabook to the topic.    3. Kramer’sgeneralization. Kramergeneralized the sampling theorem to integral transforms other than      Fourier,for example, to Legendreand Laguerre transforms.    4. Papoulis’generalization. Shannon noted that abandlimited signal could be restored when      sampling was performed at half the Nyquist rate if, at everysample location, asample of the      signal’sderivative were also taken. Recurrent nonuniform sampling is where P samples are spaced      the same in every P Nyquist intervals. Another sampling scenario is when asignal and its Hilbert      transform are both sampled at half their respectiveNyquist rates. Restoration of the signal from      these and numerousother sampling scenarios are subsumed in an eloquent generalization of the      sampling theorem by Papoulis.    5. Lagrangian interpolation. Lagrangian interpolation is atopic familiar in numerical analysis. An N th      order polynomial is ﬁt to N þ 1arbitrarily spaced sample points. If an inﬁnite number of samples are      equally spaced, then Lagrangian interpolation is equivalent to the cardinal series.    6. Trigonometric polynomials. All periodic bandlimited signals can be expressed as trigonometric      polynomials (i.e., aFourier series with aﬁnite number of terms). If the series has M terms, then      the signal has M degrees of freedom which can be determined from M samples taken within a      single period.    7. Multidimensional sampling theorems. Multidimensional signals, such as images, require      dimensional extensions of the sampling theorem. The sampling of the signal nowrequires      geometrical interpretation. Uniform sampling of an image, for example, can either be done on a      rectangular or hexagonal grid. The minimum sampling densityfor one geometrymay differ from      that of another.The smallest sampling densitythat does not result in aliasing can be achieved, in      manycases, with anumber of different uniform sampling geometries and is referred to as the      Nyquist density. Interestingly,sampling can sometimes be performed below the Nyquist density      with nonuniform sampling geometries such that the multidimensional signal can be restored. Such      is not the case for one dimension.    8. Continuous sampling. When asignal is known on one or more disjoint intervals, it is said to havebeen      continuously sampled. Divide the time line into intervals of T .Periodic continuous sampling assumes      that the signal is known on each interval over an interval of a T where a is the dutycycle. Continuously      sampled signals can be accurately interpolated even in the presence of aliasing.Other continuously6 -40                                  Broadcasting and Optical Communication Technology       sampled cases, each of which can be considered as alimiting case of continuously periodically sampled      restoration, include      a. Interpolation. The tails of asignal are known and we wishtorestorethe middle.      b. Extrapolation. We wish to generate the tails of afunction with knowledge of the middle.      c. Prediction. Asignal for t . 0istobeestimated from knowledge of the signal for t , 0.  Final Remarks Sinceits popularization in the late 1940s, the sampling theorem has been studied in depth. More than 1000 papers havebeen generated on the topic [Marks, 1993]. Its understanding is fundamental in matching the largely continuous world to digital computation engines.  Deﬁning   Terms Aliasing: Aphenomenon  that occurs when asignal is undersampled. High-frequency information about      the signal is lost. Cardinal series: The formula by which samples of abandlimited signal are interpolated to form a      continuous time signal. Fourier transform: The mathematical operation that converts atime-domain signal into the frequency      domain. Jitter: Asample is temporally displaced by an unknown, usually small, interval. Kramer’s generalization: Asampling theorybased on other than Fourier transforms and frequency. Lagrangian interpolation: Aclassic interpolation procedureused in numerical analysis. The sampling      theorem is aspecial case. Nyquist rate: The minimum sampling rate that does not result in aliasing. Papoulis’ generalization: Asampling theoryapplicable to manycases wherein signal samples are obtained      either nonuniformly and/or indirectly. Sampling rate: The number of samples per second. Sampling theorem:  Samples of abandlimited signal, if takenclose enoughtogether,exactly specify the      continuous time signal from which the samples weretaken. Signal bandwidth: The maximum  frequency component of asignal. Time bandwidth  product: The product of asignal’sduration and bandwidth approximates the number of      samples required to characterize the signal. Truncation error: The errorthat occurs when aﬁnite number of samples are used to interpolate a      continuous time signal.  References E. Borel, ‘‘Sur l’interpolation,’’ C.R. Acad. Sci. Paris, vol. 124, pp.673–676, 1897. J.R. Higgins, ‘‘Five shortstories about the cardinal series,’’ Bull. Am.Math. Soc., vol. 12, pp.45–89, 1985. A.J.Jerri, ‘‘The Shannon sampling theorem—its various extension and applications: atutorial review,’’ Proc.      IEEE, vol.65, pp.1565–1596, 1977. V.A. Kotel’nikov, ‘‘On the transmission capacity of ‘ether’ and wire in electrocommunications,’’ Izd. Red. Upr.      Svyazi RKKA (Moscow), 1933. R.J.Marks II, Introduction to Shannon Sampling and Interpolation Theory, NewYork: Springer-Verlag,1991. R.J.MarksII, Ed., Advanced Topics in Shannon Sampling and Interpolation Theory ,New York: Springer-Verlag,      1993. F.A. Marvasti, AUniﬁed Approach to Zero-Crossing and NonuniformSampling,Oak Park, Ill.: Nonuniform, 1987. C. Shannon, ‘‘Amathematical theoryofcommunication,’’ Bell System Technical Journal,vol. 27, pp.379, 623,      1948. E.T.Whittaker,‘‘On the functions which are represented by the expansions of the interpolation theory,’’ Proc.      Royal Society of Edinburgh ,vol. 35, pp.181–194, 1915.Information Theory                                                                 6 -41  J.M. Whittaker,‘‘The Fourier theoryofthe cardinal functions,’’ Proc. Math. Soc. Edinburgh ,vol.1,pp. 169–176,      1929. A.I. Zayed, Advances in Shannon’s Sampling Theory, Boca Raton, Fla.: CRCPress, 1993.  Further Information An in-depth study of the sample theorem and its numerousvariations is provided in R. J. Marks II, Ed., Introduction to ShannonSampling and Interpolation Theory ,New York:Springer-Verlag,1991.   In-depth studies of modern sampling theorywithover1000 references are available in R. J. Marks II, Ed., Advanced Topics in Shannon Sampling and Interpolation Theory ,New York: Springer-Verlag,1993.   The speciﬁc case of nonuniform sampling is treated in the monograph by F. A. Marvasti, AUniﬁed Approach to Zero-Crossing and NonuniformSampling,Oak Park, Ill:Nonuniform, 1987.   The sampling theorem is treated generically in the IEEE Transactions on Signal Processing.For applications, topical journals are the best sourceofcurrent literature.  6.5     Channel Capacity Sergio  Verdu ´ Information Rates Tens of millions of users access the Internet daily viastandard telephone lines. Modems operating at data rates of up to 28,800 bits per second enable the transmission of text, audio,color images, and even low-resolution video.The progression in modern technologyfor the standard telephone channel shown in Figure6.12 exhibits, if not the exponential increases ubiquitous in computer engineering,then asteady lope of abut 825 bits per second per year.   Fewtechnological advances can result in as manytime-savings for worldwide daily life as advances in modem  information rates. However,modem designers are faced with afundamental limitation in the maximum  transmissible information rate. Everycommunication channel has anumber associated with it called channel capacity,which determines the maximum information rate that can ﬂowthroughthe channel regardless of the complexityofthe transmitting and receiving devices. Thus, the progression of modem rates shown in Figure6.12 is sure to come to ahalt. But, at what rate? Answering this question for any communication channel model is one of the major goals of information theory—adiscipline founded in 1948 by Claude E. Shannon [Shannon, 1948].  Communication Channels The communication channel is the set of devices and systems that connects the transmitter to the receiver.The transmitter and receiver consist of an encoder and decoder,respectively,which translate the information stream produced by the source into asignal suitable for channel transmission and vice versa (Figure 6.13). For example, in the case of the telephone line, twocommunication channels (one in each direction) share the same physical channel that connects the twomodems. That physical channel usually consists of twisted copper wires at both ends and avarietyofswitching and signal processing operations that occur at the telephone exchanges. The modems themselvesare not included in the communication channel. Amicrowave radio link is another example of acommunication channel that consists of an ampliﬁer and an antenna (at both ends) and acertain portion of the radio spectrum. In this case, the communication channel model does not fully correspond with the physical channel. Whynot, for example, view the antenna as partofthe transmitter rather than the channel (Figure 6.13)? Because considerations other than the optimization of the efﬁciency of the link are likely to dictate the choice of antenna size. This illustrates that the boundaries encoder–channel and channel– decoder in Figure6.13 are not always uniquely deﬁned. This suggests an alternative deﬁnition of achannel as that partofthe communication system that the designer is unable or unwilling to change.6 -42                                  Broadcasting and Optical Communication Technology                     FIGURE 6.12 Information rates of modems for telephone channels.                           FIGURE 6.13 Elements of acommunication system.    Achannel is characterized by the probabilitydistributions of the output signals given everypossible input signal. Channels are divided into (1) discrete-time channels and (2) continuous-time channels depending on whether the input/output signals are sequencesorfunctions of arealvariable. Some examples are as follows. Example  6.1. BinarySymmetric  Channel Adiscrete-time memoryless channel with binaryinputs and outputs (Figure 6.14) wherethe probabilities that 0and 1are received erroneously are equal. Example  6.2. Z-Channel Adiscrete-time memoryless channel with binaryinputs and outputs (Figure 6.15) where0is received error-free. Example  6.3. Erasure Channel Adiscrete-time memoryless channel with binaryinputs and ternaryoutputs (Figure 6.16). The symbols 0and 1cannot be mistakenfor each other but they can be ‘‘erased’’. Example  6.4. White Gaussian Discrete-Time Channel Adiscrete-time channel whose output sequence is given by                                        y i ¼ x i þ n i                             ð 6 : 76Þ  where x i is the input sequence and n i is asequence of independent Gaussian random variables with equal variance.Information Theory                                                                 6 -43         FIGURE 6.14 Binarysymmetric channel.              FIGURE 6.15 Z-channel.                                   FIGURE 6.16 Erasure channel.  Example 6.5.  Linear Continuous-Time Gaussian Channel Acontinuous-time channel whose output signal is given by (Figure 6.17)                                   y ð t Þ¼h ð t Þ * x ð t Þþn ð t Þð6                : 77Þ  where x ( t )isthe input signal, n ( t )isastationaryGaussian process, and h ( t )isthe impulse response of alinear time-invariant system. The telephone channel is typically modeled by Equation (6.77).   The goal of the encoder Figure6.13 is to convert strings of binarydata (messages) into channel-input signals. Source strings of m bits are translated into channel input strings of n symbols (with m # n )for discrete                          FIGURE 6.17 Linear continuous-time Gaussian channel.6 -44                                  Broadcasting and Optical Communication Technology  channels, and into continuous-time signals of duration T for continuous-time channels. The channel code(or moreprecisely the codebook) is the list of 2 m codewords (channel input signals) that maybesent by the encoder.The rate of the codeisequal to the logarithm of its size divided by the duration of the codewords. Thus, the rate is equal to                                 m                                      bits per channel use                                  n  for adiscrete-time channel, whereas it is equal to                                    m                                        bits per second                                    T  for acontinuous-time channel.   Onceacodewordhas    been chosen by the encoder,the channel probabilistic mechanisms govern the distortion suffered by the transmitted signal. The roleofthe decoder is to recover the transmitted binarystring (message) upon reception of the channel-distorted version of the transmitted codeword. To that end, the decoder knows the codebook used by the encoder.For most channels (including those above) thereisanonzeroprobabilitythat the best decoder(maximum likelihood decoder) selects the wrong message. Thus, for agiven channel the two ﬁgures of merit and of interest are the rate and the probabilityoferror.The higher the tolerated probabilityoferror,the higher the allowed rate; however,computing the exact tradeoffisaformidable task unless the codesize either is very small or tends to inﬁnity. The latter case was the one considered by Shannon and treated in the following section.   Reliable Information   Transmission:   Shannon’sTheorem Shannon [1948] considered the situation in which the codewordduration grows without bound. Channel capacityisthe maximum rate for which encoders and decoders exist whose probabilityoferror vanishes as the codewords get longer and longer. Shannon’s Theorem [Shannon, 1948] The capacityofadiscrete memoryless channel is equal to                                      C ¼ max  I ð X ; Y Þð6                          : 78Þ                                           X  where I ( X ; Y )stands for the input-output mutual information, which is ameasureofthe dependenceofthe input and the output deﬁned as the divergencebetween the joint input/output distribution and the product of  its marginals, D ( P XYk P X P Y ). Forany pair of probabilitymass functions P and Q deﬁned on the same space, divergence is an asymmetric measureoftheir similarity:                                           X          P ð x Þ                                D ð P jjQ Þ¼ P ð x Þ log :                         ð 6 : 79Þ                                                    Q ð x Þ                                          x 2 A  Divergence is zeroifboth distributions are equal; otherwise it is strictly positive.The maximization in Equation (6.78) is over the set of input distributions. Although, in general, thereisnoclosed-form solution for that optimization problem, an efﬁcient algorithm was obtained by Blahut and Arimoto in 1972 [Blahut, 1987]. The distribution that attains the maximum in Equation (6.78) determines the statistical behavior of optimal codes and, thus, is of interest to the designer of the encoder.For the discrete memoryless channels mentioned above,the capacityisgiven by the following examples.Information Theory                                                                 6 -45            FIGURE 6.18 Capacityofthe binarysymmetric channel as afunction of crossover probability.                               FIGURE 6.19 Capacityofthe Z-channel.  Example 6.6.  BinarySymmetric Channel                                          1              1                             C  ¼ 1   d log  ð1   d Þ log                                          d            1   d  attained by an equiprobable distribution and shown in Figure 6.18.  Example 6.7.  Z-Channel                                 C ¼ logð 1   d 1 = 1   d þ d d = 1   d Þ  attained for adistribution whose probabilitymass at 0ranges from 1/2 ( d ¼ 0) to 1/e ( d ! 1) (Figure 6.19).  Example 6.8.  Erasure Channel                                         C ¼  1   d  attained for equiprobable inputs.   Oftentimes the designer is satisﬁed with not exceeding acertain ﬁxed level of bit error rate, E ,rather than the morestringent criterion of vanishing probability of selecting the wrong block of data. In such acase, it is6 -46                                  Broadcasting and Optical Communication Technology                    FIGURE 6.20 Capacityexpansion factor as afunction of bit-error-rate.  possible to transmit information at arate equal to capacitytimes                                                                       1             1     1                               1   e log  ð1   e Þ log                                       e            1   e  which is shown in Figure6.20.   If, contrarytowhat we haveassumed thus far,the message sourceinFigure6.13 is not asourceofpure bits, the signiﬁcanceofcapacity can be extended to show that as long as the sourceentropy (see Chapter 6.6 on Data Compression) is below the channel capacity, an encoder/decoder pair exists that enables arbitrarily reliable communication. Conversely,ifthe sourceentropy is above capacity, then no such encoder/decoder pair exists.  Bandwidth   and  Capacity The foregoing formulas for discrete channels do not lead to the capacity of continuous-time channels such as Example 5. We haveseen that in the case of the telephone channel whose bandwidth is approximately equal to 3kHz, capacity is lowerbounded by 28,800 bits per second. Howdoes bandwidth translate into capacity? The answer depends on the noise level and distribution. Forexample, if in the channel of Example 5, the noise is absent, capacityisinﬁnite regardless of bandwidth. We can encode anyamount of information as the binary expansion of asingle scalar,which can be sent over the channel as the amplitude or phase of asingle sinusoid; knowing the channel transfer function, the decoder can recover the transmitted scalar error-free. Clearly,such atransmission method is not recommended in practicebecause it hinges on the non-physical scenario of noiseless transmission.   In the simplest special case of Example 5, the noise is white, the channel has an ideal ﬂat transfer function with bandwidth B (in Hz), and the input power is limited. Then, the channel capacityisequal to                                      dB      0 : 1                          C  ¼ B SNR   log2 10   bits per second                   ð 6 : 80Þ             0.1            dB wherelog2 10 ¼ 0.33 and SNR is equal to the optimum signal-to-noise ratio (in dB) of alinear estimate of aﬂat input signal given the channel output signal. Such an optimum signal-to-noise ratio is equal to one plus the power alloted to the input divided by the noise power in the channel band, i.e.,                                                                                      dB                P                               SNR    ¼  10 log10 1 þ                                                     BN0Information Theory                                                                 6 -47  It is interesting to notice that as the bandwidth grows, the channel capacitydoes not grow without bound. It tends to                                   P                                    log2 ebits per second                                 N 0  wherelog2 e ¼ 1.44. This means that the energyper bit necessaryfor reliable communication is equal to 0.69 times the noise powerspectral density level.When the channel bandwidth is ﬁnite, the energynecessaryto send one bit of information is strictly larger.The energyrequired to send one bit of information reliably can be computed for other (non-Gaussian) channels even in cases whereexpressions for channel capacityare not known [Verdu´ ,1990].   When the channel transfer function H ( f )and/or noise spectral density N ( f )are not ﬂat, the constant in Equation (6.80) no longer applies. The so-called water-ﬁlling formula [Shannon, 1949] gives the channel capacity as                                                                      1 Z        maxf 0 ; w   M ð f Þg                           C ¼     log 1 þ                  d f                               2                M ð f Þ  where w is chosen so that                                Z                                  maxf 0 ; w   M ð f Þgd f ¼ P  and                                               N ð f Þ                                     M ð f Þ¼                                             j H ð f Þj2    The linear Gaussian-noise channel is awidely used model for spacecommunication (in the power limited region) and for the telephone channel (in the bandwidth limited region). Thanks to the prevalence of digital switching and digital transmission in modern telephone systems, not only signal-to- noise ratios haveimprovedovertime but the Gaussian-noise model in Example 5becomes increasingly questionable because quantization is responsible for amajor component of the channel distortion. Therefore,future improvements in modem speeds are expected to arise mainly fromﬁner modeling of the channel.   Due to the effect of time-varying received power(fading), several important channels fall outside the scope of Example 5such as high-frequency radio links, tropospheric scatter links, and mobile radio channels.  Channel Coding Theorems In information theory, the results that giveaformula for channel capacityinterms of the probabilistic description of the channel are known as channel coding theorems. They typically involve two parts: an achievability part, which showsthat codes with vanishing error probabilityexist for anyrate below capacity;and aconverse part, which shows that if the code rate exceeds capacity, then the probability of error is necessarily bounded away from zero. Shannon gavethe ﬁrst achievabilityresults in [Shannon, 1948] for discrete memorychannels. His method of proof, later formalized as the method of ‘‘typical sequences’’(e.g., [Cover and Thomas, 1991]), is based on showing that the average probabilityoferror of acode chosen at random  vanishes with blocklength. Other known achievabilityproofs such as6 -48                                  Broadcasting and Optical Communication Technology  Feinstein’s [1954], Gallager’s[1968], and the method of types [Csiszar and Korner,1981] are similarly non-constructive. The discipline of coding theorydeals with constructive methods to design codes that approach the Shannon limit. The ﬁrst converse channel coding theorem was not given by Shannon, but by Fano in 1952. Adecade after Shannon’s pioneering paper,several authors obtained the ﬁrst channel coding theorems for channels withmemory[Dobrushin, 1963]. The most general formula for channel capacityknown to date can be found in [Verdu´ and Han, 1994]. The capacity of channels with feedback was ﬁrst considered by Shannon in 1961 [Shannon, 1961], with later developments for Gaussian channels summarized in [Coverand Thomas, 1991]. In his 1961 paper [Shannon, 1961], Shannon founded the discipline of multiuser information theorybyposing several challenging channels with morethan  one transmitter and/or receiver.Incontrast to the multiaccess channel (one receiver) which has been solved in considerable generality, the capacities of channels involving more than one receiver, such as broadcast channels [Cover, 1972] and interference channels remainunsolved except in special cases.   Channel capacityhas been shown to haveameaning outside the domain of information transmission [Han and Verdu ´ ,1993]: it is the minimum rate of random bits required to generate anyinput random process so that the output process is simulated with arbitraryaccuracy.  Deﬁning   Terms Blocklength: The duration of acodeword, usually in the context of discrete-time channels. Channel capacity: The maximum  rate for which encoders and decoders exist whose probabilityoferror      vanishes as the codewords get longer and longer. Codeword:   Channel-input signal chosen by the encoder to represent the message. Communication   channel:  Set of devices and systems that connect the transmitter to the receiver,not      subject to optimization. Broadcast  channel: Acommunication    channel with  one  input and  several outputs each      connectedtoadifferent receiver such that possibly different messages are to be conveyed to      each receiver. Discrete memoryless  channel: Adiscrete-time memoryless channel whereeach channel input and      output takesaﬁnite number of values. Discrete-time channel: Acommunication channel whose input/output signals are sequences of values. Its      capacityisgiven in terms of bits per ‘‘channel use’’. Continuous-time channel:  Acommunication channel whose input/output signals are functions of areal      variable (time). Itscapacityisgiven in terms of bits per second. Interference channel: Achannel with several inputs/outputs such that autonomous transmitters are con-      nected to each input and such that each receiver is interested in decoding the message sent by one and      only one transmitter. Memoryless  channel: Achannel wherethe conditional probabilityofthe output given the current input is      independent of all other inputs or outputs. Multiaccess channel: Achannel with several inputs and one output such that autonomous transmitters      are connected to each input and such that the receiver is interested in decoding the messages sent by all      the transmitters. Decoder:  Mapping from the set of channel-output signals to the set of messages. Maximum-likelihood   decoder: Adecoderwhich  selects the message that best explains the received      signal, assuming all messages are equally likely. Encoder:  Mapping from the set of messages to the set of input codewords. Modem:   Device that converts binaryinformation streams into electrical signals (and vice-versa) for trans-      mission through the voiceband telephone channel. Rate:  The rate of acode is the number of bits transmitted (logarithm of codesize) per second for a      continuous-time channel or per channel use for adiscrete-time channel.Information Theory                                                                 6 -49  References R.E. Blahut, Principles of Information Theory. Reading,Mass.: Addison-Wesley,1987. T.M. Cover,‘‘Broadcast channels,’’ IEEE Trans. on Information Theory ,pp. 2–14, Jan. 1972. T. M. Cover and J. A. Thomas, Elements of Information Theory, NewYork: Wiley,1991. I. Csiszar and J. Korner, Information Theory: Coding Theorems for Discrete Memoryless Systems,New York:      Academic Press, 1981. R.L. Dobrushin, GeneralFormulation of Shannon’s Main Theorem in Information Theory ,American Math-      ematical SocietyTranslations, pp.323–438, 1963. A. Feinstein, ‘‘Anew basic theorem of information theory,’’ IRE Trans. PGIT, pp.2–22, 1954. R.G. Gallager, Information Theoryand ReliableCommunication, NewYork: Wiley,1968. T.S. Han and S. Verdu´ ,‘‘Approximation theoryofoutput statistics,’’ IEEE Trans. on Information Theory ,IT-39,      752–772, May1993. C.E. Shannon, ‘‘Amathematical theoryofcommunication,’’ Bell Sys. Tech. J., 27, 379–423, 623–656, July–Oct.      1948. C.E. Shannon, ‘‘Communication in the presence of noise,’’ Proc. Institute of Radio Engineers, 37, 10–21, 1949. C.E. Shannon, ‘‘Two-way communication channels,’’ Proc. 4th. BerkeleySymp.Math. Statistics and Prob. ,      pp.611–644, 1961. S. Verdu´ ,‘‘On channel capacityper unit cost,’’ IEEE Trans. Information Theory, IT-36(5), 1019–1030,      Sept. 1990. S. Verdu ´ and T.S. Han,‘‘A general formula for channel capacity,’’ IEEE Trans. on Information Theory ,vol. 40,      no.4,pp. 1147–1157, July 1994.  Further Information The premier journal and conferenceinthe ﬁeld of information theoryare the IEEE Trans. on Information Theory and the IEEE International Symposium on Information Theory, respectively. Problems of Information Transmission is atranslation of aRussian-language journal in information theory. The newsletter of the IEEE Information TheorySocietyregularly publishes expositoryarticles.  6.6     Data Compression Joy A. Thomasand Thomas M. Cover  Data compression is aprocess of ﬁnding the most efﬁcient representationofaninformation sourceinorderto minimize communication or storage. It often consists of two stages—the ﬁrst is the choice of a(probabilistic) model for the source, and the second is the design of an efﬁcient coding system for the model. Here, we will concentrate on the second aspect of the compression process, thoughwewilltouch on some common sources and models in the last section.   Thus, adata compressor (sometimes called asource coder) maps an information sourceinto asequence of bits, with acorresponding decompressor that, given these bits, provides areconstruction of the source. Data compression systems can be classiﬁed into twotypes: lossless, wherethe reconstruction is exactly equal to the original source, and lossy,wherethe reconstruction is adistortedversion of the original source. Forlossless data compression, the fundamental lower bound on the rate of the data compression system is given by the entropy rate of the source. Forlossy data compression, we haveatradeoffbetween the rate of the compressor and the distortion we incur,and the fundamental limit is given by the rate distortion function, which is discussed later in this section.   Shannon [1948] was the ﬁrst to distinguish the probabilistic model that underlies an information source from the semantics of the information. An information source produces one of manypossible messages; the goal of communication is to transmit an unambiguous speciﬁcation of the message so that the receiver can6 -50                                  Broadcasting and Optical Communication Technology  reconstruct the original message. Forexample, the information to be sent maybethe result of ahorse race. If the recipient is assumed to know the names and numbers of the horses, then the only data that must be transmitted is the number of the horse that won. In adifferent context,the same number mightmean something quite different, e.g., the price of abarrel of oil. The signiﬁcant fact is that the difﬁcultyin communication depends only on the length of the representation. Thus, ﬁnding the best (shortest) representation of an information source is critical to efﬁcient communication.   When the possible messages are all equally likely,then it makes sense to represent them by strings of equal length. For example, if thereare 32 possible equally likely messages, then each message can be represented by a binarystring of 5bits. However,ifthe messages are not equally likely,then it is moreefﬁcient on the average to allot shortstrings to the frequently occurring messages and longer strings to the rare messages. Forexample, the Morse code allots the shortest string (a dot) to the most frequent letter (E), and allots long strings to the infrequent letters (e.g., dash, dash, dot, dash for Q ). The minimum average length of the representation is a fundamental quantitycalled the entropy of the source, which is deﬁned in the next section.  Entropy An information sourcewill be represented by arandom variable, X ,which takes on one of aﬁnite number of  possibilities, i 2 w ,with probability, p i ¼ Prð X ¼ i Þ .The entropy of the random variable X is deﬁned as                                             X                                   H ð X Þ¼     p i log p i                        ð 6 : 81Þ                                             i Ew  wherethe log is to base 2and the entropy is measured in bits.Wewilluse logarithms to base 2throughout this chapter.   Example 6.9.  Let Xbearandom variable that takes on avalue, 1, with probability, y ,and takes on the   value 0with probability, 1   y .Then, H ð X Þ¼  y log y  ð1   y Þ logð 1   y Þ .Inparticular,the entropy of a   fair coin toss is 1bit.   This deﬁnition of entropy is related to the deﬁnition of entropy in thermodynamics. It is the fundamental lowerbound on the average length of acode for the random variable.   A code for arandom variable, X ,isamapping from w ,the range of X ,tothe set of ﬁnite length binarystrings. We will denote the codewordcorresponding to i by C(i),and the length of the codewordby l .The average                             P                                               i length of the codeisthen, L ð C Þ¼ i p i l i .   Acodeissaid to be instantaneous or preﬁx-free if no codeword is apreﬁx of anyother codeword. This condition is sufﬁcient (but not necessary) to allow asequence of received bits to be parsed unambiguously into asequence of codewords.   Example  6.10. Consider arandom  variable, X ,taking on the values {1,2,3} with probabilities   (0.5,0.25,0.25). An instantaneous codefor this random variable might be (0,10,11). Thus, astring   01001110 can be uniquely parsed into 0,10,0,11,10, which decodes to the string x ¼ (1,2,1,3,2). Note that   the average length of the code is 1.5 bits, which is the same as the entropy of the source. The following property,called the Kraft inequality, holds for anyinstantaneous code                                       X                                           2   l i < 1                             ð 6 : 82Þ                                        i   where l i ; i ¼ 1 ; 2 ; ... are the lengths of the codewords. Conversely,itcan be shown that, given aset of lengths that satisﬁes the Kraft inequality, we can ﬁnd aset of preﬁx free codewords withthose lengths.   The problem of ﬁnding the best source codethen reduces to ﬁnding the optimal set of lengths that satisﬁes the Kraft inequalityand minimizes the average length of the code. Simple calculus can then be used to show [Coverand Thomas, 1991] that the average length of anyinstantaneous code is larger than the entropy of theInformation Theory                                                                 6 -51                                 P                   P           P random variable, i.e., the minimum of p l over all l satisfying 2   l i < 1is   p log p .Also,ifwetake into                                   i   i   i                       i   i                                       1 account the integer constraint and let l i ¼ log (where d t e denotes the smallest integer greater than t ), we                                       p i can verify that this choice of lengths satisﬁes the Kraftinequality and that                                                                           X        1     X        1                    L ð C Þ¼  p  log   5     p log   þ 1  ¼ H ð X Þþ1              ð 6 : 83Þ                               i    p         i    p                            i        i     i        i  Therefore,wehavethe following theorem:    Theorem 6.1. Let L* be the average length of the optimal instantaneous code for arandom variable, X. Then                                    H ð X Þ < L * ð X Þþ1 :                        ð 6 : 84Þ    This theorem is one of the fundamental theorems of information theory. It identiﬁes the entropy as the fundamental limit for the description of adiscrete information source, and shows that we can ﬁnd representationswithaverage length within 1bit of the entropy.  The Huffman Algorithm                                lm                                    1 The choice of codewordlengths, l i ¼ log (called the Shannon codelengths), is close to optimal, but not                                   p i necessarily optimal, in terms of average codeword length. We willnow describe an algorithm (the Huffman algorithm)that produces an instantaneous code of minimal average length for arandom variable with the  distribution p 1 ; p 2 ; ...; p m .The algorithm is agreedyalgorithm for building the tree from the bottom up.    Step 1. Arrange the probabilities in decreasing ordersothat p 1 > p 2 >    > p m .   Step 2. Form asubtreebycombining the last twoprobabilities, p m   1 and p m ,toasingle node of weight,          0         p m   1 ¼ p m   1 þ p m .   Step 3. Recursively execute Steps 1and 2, decreasing the number of nodes each time, until asingle node is         obtained.   Step 4. Usethe tree constructed above to allot codewords.   The algorithm for tree construction is illustrated for asourcewithdistribution (0.5,0.2,0.2,0.1) in Figure6.21. After constructing the tree, the leavesofthe tree (which correspond to the symbols of w )can be assigned codewords that correspond to the paths from the root to the leaf. We will not giveaproof of the optimalityofthe Huffman algorithm; the reader is referred to [Gallager,1968] or [Cover and Thomas, 1991] for details.                            FIGURE 6.21 Example of the Huffman Algorithm.6 -52                                  Broadcasting and Optical Communication Technology  Entropy  Rate  The entropy of asequence of random variables, X 1 ; X 2 ; ... ; X n ,withjoint distribution, p ð x 1 ; x 2 ; ... ; x n Þ ,is deﬁned analogously to the entropy of asingle random variable as                                 X  X     X            H ð X 1 ; X 2 ; ... ; X n Þ¼  ... p ð x 1 ; x 2 ; ... ; x n Þ log p ð x 1 ; x 2 ; ... ; x n Þð6 : 85Þ                                  x 1 x 2 x n  Forastationaryprocess, X 1 ; X 2 ; ... ,wedeﬁne the entropy rate, H ð w Þ ,ofthe process as                                            H ð X ; X ; ... ; X Þ                              H ð w Þ ¼ lim    1  2      n                         ð 6 : 86Þ                                      n ! 1       n  It can be shown [Cover and Thomas, 1991] that the limit exists for all stationaryprocesses. In particular,if  X 1 ; X 2 ; ... ; X n is asequence of independent and identically distributed random variables, then H ð X 1 ; X 2 ; ... ; X n Þ¼nHð X 1 Þ ,and H ( w ) = H ( X 1 ).   In the previous section, we showedthe existence of apreﬁx-free codehaving an average length within 1bit of the entropy.Now instead of trying to represent one occurrence of the random variable, we can form acode to represent ablock of n random variables. In this case, the average codelength is within 1bit of  H ð X 1 ; X 2 ; ... ; X n Þ .Thus the average length of the code per input symbol satisﬁes                       H ð X ; X ; ... ; X Þ L * H ð X ; X ; ... ; X Þ 1                          1   2      n <   n <     1  2      n þ                   ð 6 : 87Þ                              n           n           n           n      H ð X ; X ; ... ; X Þ Since   1  2     n !  H ð w Þ ,wecan get arbitrarily close to the entropy rate by using longer and longer            n block lengths. Thus, the entropy rate is the fundamental limit for data compression for stationarysources, and we can achieve rates arbitrarily close to this limit by using long blocks.   All the aboveassumes that we knowthe probabilitydistribution that underlies the information source.In manypractical examples, however, the distribution is unknown or too complex to be used for coding.There are various ways to handle this situation:      . Assume asimple distribution and design an appropriate code for it. Usethis codeonthe real source. If       an estimated distribution, pp^ ,isused when in fact the true distribution is p ,then the average length of                              P        p ð x Þ       the code increases to H ð X Þþ p ð x Þ log .The second term, which is denoted D ð p jjpp^ Þ ,iscalled the                                x      pp^ ð x Þ       relative entropy or the Kullback–Leibler distance between the twodistributions.      . Estimate the distribution empirically from the source, and adapt the code to the distribution. For       example, with adaptive Huffman coding,the empirical distribution of the source symbols is used to       design the Huffman code used for the source.     . Useauniversal coding algorithm like the Lempel-Ziv algorithm.  Arithmetic   Coding In the previous sections, it was shown how to construct acode for asource that achieves an average length within 1bit of the entropy.For small sourcealphabets, however, we haveefﬁcient coding only if we use long blocks of source symbols. Forexample, if the source is binary, and we code each symbol separately,wemust use 1bit per symbol irrespective of the entropy of the source. If we use long blocks, we can achieveanexpected length per symbol close to the entropy rate of the source.   Therefore, it is desirable to haveanefﬁcient coding procedurethat works for long blocks of source symbols. Huffman coding is not ideal for this situation, since it is abottom-up procedure with acomplexity that growsInformation Theory                                                                 6 -53                           F(x)                      1                                                 p(x)                         0                                               x                      FIGURE 6.22 Cumulativedistribution function for sequences x .  rapidly with the block length. Arithmetic coding is an incremental coding algorithm that works efﬁciently for long block lengths and achieves an average length within 1bit of the entropy of the block.                                                             n   The essential idea of arithmetic coding is to represent asequence, x ¼ x 1 x 2 ... x n ,bythe cumulative                       n    P       n distribution function, F ð x Þ¼ xx~ n < x n p ð xx~ Þ ,expressed to an appropriate accuracy. The cumulative distribution function for x n is illustrated in Figure6.22. We can use anyreal number in the interval,                                                                                                                                   1 ð F ð x n Þ p ð x n Þ ; F ð x n Þ ,asthe code for x n .Expressing F ( x n )toanaccuracy of log will give us acode for                                                                 p ð x n Þ the source. The receiver can drawthe cumulativedistribution function, drawahorizontal line corresponding to the truncated value, bcF ð x n Þ ,that was sent, and read offthe corresponding x n .(This code is not preﬁx-free but can be easily modiﬁed to construct apreﬁx-free code [Coverand Thomas, 1991].) To implement arithmetic coding,however,weneed efﬁcient algorithms to calculate p ( x n )and F ( x n )tothe appropriate accuracy based on aprobabilistic model for the source. Details can be found in [Langdon, 1984]. Lempel–Ziv Coding The Lempel–Ziv algorithm [Ziv and Lempel, 1978] is auniversal coding procedurethat does not require knowledge of the sourcestatistics but is asymptotically optimal. The basic idea of the algorithm is to construct atable or dictionaryoffrequently occurring strings and to represent new strings by pointing to their preﬁxes in the table. We ﬁrst parse the string into sequences that havenot appearedsofar.For example, the binarystring 11010011011100 is parsed into 1,10,100,11,0,111,00, etc. Then instead of sending the bits of each phrase, we send apointer to its preﬁx and the value of the last bit. Thus, if we use three bits for the pointer,wewill represent this string by (000,1), (001,0), (010,0), (001,1), (000,0), (100,1), (101,0), etc. Forthis shortexample, the algorithm has not compressed the string —ithas in fact expanded it.   But the verysurprising fact is that, as Lempel and Ziv haveshown, the algorithm is asymptotically optimal for anystationaryergodic source. This fact is expressed in the following theorem [Ziv and Lempel, 1978; Cover and Thomas, 1991]:     Theorem 6.2  Let L n be the length of the Lempel–Ziv code for n symbols drawn from astationaryergodic process, X 1 ; X 2 ; ...; X n ,withentropy rate H ð w Þ .Then,                               L                                n ! H ð w Þ with probability 1                     ð 6 : 88Þ                               n6 -54                                  Broadcasting and Optical Communication Technology  Thus, for long enoughblock lengths, the Lempel-Ziv algorithm (which does not makeany assumptions about the distribution of the source) does as well as if we knew the distribution in advanceand designed the optimal codefor this distribution.   The algorithm described above is only one of alarge class of similar adaptivedictionary-based algorithms, which are all rather loosely called Lempel–Ziv.These algorithms are simple and fast, and havebeen implemented in both software and hardware, e.g., in the compress command in UNIX and the PKZIP command  on PCs. On ASCII text ﬁles, the Lempel–Ziv algorithms achieve compressions of the order of 50%. It has also been implemented in hardware and has been used to ‘‘double’’the capacityofdata storage media or main memory, or to ‘‘double’’the effective transmission rate of amodem. Manyvariations on the basic algorithm can be found in [Bell et al., 1990].  Rate  Distortion Theory An inﬁnite number of bits are required to describe an arbitraryreal number,and thereforeitisnot possible to perfectly represent acontinuous random variable with aﬁnite number of bits. How‘‘good’’can the representation be? First, deﬁne adistortion measure, which is ameasureofthe distance between the random variable and its representation. Then, consider the tradeoffbetween the number of bits used to represent a random variable and the distortion incurred. This tradeoffisrepresented by the rate distortion function, R ( D ), which represents the minimum rate required to represent arandom variable with adistortion, D .    We will consider adiscrete information sourcethat produces random variables, X 1 ; X 2 ; ... ; X n ,that are drawn independently and identically distributed (i.i.d.) according to p ( x ). (The results are also valid for continuous sources.) The encoder of the rate distortion system of rate, R ,will encode ablock X n of n outputs as an index, f ð X n Þ 2 f 1 ; 2 ; ...; 2 nRg .(Thus the index will require R bits/input symbol.) The decoder will calculate arepresentation, XX ^ n ð f ð X n ÞÞ of X n .Normally,the representation alphabet, ww^ ,ofthe representation is the same as the alphabet source, w ,but that need not be the case.   Deﬁnition 6.1.  Adistortion function or distortion measure is amapping                                      d : w · ww^ ! R þ                           ð 6 : 89Þ    from the set of sourcealphabet-reproduction alphabet pairs into the set of non-negativereal numbers.   The distortion d ð x ; xx^ Þ is ameasureofthe cost of representingthe symbol x by the symbol xx^ . Examples of common distortion functions are      . Hamming (probability of error) distortion. The Hamming distortion is given by                                                                                             0if   x ¼ xx^                                    d ð x ; xx^ Þ¼                                 ð 6 : 90Þ                                              1if   x 6¼ xx^        and thus, Edð X ; XX ^ Þ¼Prð X 6¼ XX ^ Þ ;the distortion is the probability of error.     . Squared error distortion. The squared error distortion,                                       d ð x ; xx^ Þ¼ð x   xx^ Þ 2                  ð 6 : 91Þ        is the most popular distortion measure used for continuous alphabets. Its advantages are its simplicity       and its relationship to least squaresprediction. However,for information sources such as images and       speech, the squared error might not be an appropriate measurefor distortion as perceived by ahuman       observer.Information Theory                                                                 6 -55  The distortion between sequences, x n and xx^ n ,oflength nisdeﬁned by                                             1 X n                                 d ð x n ; xx^ n Þ¼ d ð x ; xx^ Þð6                  : 92Þ                                            n       i i                                              i ¼ 1  Forarate distortion system, the expected distortion D is deﬁned as                                              X                      D ¼ Edð X n ; XX ^ n ð f ð X n ÞÞÞ ¼ p ð x n Þ d ð x n ; XX ^ n ð f ð x n ÞÞÞ ð 6 : 93Þ                                              x n   Deﬁnition 6.2.  The rate distortion pair (R,D) is said to be achievable if there exists arate distortion code   of rate, R, that has expected distortion, D. The rate distortion function, R(D), is the inﬁmum of rates, R, such   that (R,D) is achievable for agiven D.   Deﬁnition 6.3.  The mutual information, I ð X ; XX ^ Þ ,between random variables, Xand XX ^ ,with joint   probability distribution function, p ð x ; xx^ Þ ,and marginal probability distribution functions, p(x) and p ð xx^ Þ ,is   deﬁned as                                      X  X            p ð x ; xx^ Þ                           I ð X ; XX ^ Þ¼ p ð x ; xx^ Þ log                     ð 6 : 94Þ                                                    p ð x Þ p ð xx^ Þ                                     x 2 w xx^ 2 ww^  The mutual information is ameasureofthe amount of information that one random variable carries about another.   The main result of rate distortion theoryiscontained in the following theorem, which provides a characterization of the rate distortion function in terms of the mutual information of joint distributions that satisfy the expected distortion constraint    Theorem 6.3  The rate distortion function for an i.i.d. source, X, withdistribution, p(x),and distortion   function, d ð x ; xx^ Þ ,is                             R ð D Þ¼    P  min         I ð X ; XX ^ Þð6              : 95Þ                                   p ð xx^ j x Þ : p ð x Þ p ð xx^ j x Þ d ð x ; xx^ Þ < D                                       ð x ; xx^ Þ  We can construct rate distortion codes that can achieve distortion, D ,atany rate greater than R ( D ), and we cannot construct such codes at anyrate below R ( D ).   The proof of this theorem uses ideas of random coding and long block lengths, as in the proof of the channel capacitytheorem. The basic idea is to generate acodebook of 2 nR reproduction codewords, XX ^ n ,at random and show that, for long block lengths, for anysourcesequence,itisverylikely that there is at least one codewordinthis codebook that is within distortion, D ,ofthat source sequence.See [Gallager,1968] or [Cover and Thomas, 1991] for details of the proof.    Example 6.11.  ( Binarysource.)The rate distortion function for aBernoulli(p) source(arandom   variable that takes on values {0,1} with probabilities p ; 1   p )withHammingdistortion is given by                                                             H ð p Þ H ð D Þ ; 0 < D < minf p ; 1   p g                     R ð D Þ¼                                                    ð 6 : 96Þ                              0 ;            D >  minf p ; 1   p g    where H ð p Þ¼  p log p  ð1   p Þ log ð 1   p Þ is the binaryentropy function.6 -56                                  Broadcasting and Optical Communication Technology    Example  6.12. ( Gaussian source.)The rate distortion function for aGaussian random variable with   variance s 2 and squared error distortion is                                    (                                      1 log s 2 ; 0 < D < s 2 ;                            R ð D Þ¼  2   D                                      ð 6 : 97Þ                                      0 ;      D  > s 2                                                                                2   Thus with nR bits, we can describe n i.i.d. Gaussian random variables, X 1 ; X 2 ; ...; X n ,  Nð 0 ; s Þ with a   distortion of s 2 2   2 R per symbol. Quantization   and Vector  Quantization The rate distortion function represents the lower bound on the rate that is needed to represent asource with a particular distortion. We now consider simple algorithms that represent acontinuous random variable with a few bits. Suppose we want to represent asingle sample from acontinuous source. Let the random variable to be represented be X and let the representation of X be denoted as XX ^ ð X Þ .Ifweare given R bits to represent X , then the function XX ^ can take on 2 R values. The problem of optimum quantization is to ﬁnd the optimum set of values for XX ^ (called the reproduction points or codepoints) and the regions that are associated with each value, XX ^ ,sotominimize the expected distortion.   Forexample, let X be aGaussian random variable with mean, 0, and variance, s 2 ,and assume asquared error distortion measure. In this case, we wish to ﬁnd the function XX ^ ð X Þ such that XX ^ takesonatmost 2 R values and minimizes E ð X   XX ^ ð X ÞÞ2 .Ifweare given 1bit to represent X ,itisclear that the bit should distinguish whether X > 0ornot. To minimize squared error,each reproduced symbol should be at the conditional mean of its region. If we are given 2bits to represent the sample, the situation is not as simple. Clearly,wewant to divide the real line into four regions and use apoint within each region to represent the sample. We can state twosimple properties of optimal regions and reconstruction points for the quantization of asingle random variable      . Given aset of reconstruction points, the distortion is minimized by mapping asource random variable,       X ,tothe representation, XX ^ ð o Þ ,that is closest to it (in distortion). The set of regions of w deﬁned by this       mapping is called aVoronoi or Dirichlet partition, deﬁned by the reconstruction points.     . Given aset of reconstruction regions, the reconstruction points should be chosen to minimize the       conditional expected distortion over their respective assignment regions.   These twoproperties enable construction of asimple algorithm to ﬁnd a‘‘good’’quantizer:start with aset of reconstruction points, ﬁnd the optimal set of reconstruction regions (which are the nearest neighbor regions with respect to the distortion measure), then ﬁnd the optimal reconstruction points for these regions (the centroids of these regions if the distortion is squared error), and then repeat the iteration for this new set of reconstruction points. The expected distortion is decreased at each stage in the algorithm, so the algorithm will converge to alocal minimum of the distortion. This algorithm is called the Lloyd algorithm.   It follows from the arguments of rate distortion theorythat we will do better if we encode long blocks of source symbols rather than encoding each symbol individually.Inthis case, we will consider ablock of n symbols fromthe sourceasavector-valued random variable, and we willrepresent these n -dimensional vectors by aset of 2 nR codewords. This process is called vector quantization (VQ).Wecan apply the Lloyd algorithm to design aset of representation vectors (the codebook) and the corresponding nearest neighbor regions. Instead of using the probabilitydistribution for the sourcetocalculate the centroids of the regions, we can use the empirical distribution from atraining sequence.Manyvariations of the basic vector quantization algorithm are described in [Gersho and Gray,1992].   Common   information sources like speech produce continuous waveforms, not discrete sequencesof random variables as in the models we havebeen considering so far.But by sampling the signal at twice the maximum  frequency present (the Nyquist rate), we convert the continuous time signal into aset of discrete samples from which the original signal can be recovered (the sampling theorem). We can then apply the theory of rate distortion and vector quantization to such waveform sources as well.Information Theory                                                                 6 -57  Kolmogorov Complexity In the 1960s, the Russian mathematician Kolmogorov considered the question, ‘‘What is the intrinsic descriptive complexityofabinarystring?’’From the discussion above, it follows that if the binarystring is asequence of  independent and identically distributed random variables, X 1 ; X 2 ; ... ; X n ,then on average it would take nH( x ) bits to represent the sequence.But what if the bits were the ﬁrst million bits of the binaryexpansion of p ?Inthat case, the string appears random, but can be generated by asimple computer program. So if we wanted to send these million bits to another location that has acomputer,wecould instead send the program and ask the computer to generate these million bits. Thus, the descriptive complexityof p is quite small.   Motivated by such considerations, Kolmogorov deﬁned the complexity of abinarystring to be the length of the shortest program for auniversal computer that generated that string.(This concept was also proposed independently and at about the same time by Chaitin and Solomonoff.)    Deﬁnition 6.3.  The Kolmogorov complexity,Ku ð x Þ ,ofastring, x, with respect to auniversal computer,   U ,isdeﬁned as                                   K U ð x Þ¼ min  l ð p Þð6                        : 98Þ                                           p : U ð p Þ¼x    the minimum length over all programsthat print xand halt. Thus, K U ð x Þ is the shortest description length of   xover all descriptions interpreted by computer U .    Auniversal computer can be thought of as aTuring machine that can simulate anyother universal computer.Atﬁrst sight, the deﬁnition of Kolmogorov complexityseems to be useless, sinceitdepends on the particular computer that we are talking about. But using the fact that anyuniversal computer can simulate any other universal computer,any program for one computer can be converted to aprogram for another computer by adding aconstant length ‘‘simulation program’’ as apreﬁx. Thus we can show that for anytwo universal computers, U and A ,                                      j K U ð x Þ K A ð x Þj                        ð 6 : 99Þ  wherethe constant, c ,thoughlarge, does not depend on the string, x ,under consideration. Thus, Kolmogorov complexityisuniversal in that it does not depend on the computer (up to aconstant additive factor).   Kolmogorovcomplexityprovides auniﬁed waytothink about problems of data compression. It is also the basis of principles of inference(Occam’s Razor:‘‘The simplest explanation is the best’’)and is closely tied with the theoryofcomputability.  Data Compression in Practice The previous sections discussed the fundamental limits to compression for astochastic source. We will now consider the application of these algorithms to some practical sources, namely,text, speech, images, and video. In real applications, the sources may be neither stationarynor ergodic, and the distributions underlying the source are often unknown. Also,inaddition to the efﬁciency of the algorithm, important considerations in practical applications include the computational speed and memoryrequirements of the algorithm, the perceptual qualityofthe reproductions to ahuman observer, etc. There is aconsiderable amount of research and engineering that has gone into the development of these algorithms, and manyissues are only nowbeing explored. We will not go into the details, but simply list some popular algorithms for the different sources. Text English text is normally represented in ASCII, which uses 8bits/character.There is considerable redundancy in this representation (the entropy rate of English is about 1.3 bits/character). Popular compression algorithms include variants of the Lempel–Ziv algorithm, which compress text ﬁles by about 50% (to about 4bits/character).6 -58                                  Broadcasting and Optical Communication Technology  Speech Telephone-qualityspeech is normally sampled at 8KHz and quantized at 8bits/sample (a rate of 64 Kbits/sec) for uncompressed speech. Simple compression algorithms like ADPCM (AdaptiveDifferential Pulse Code Modulation) [Jayant and Noll, 1984] use the correlation between adjacent samples to reduce the number of bits used by afactor of 2to4ormorewith almost imperceptible distortion. Much higher compression ratios can be obtained with algorithms like LPC (Linear Predictive Coding), which model speech as an autoregressive process, and send the parameters of the process as opposed to sending the speech itself. With LPC based methods, it is possible to codespeech at less than 4Kbits/sec. However,atverylow bit rates, the reproduced speech sounds ‘‘synthetic.’’  Music Music on CDs is stored as an uncompressed stream at asampling frequency of 44,100 samples/sec, 16 bits/ sample in 2channels with an effective bit rate of about 1.4 Mb/sec. There is much redundancy in the signal, and strong correlation between the twochannels. Therefore, it is possible to compress the signal with very little loss of quality. The most popular standardfor compressing audio signals is derived from astandard from the MPEG  group called MP3, which achieves compressions ratios of about 10:1 with near CD quality, enabling users to easily download songs over the Internet.  Images Asingle highqualitycolor image of 1024 by 1024 pixels with 24 bits per pixel represents about 3MBofstorage in an uncompressedform. It is thereforeveryimportant to use compression to save storage and communication capacityfor images. There are manydifferent algorithms that have been proposed for image compression, and standardsare still being developed for compression of images. For example, the popular GIF standarduses Lempel–Ziv coding, and the JPEG standarddeveloped by the Joint Photographic Experts Group uses an 8by8discrete cosinetransform(DCT)followed by quantization (the qualityofwhich can be chosen by the user) and Huffman coding.The compression ratios achievedbythese algorithms are verydependent on the image being coded. The lossless compression methods achievecompression ratios of up to about 3:1, whereas lossy compression methods achieve ratios up to 50:1, with very little perceptible loss of quality. The JPEG standardisnow widely used for images on the Weband within digital cameras to compress images beforethey are stored in the camera’s memory.  Video Video compression methods exploit the correlation in both space and time of the sequence of images to improvecompression.There is averyhighcorrelation between successive frames of avideo signal. This correlation can be exploited along with methods similar to those used for coding images to achieve compression ratios up to 200:1 for highqualitylossy compression. Standards for full-motion video and audio compression are being developed by the Moving Pictures Experts Group (MPEG), including MPEG-1 (used for Video CDs), MPEG-2 (used in digital TVs and DVDs), MPEG-4 (used for multimedia on the Web), and MPEG-7  (a standard that allows for description of the content to enable search). Applications of video compression techniques include videoconferencing,multimedia CD-ROMs, and HDTV.   Afascinating and very readableintroduction to different sources of information, their entropy rates, and different compression algorithms can be found in the book by Lucky [1989]. Implementations of popular data compression algorithms, including adaptive Huffman coding,arithmetic coding,Lempel–Ziv,and the JPEG algorithm can be found in Nelson and Gailly [1995] and Sayood [2000].  Deﬁning   Terms Code:  Amapping  from aset of messages into binarystrings. Entropy:  Ameasureofthe average uncertaintyofarandom variable. Forarandom variable with probability                                               P      distribution, p ( x ), the entropy, H ð X Þ ,isdeﬁned as   p ð x Þ log p ð x Þ .                                                xInformation Theory                                                                 6 -59  Huffman coding:  Aprocedurethat constructs the code of minimum average length for arandom variable. Kolmogorov complexity:   The minimum  length description of abinarystring that would enable a      universal computer to reconstruct the string. Lempel–Ziv  coding:  Adictionary-based procedurefor coding that does not use the probability      distribution of the sourceand is nonetheless asymptotically optimal. Quantization:  Aprocess by which the output of acontinuous source is represented by one of aset of      discrete points. Rate distortion function: The minimum rate at which asourcecan be described to within agiven average      distortion. Vector quantization: Quantization applied to vectors or blocks of outputs of acontinuous source.  References T. Bell, J. Cleary, and I. Witten, Text Compression,Englewood Cliffs, N.J.: Prentice Hall,1990. T.M. Cover and J.A. Thomas, Elements of Information Theory ,New York: John Wiley,1991. R. Gallager, Information Theoryand Reliable Communication,New York: Wiley,1968. A. Gersho and R. Gray, Vector Quantization and Source Coding,Boston: KluwerAcademic Publishers, 1992. B. Haskell, P. Howard, Y. Le Cun, A. Puri, J. Ostermann, M. Civanlar,L.Rabiner,L.Bottou, and P. Haffner,      ‘‘Image and video coding: emerging standardsand beyond,’’ CirSysVideo,vol. 8, no.7,p.814, 1998. S. Verdu, Ed., ‘‘IEEE IT October,special issue: the ﬁrst ﬁfty years,’’ IEEE Transactions on Information Theory,      vol.IT-44, 1998. N. Jayant and P. Noll, Digital Coding of Waveforms: Principles and Applications to Speech and Video,Englewood      Cliffs, N.J.: Prentice Hall,1984. G. Langdon, ‘‘An introduction to arithmetic coding,’’ IBM JRes.Dev. ,vol.28, pp.135–149, 1984. R. Lucky, Silicon Dreams: Information, Manand Machine, NewYork: St. Martin’s Press, 1989. M. Nelson and J. Gailly, The Data Compression Book,2nd ed., NewYork: M&TBooks, 1995. K. Sayood, Introduction to Data Compression,2nd ed., San Francisco,CA: Morgan Kauffman, 2000. C.E. Shannon, ‘‘Amathematical theoryofcommunication,’’ Bell Sys. Tech. J.,vol.27, no.379–423, pp.623–      656, 1948. J. Ziv and A. Lempel, ‘‘Compression of individual sequencesbyvariable rate coding,’’ IEEE Trans. Info.Theory ,      IT-24, 530–536, 1978.  Further Information Discussion of various data compression algorithms for sources like speech and images can be found in the IEEE Transactions on Communications and the IEEE Transactions on Acoustics, Speech and Signal Processing, while the theoretical underpinnings of compression algorithms are discussed in the IEEE Transactions on Information Theory ,including aspecial issue [IEEE IT October,1998] that surveys the development of data compression algorithms over the last 50 years.   Some of the latest developments in the areas of image and video compression are described in the IEEE Transactions on Circuits and Systems for Video Technology .Itincludes asurvey[Haskell et al., 1998] of current work on image compression, including various image and video compression standards.This page intentionally left blank                                                                                 7                                Satellites and Aerospace                                 7.1 Introduction........................................................................ 7 -1                               7.2 Satellite Applications ............................................................. 7 -1                               7.3 Satellite Functions ................................................................ 7 -3                               7.4 Satellite Orbits and Pointing Angles ........................................ 7 -5                               7.5 Communications Link........................................................... 7 -8                               7.6 System Noise Temperature and G/T ........................................ 7 -9                               7.7 Digital Links ...................................................................... 7 -11                               7.8 Interference ....................................................................... 7 -11                               7.9 Some Particular Orbits ........................................................ 7 -12                               7.10 Access and Modulation ....................................................... 7 -13                               7.11 Frequency Allocations ......................................................... 7 -14 Daniel F. DiFonzo                               7.12 Satellite Subsystems ............................................................ 7 -15 Planar Communications Corporation                   7.13 Trends .............................................................................. 7 -16   7.1     Introduction  The impact of satellites on world communications since commercialoperations began in the mid-1960s is such that we nowtake for granted manyservicessuch as: worldwide TV;reliable communications to remote areas; mobile communications to ships, aircraft, and moving vehicles; wide area data networks; direct satellite TV and audio broadcast to homes and moving vehicles; position determination; Earth observation (weather and mapping); and personal communications such as hand-held mobile telephones.   Communications satellites function as line-of-sight microwave relays in orbits highabove the Earth, wherethey can see large areas of the Earth’s surface. Because of this unique feature, satellites are particularly well suited to communications over widecoverage areas for such applications as broadcasting, mobile communications, and point-to-multipoint communications. Satellite systems can also provide cost- effective access for manylocations wherethe highinvestment cost of terrestrial facilities might not be warranted.  7.2     Satellite Applications  Figure7.1 depicts several kinds of satellite links and orbits. The geostationaryEarth orbit (GEO) is in the equatorial plane at an altitude of 35,786 km withaperiod of one sidereal day (23 h56m 4.09s). This orbit is sometimes called the Clarke orbit in honor of Arthur C. Clarke, who ﬁrst described its usefulness for communications in 1945 [Clarke, 1945]. GEO satellites appear to be almost stationaryfrom the ground (subject to small perturbations), and the Earth antennas pointing to these satellites mayneed only limited                                                                                      7 -17 -2                                   Broadcasting and Optical Communication Technology   FIGURE 7.1 Several types of satellite links. Illustrated are point-to-point, point-to-multipoint, VSAT, direct broadcast, mobile, personal communications, and intersatellite links.  or no tracking capability. Satellites having orbit periods that are synchronous with the Earth’s rotation such as 12 hor6h, but are not generally in the equatorial plane, are called geosynchronous orbits (GSO).   An orbit for which the highest altitude (apogee) is greater than GEO and is sometimes referred to as a High Earth orbit (HEO). An orbit with altitude between afew hundred km and about 2000 km is considered to be in Low Earth orbit (LEO). AMedium Earth orbit (MEO) occupies an intermediate altitude.   LEO systems include those of Iridium Satellite LLC(www.iridium.com) with 66 operational satellites at an altitude of 780 km and GlobalstarTM with 40 satellites at an altitude of 1414 km for voiceand data communications by satellite to hand held telephones and pagers (www.globalstar.com).Iridium uses intersatellite links to relay information beyond the coverage of asingle satellite, whereas Globalstar uses an extensive network of Earthterminals for signal relay in multiple ‘‘hops’’ and for interconnection to terrestrial phone networks. MEO systems include the U.S. Department of Defense Global Positioning System (GPS) with 24 operational satellites at an altitude of 20,200 km and the proposed Galileo positioning system with 27 operational satellites at 23,616 km.   GEO  satellites are the most prevalent. Initially,communications satellites were used primarily for point- to-point trafﬁc in the GEO Fixed Satellite Service (FSS), e.g., for telephonyacross the oceans and for point- to-multipoint TV distribution to cable head end stations. Large Earth station antennas with high-gain narrow beams and highuplink powers were needed to compensate for limited satellite power. This type of system, exempliﬁed by the early global network of Intelsat, which was formerly an international treaty organization and nowisaprivate company(www.intelsat.com). The early Intelsat system used large Earth terminal antennas with 30 mdiameters. Inmarsat, another former treaty organization that is nowaprivate company, was formed to offer mobile communications servicestoships and has evolved to offer medium speed data services to small portable terminals (www.inmarsat.com). Manyother satellite organizationsSatellites and Aerospace                                                            7 -3  havebeen formed around the world to provide international, regional, and domestic services[Rees, 1990; Roddy,2001; Maral and Bousquet, 2002].   As satellites havegrown in power and sophistication, the average size of Earth terminals has been reduced. High gain satellite antennas and relatively highpower satellite transmitters haveled to verysmall aperture Earth terminals (VSAT) with diameters of less than 2m,modest powers of less than 10 W[Maral, 2003] and to even smaller diameters typically less than one meter.Asdepicted in Figure7.1, VSATterminals may be placed atop urban ofﬁce buildings, permitting private networks of hundreds or thousands of terminals, which bypass terrestrial lines. Anew development of in-motion VSATswillallow two-way high-speed data and Internet communication to and from moving vehicles (www.raysat.com). VSATsare typically incorporated into star or hub-and-spoke networks where the small terminals communicate throughthe satellite with alarger hub terminal. The hub retransmits through the satellite to another small terminal, or it can connect to a gateway that routes the signal to terrestrial facilities. Star connections between VSATsrequiretwo hops with attendant time delays. With high-gain satellite antennas and relatively narrow-band digital signals, direct single-hop mesh interconnections of VSATsmay be used. Descriptions of Earth terminals and their properties may be found in Ebert[2000].  7.3     Satellite Functions  The traditional satellite function is that of abent pipe quasilinear repeater in space. As shown in Figure7.2, uplink signals from Earth terminals directed at the satellite are received by the satellite’santennas, ampliﬁed, translated to adifferent downlink frequency band, channelized into transponder channels,further ampliﬁed to relatively highpowerand retransmitted towardthe Earth. Transponder channels are generally rather broad (e.g., bandwidths from 24 MHz to morethan 100 MHz) and each may contain manyindividual or user channels.   The functional diagram in Figure7.2 is appropriate to asatellite using frequency-division duplex (FDD), which refers to the fact that the satellites use separate frequency bands for the uplink and   FIGURE 7.2 Asatellite repeater receives uplink signals (U), translates them to adownlink frequency band (D), channelizes, ampliﬁes to highpower,and retransmits to Earth. Multiple beams allow reuse of the available band. Interference(dashed lines) can limit performance. Downconversion mayalso occur after the input multiplexers. Several intermediate frequencies and downconversions maybeused.7 -4                                   Broadcasting and Optical Communication Technology  downlink and whereboth links operate simultaneously.This diagram also illustrates aparticular multiple access technique, known as frequency-division multiple access (FDMA), which has been prevalent in mature satellite systems.   Multiple access allows manydifferentuser signals to utilize the satellite’sresourcesofpowerand bandwidth without interfering with each other.Multiple access techniques to segregate users include: frequency division (FDMA), where each user is assigned aspeciﬁc frequency channel; space-division multiple access (SDMA) that reuses the same frequenciesonmultiple spatially isolated beams; time-division multiple access (TDMA), whereeach user signal occupies an entireallocated frequencyband but for only partofthe time; polarization- division (PD), wherefrequenciesmay be reused on spatially overlapping but orthogonally polarized beams; and code-division multiple access (CDMA), wheredifferent users occupy the same frequency band, but use spread spectrum signals that contain orthogonal signaling codes [Roddy, 2001; Sklar,2001; Richharia, 1999].   Frequency modulation (FM) was widely used, but advances in digital voice, music and video compression haveled to the widespread use of digital modulation methods such as quadraturephase shift keying (QPSK) and quadrature amplitude modulation (QAM) [Schwartz 1990, Sklar 2001].   Some satellite architecturesincorporate on-boarddemodulation of the uplink digitally modulated signals. The demodulated baseband bits are routed by digital processors, switched to an appropriate downlink antenna beam, and then remodulatedprior to downlink transmission. Such regenerative repeaters or onboard processors permit ﬂexible routing of the user signals and can improvethe overall communications link by separating the uplink noise from that of the downlink. The baseband signals maybethose of individual users or they may represent frequency-division multiplexed (FDM), or time-division multiplexed (TDM) aggregate signals from manyusers.   Examples include the experimental NASA ACTS Ka-band (20 and 30 GHz) satellite, the Ka-band Spaceway satellites for data networks and direct TV broadcast (www.directtv.com), and Iridium. Manynew Ka-band satellite systems wereproposed in the 1990s for high-speed data networks and TV distribution. Their deployment has been slowerthan expected for data networks but Ka-band satellites are expected to be used for TV broadcast.   The 66 Iridium LEO satellites operate with on-boardprocessing and time-division duplex (TDD), using the same 1.6-GHz L-band frequencies for transmission and reception, but only receiving or transmitting for somewhat less than half the time each. Globalstar operates at 1.6 GHz for the uplink and 2.5 GHz for the downlink. Both systems were developed at huge cost and are operational after having emerged from bankruptcy [Finkelstein, Inkpen et al., 2000].   High-power direct broadcast satellites (DBS) provide TV to millions of users worldwide. In the United States, these systems operate in the Broadcast Satellite Service (BSS) Ku-band (12.2–12.7 GHz) and provide hundreds of TV channels directly to morethan 25 million subscribers (as of 2004) with parabolic dish antennas as small as 45 cm. DBS service providers include: U.S. companies DirecTV and EchoStar (Dish Network); Canadian provider Bell ExpressVu; and manyorganizations in Europe and Asia. The U.S. BSS satellites have32transponder channels, each with 24-MHz bandwidth, and each transponder carries typically about 12 NTSC TV channels or 1–2 High Deﬁnition TV (HDTV) channels using the Motion PictureExperts Group MPEG  2compression  standard(http://www.chiariglione.org/mpeg/). MPEG 4 compression (www.mpeg4.net) is expected to further increase the number of TV channels that each transponder can carry. Digital Audio Radio Service (DARS) is nowproviding direct nationwide highquality audio broadcast in the United States from highpower satellites operating in the S-band (2.3 GHz). XM Radio uses two highpowerGEO satellites to provide morethan 100 channels of highly compressed near- CD  qualityaudio to vehicles in the United States (www.xmradio.com). Sirius Satellite Radio provides a competing DARS S-band service using aconstellation of three satellites with inclined elliptical orbits (www.Sirius.com). Both companies supplement their satellite coverages with terrestrial repeaters in urban areas.   Links between LEO satellites (or the NASA Shuttle) and GEO satellites are used for data relay,for example, through the NASA tracking and data relaysatellite system (TDRSS). Iridium uses intersatellite links (ISL) to improvethe interconnectivityand minimize the need for ground terminals to route signals over awide-area network. ISL systems may typically operate at frequencies such as 23 GHz, 60 GHz, or even use optical links.Satellites and Aerospace                                                            7 -5    As of 2005, the usage of satellites has evolved considerably,such that it is now dominated by video services such as TV broadcast and cable head end distribution, followed by data network and Internet trunking services. Voice services rank behind these applications. By 2010 it is expected that DBS will represent nearly 60% of U.S. Satellite capacity. (www.futron.com)[Futron Corporation, January2005]  7.4     Satellite Orbits and Pointing Angles  Reliable communication to and from asatellite requires knowledge of its position and velocityrelative to a location on the Earth.Details of the relevant astrodynamic formulas for satellite orbits are given in Grifﬁn and French [1991] and Morgan and Gordon [1989]. Launch vehicles needed to deliverthe satellites to their intended orbits are described in Isakowitz [1994].    Asatellite, having mass m ,inorbit around the Earth,having mass M e ,traverses an elliptical path such that the centrifugal forcedue to its acceleration is balanced by the Earth’s gravitational attraction, leading to the equation of motion for two bodies                                       d 2 r m                                          þ   r ¼ 0                                 ð 7 : 1 Þ                                      d t 2 r 3   where r is the radius vector joining the satellite and the Earth’s center and m ¼ G ð m þ M e Þ < GMe ¼           3  2 398; 600: 5 km = s is the productofthe gravitational constant and the mass of the Earth. Because m 55 M e , the center of rotation of the twobodies may be takenasthe Earth’s center,which is at one of the focal points of the orbit ellipse.   Figure7.3 depicts the orbital elements for ageocentric right-handed coordinate system wherethe x -axis points to the ﬁrst point of Aries, which is the ﬁxed position against the stars wherethe sun’s apparent path around the Earthcrosses the Earth’s equatorial plane while traveling from the southern towardthe northern hemisphereatthe vernal equinox. The z -axis points to the north and the y -axis is in the equatorial plane and points to the winter solstice. The elements shown are: longitude or right ascension of the ascending node O                                   FIGURE 7.3 Orbital elements.7 -6                                   Broadcasting and Optical Communication Technology  measured in the equatorial plane, the orbit’sinclination angle i relativetothe equatorial plane; the ellipse semimajor axis length a ,the ellipse eccentricity e ,the argument (angle) of perigee o measured in the orbit plane fromthe ascendingnode to the satellite’sclosest approach to the Earth; and the true anomaly (angle) in the orbit plane from the perigee to the satellite n .   The mean anomaly M is the angle from perigee that would be traversed by asatellite moving at its mean angular velocity n .Given an initial value M 0 ,usually takenap szﬃﬃﬃﬃﬃeroﬃ for aparticular epoch (time) at perigee, the                                                    3 mean anomaly at time t is M ¼ M 0 þ n ð t   t 0 Þ ,where n ¼ m = a .The eccentric anomaly E may then be found from Kepler’stranscendental equation M ¼ E   e sin E ,which must be solved numerically by,for example, guessing an initial value for E and using aroot ﬁnding method. Forsmall eccentricities, the series approximation E < M þ e sin M þðe 2 = 2 Þ sin 2 M þðe 3 = 8 Þð3sin 3 M   sin M Þ yields good accuracy[Morgan and Gordon, 1989, p. 806]. Other useful quantities include the orbit radius, r ;the period, P ;ofthe orbit,  [i.e., for n ð t   t 0 Þ¼2 p ]; the velocity, V ;and the radial velocity, V r                                     r ¼ a ð 1   e cos E Þð7                          : 2 Þ                                             q ﬃﬃﬃﬃﬃﬃ                                      P ¼  2 p a 3 = m                              ð 7 : 3 Þ                                                                                            2  1                                      V 2 ¼ m                                       ð 7 : 4 Þ                                              r  a                                          e ð m a Þ 1 = 2 sin E                                    V  ¼                                            ð 7 : 5 Þ                                     r   a ð 1   e cos E Þ  Figure7.4 and Figure 7.5 depict Earth-centered geometrical quantities useful for communications links.  In Figure 7.4, the satellite position vector for altitude h above the equator at longitude l s is r 0 ¼ðr e þ h Þ xx^ , where r e ¼ 6378.14 km is the mean Earth radius and xx^ is aunit vector.The position vector of an Earth terminal at north latitude C and east longitude l e is r e ¼ r e ð cos c cos d xx^ þ cos c sin d yy^ þ sin c zz^ Þ ,where d ¼ l e   l s .   Figure7.5 shows the geometryinthe plane formed by asatellite, the terminal point on the Earth’s surface and the Earth’s center,where g is the Earth central or coreangle, y is the nadir angle relative to the subsatellite axis, and e is the elevation or angle abovethe local horizon to the satellite.   Note that y þ e þ g ¼ 908 .The normalized orbit radius is                                                                                 r þ  h    cos e                                   k ¼   e     ¼                                    ð 7 : 6 Þ                                          r e     sin y                                                                     r                                                                    s           r                                                                    e            e                                                                 q             g                                                                    h           r e                                                                     r o                                                   FIGURE 7.5 Geometryinthe plane of the satellite,    FIGURE 7.4 Geometryfor asatellite above the equator. Earth terminal, and Earth center.Satellites and Aerospace                                                            7 -7  Then,                                                    q ﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃ                     Slant range : r ¼  jjr   r ¼ r  1 þ k 2   2 k cos g                                    s    e    0    e                                ð 7 : 7 Þ                                                                                                            sin g                            Nadir angle : y ¼  tan  1                               ð 7 : 8 Þ                                                     k   cos g                                                                                                            cos g   1 = k                         Elevation angle : e ¼ tan  1                               ð 7 : 9 Þ                                                        sin g  Forthe speciﬁc case of an equatorial orbit (inclination ¼ 0) as shown in Figure7.4, the Earthcentral angle is given by cos g ¼ cos c cos d ,and the tilt of the satellite-Earth terminal-Earth center plane is                                                                                               sin d                                    f ¼  tan  1                                    ð 7 : 10Þ                                               tan c  The Earth terminal azimuth angle, a ,measured clockwise from north viewed fromlocal zenith as shown in Figure7.4 is given ﬁrst by computing b ¼ tan  1 ð tan d = sin c Þ .Ifthe terminal is in the northern hemisphere ( c . 0 – ), then a ¼ b þ 180– .Ifthe terminal is in the southern hemisphere, then a ¼ b þ 360– if it is east of the satellite ( d . 0 – )and a ¼ b if it is west of the satellite.   The fraction of the Earth’s surfacearea covered by the satellite for agiven elevation angle, e ,and the corresponding Earth central angle, g ,is                                   A c = A e ¼ð1   cos g Þ = 2                      ð 7 : 11Þ   ForaGEO   satellite, k ¼ 6.6107, r 0 ¼ 42164 km, h ¼ 35786 km and the maximum nadir angle, from Equation (7.6) when e ¼ 0issin y ¼ 1 = k for which y ¼ 8.7– and g ¼ 81.3– that represents the maximum latitude that can be covered from GEO.For this case, aGEO satellite sees about 42% of the Earth’s surface. However,sincethey are not typically operated below approximately e < 10– ,the practical relative Earth coverage areaisinthe region of 34%.   ForaGEO satellite transmitting (or receiving) linear polarization, auseful quantityisthe polarization tilt as seen by the Earth terminal when it is aimed towardthe satellite. The general method is developed from the vector relationships among the satellite antenna polarization vector,the direction of propagation, and the Earthterminal location (refer to Figure7.4). Forageneral linear polarization unit vector ee^ at the satellite, a  unit vector component normal to the direction of propagation, rr^ s ,is ee^ p ¼ðrr^ s · ee^ Þ · rr^ s ¼ ee^  ðee^ * rr^ Þ rr^ s . The tilt angle may be takenasthe angle between this vector and avector in areferenceplane containing the Earthcenter,satellite, and Earth terminal. This plane, shown in Figure7.4, appears tilted at angle f as  viewed from the satellite. Aunit vector normal to this plane is given by ð rr^ e · r s Þ = r s and the tilt angle relative to this plane is                                           1                                 t ¼ sin  ð ee^ p * ð rr^ e · r s Þ = r s Þð7        : 12Þ  Forthe special case of a‘‘vertical’’polarization vector in the x   z or north–south plane as seen at the satellite, the tilt angle relativetothe local ‘‘vertical’’orup-and down direction as seen from behind the terminal looking at the satellite is just f (see Equation (7.10)).7 -8                                   Broadcasting and Optical Communication Technology  7.5     Communications        Link  Figure7.6 illustrates the elements of the radio frequency (RF) link between asatellite and Earth terminals. The overall link performanceisdetermined by computing the link equation for the uplink and downlink separately and then combining the results along with interferenceand intermodulation effects.   Foraradio link with only thermal noise, the received carrier-to-noise power ratio is                                                   !                                                     2                                   c             1    g r 1   l   1      1                            ¼ðp  t g t Þ 2                 ð r Þ                  ð 7 : 13aÞ                         n           4 p r s T  k  4 p  a      b  The same quantities expressed in dB are                                                                  .                               2                                      2     ð C = N Þ¼EIRP   10 log 4 p r s þðG r   10 log T Þþ228: 6   10 log 4 p l   A þ G   B                                                                                  ð 7 : 13bÞ  where l ¼ c / f is the wavelength and c ¼ 2.9979 · 108 m/s is the velocityoflight. Subscripts refer to transmit ( t )and receive(r ). Lowercase terms are the actual quantities in watts, meters, etc., and the capitalized terms in Equation (7.13b) correspond to the decibel (dB) versions of the parenthesized quantities in Equation (7.13a).  Forexample, EIRP ¼ P þ G ¼ 10 log p þ 10 log g decibels relativeto1W (dBW) and the expression C = N should be interpreted as 10 log c   10 log n .The uplink and downlink equations haveidentical form with the appropriate quantities substituted in Equation (7.13). The relevant quantities are described below.   The ratio of received carrier powertonoise power c / n and its corresponding decibel value ð C = N Þ¼  10 logð c = n Þ dB is the primarymeasureoflink quality. The product of transmit power p t (W)and the transmit antenna gain g t ,orequivalently, P t (dBW) þ G t ( dBi), is wherethe antenna gain is expressed in decibel relative to an isotropic antenna, is called the equivalent isotropically radiated power (EIRP), and its unit is dBW because the antenna gain is dimensionless. The antenna gain is that in the direction of the link,i.e., it is not necessarily the antenna’s peak gain. The received thermal noise poweris n ¼ kTB W, where k ¼ 1.38 · 10  23 J/K is Boltzmann’s constant and 10 log ( k ) ¼ 228.6 dBW/K/Hz. T is the system noise temperatureinkelvins (K), and B is the bandwidth in dB Hz. Then, G   10 log T dB/K is aﬁgure of merit for the receiving system. It is usually written as G/T and read as gee over tee.The antenna gain and noise temperature must be deﬁned at the same reference point, e.g., at the receiver’sinput portoratthe antenna terminals.                                Satellite                                                            T        P   The spreading factor 4 p r 2 is independent of            su       sd                         s                                G     C frequency and depends only on the slant range             su    su     G sd                                                                          r distance r .The gain of an antenna with an effective   r su              sd         s                                                  f          f d aperturearea of 1m2 is 10 logð 4 p = l 2 Þ .The dB sum of  u                                                                T sky   L the spreading factor and the gain of a1m 2 antenna is    L u            d                                                                           G the frequency-dependent path loss.‘‘A’’ is the signal G eu                 ed    T ed  attenuation due to dissipative losses in the propaga-           T e tion medium. B ¼ 10 log(b )isthe bandwidth in dB P eu                            C ed Hz, where b is the bandwidth in Hz.   The polarization mismatch factor between the incident waveand the receive antenna, is given by FIGURE 7.6 Quantities for asatellite RF link. P ¼ transmit G ¼ 10 log r where 0 # r # 1. This factor maybe power (dBW). G ¼ antenna gain (dBi). C ¼ received carrier obtained from the voltage axial ratio of the incident power (dBW). T ¼ noise temperature (K). L ¼ dissipative  wave r w ,the voltage axial ratio of the receive loss (dB). r s ¼ slant range (m). f ¼ frequency (Hz). u ¼ antenna’s polarization response r a ,and the difference uplink. d ¼ downlink. e ¼ Earth. s ¼ satellite.Satellites and Aerospace                                                            7 -9   in tilt angles of the wave and antenna polarization ellipses D t ¼ t w   t a ,asfollows                                                                           1   4 r r þ r 2   1 r 2   1 cosðÞ2 D t                                    w a     w      a                          r ¼   þ           2      2                               ð 7 : 14Þ                              2          2 r w þ 1 r a þ 1  wherethe axial ratios are each signed quantities, having apositive sign for right-hand sense and a  negativesign for left-hand sense. Therefore, if the wave and antenna haveopposite senses, the sign of 4rw r a is negative. The axial ratio in dB is R ¼ 20 logjjr .The maximum polarization coupling occurs when the wave and antenna are copolarized, haveidentical axial ratios, and their polarization ellipses are aligned ( Dt ¼ 0). It is minimum when the axial ratios are identical, the senses are opposite, and the tilt angles differ by 90– .  7.6     System    Noise   Temperature and G/T  The system noise temperature T incorporates contributions to the noise powerradiated into the receiving antenna from the sky,ground, and galaxy,aswell as the noise temperature due to circuit and propagation losses and the noise ﬁgure of the receiver.The clear sky antenna temperaturefor an Earth station depends upon the elevation angle since the antenna’s sidelobes will receiveasmall fraction of the thermal noise power  radiated by the Earth, which has anoise temperature T Earth < 290K. At 11 GHz, the clear sky antenna noise                                                    –                         – temperature, T Earth,ranges from 5to10Kat zenith ( el ¼ 90 )tomorethan 50 Kat el ¼ 5 [Pratt and Bostian, 1986].   As shown in Figure 7.7, the system noise temperature is developed from the standardformula for the equivalent temperature of tandem elements including: the antenna in clear sky,propagation(rain) loss of  A ¼ 10 log(a) dB,circuit losses between the aperture and receiver of L c dB,and receiver noise ﬁgure of F dB (corresponding to receiver noise temperature T r K). The system noise temperaturereferred to the antenna apertureisapproximated by the following equation, where T rain < 280K is areasonable approximation for the physical temperatureofthe rain [Pratt et al., 2000].                        T ¼ T aclear þ T rainð a   1 Þ = a þ T r l c þ 290ð l c   1 Þð7 : 15Þ   FIGURE 7.7 Tandem connection of antenna, loss elements such as waveguide, and receiver front end. The noise temperature depends on the reference plane but g / T is the same for both points shown.7 -10                                  Broadcasting and Optical Communication Technology  The system noise temperatureisdeﬁned at aspeciﬁc reference point such as the antenna apertureorthe receiver input. However, G / T is independent of the referencepoint when G correctly accounts for circuit losses. The satellite’s noise temperature is generally higher than an Earth terminal’sunder clear sky conditions because the satellite antenna sees awarm Earth temperatureof < 150–300K, depending on the proportion of clouds, oceans, and land in the satellite antenna’s beam; whereas adirective Earth antenna generally sees cold sky,and the sidelobes generally receiveonly asmall fraction of noise powerfrom the warm Earth. Furthermore,asatellite receiving system typically has ahigher noise temperaturedue to circuit losses in the beam forming networks, protection circuitry,and extra components for redundancy.   Figure7.8 illustrates the link loss factors, maximum nadir angle, y ,Earth central angle, g ,and Earth-space propagation time delayasafunction of satellite altitude. The delayfor asignal hop between twoEarth locations includes the delays for the Earth-spacepath, the space-Earth path, and all circuit delays. The path losses are shown for several satellite frequencies in use. The variation in path loss and Earth central angle is substantial. Forexample, L-bandLEO personal communications systems to low-cost hand-held telephones with low gain (e.g., G <   2to þ 3dBi) need less link power than for MEO or GEO.Onthe other hand, more satellites are needed from LEO constellations to provide full Earth coverage sinceeach satellite sees amuch smaller fraction of the Earth compared with higher orbits.   The design for aconstellation of satellites to serve communications needs such as the number of satellites, their orbital parameters, the satellite G / T and EIRP,etc., are topics related to mission analysis and design and these involve trades of manyfactors such as total communications capacity, link margins, space segment and Earth segment costs, reliability, interconnectivity, availability and cost of launch vehicles, mission lifetime, and system operations [Wertz and Larson, 1991].   FIGURE 7.8 Satellite link losses, spreading factors, maximum nadir angle, y max,Earth central angle, g ,and one-way time delay vs. satellite altitude, h km.Satellites and Aerospace                                                           7 -11  7.7     Digital Links  Fordigital modulation systems, the bit errorrate (BER) is related to the dimensionless ratio (dB difference) of  energyper bit, E b dB J, to the total noise power density N 0 ¼ 10 logð kTÞ dB J[Sklar,2001]. For asystem with only thermal noise N 0 ,                         ð E b = N 0 Þ¼ð C = N ÞþB   R ¼ðC = N 0 Þ R dB             ð 7 : 16Þ  where R ¼ 10 log (bit rate in bit/s), B is the bandwidth (dB Hz), and ( C / N 0 )isdBratio of carrier power to thermal noise power density,that is, ( C / N )normalized to unit bandwidth. Curves relating the communications  performancemeasure of (BER) vs ( E b / N 0 )for differentmodulations maybefound in Sklar [2001]. The link equation may then be expressed in terms of ( E b / N 0 )and transmission data rate, R ,without explicit reference to the bandwidth               ð E b = N 0 Þ¼EIRP þðG = T Þþ228: 6   20 logð 4 p r s = l Þ A þ G   R dB ð 7 : 17Þ  wherethe appropriate quantities are substituted depending on whether the uplink or downlink is being considered.  7.8     Interference  Acomplete transponder link analysis must include the contributions of the uplink, downlink, and also the power sum of all interference signals due, for example, to intermodulation products that are generated in the output stages of the ampliﬁers, external interference from other systems, and intra-system interference from reusing the same frequency band on spatially isolated or dual-polarized antenna beams to increase communications capacity. Formost applications the total interference power may taken as the power sum of interfering signals as long as they are not correlated with the desiredcarrier.The values for the interfering signals due to effects outside the satellite, for example, frequency reuse cross-polarization, multiple beam interferers, and interference powerreceived from other systems must be obtained by carefully constructing the link equation for each case, taking into account the antenna gains for each polarization and beam direction of concern.   Foraninterferencepower i wand carrier power c w, the interference ratio c / i must be combined with the uplink and downlink c / n values to yield the total c / n .Here, the ratios are written in lowercase to indicate they are numerical power ratios.                                                    c                      1                               ¼     1      1     1      1     1                   ð 7 : 18Þ                         n  total c   þ  c  þ  c   þ  c  þ  c                                  n u    n d   i u    i d   i other  Equation (7.18) applies to a bent pipe satellite. If on-boardsignal regeneration is used for digital transmission, the uplink signal is demodulated and a clean set of baseband bits is remodulated. This has the effect of separating the accumulation of uplink and downlink noise contributions by causing the uplink noise to be effectivelymodulated onto the downlink carrier with the desiredsignal [Gagliardi, 1991]. In that case, only the uplink or the downlink term in the denominator of Equation (7.18) would be used as appropriate. Remodulation is also useful for intersatellite links. In each case, asaving in powerorantenna size maybe obtained at the expense of circuit and processing complexity.   The degradation to adigital link from interference follows aform similar to that of Equation (7.18) in terms  of e b / n 0 ,wherethe lower case quantities refer to numerical ratios. Foralink that is subject to a given additive white noise-like interference powerexpressed as aratio of desiredsignal powertointerference power c / i ,and7 -12                                  Broadcasting and Optical Communication Technology  assuming digital modulation with m bits per symbol,                                          e    1 c                                          b ¼                                      ð 7 : 19Þ                                         i 0 m  i  The ratio of energyper bit to total thermal noise plus interference power densityis                                                                      e              1                                    b    ¼                                         ð 7 : 20Þ                                 n þ  i      e    1  e    1                                  0   0       b   þ   b                                             n 0     i 0  Forasystem employing frequency reuse viadual polarizations, the polarization coupling factor G between a wave and antenna determines the interference power.The ( C / I )due to polarization is the ratio of desired (copolarized) receive power and undesired (cross polarized) powers and is called the polarization isolation.It maybefound  by application of Equation (7.14) to copolarized and cross-polarized cases.  7.9     Some   Particular    Orbits  A geosynchronous orbit has aperiod that is amultiple of the Earth’s rotation period, but it is not necessarily circular,and it maybeinclined. Therefore, a geostationary Earth orbit (GEO) is aspecial case of a  geosynchronous orbit where: e ¼ 0, i ¼ 0, k ¼ðr e þ h = r e Þ¼6 : 61, and h ¼ 35,786 km. When el ¼ 0, the maximum  nadir angle y ¼ 8.7– ,the maximum slant range is 41,680 km, and from Equation (7.8), g ¼ 81.3– .Therefore, aGEO satellite cannot see the Earth above 81.3– latitude.   Molniya and Tundra orbits are critically inclined ( i ¼ 63.4– )and haveorbital periods of 12 hand 24 h, respectively.These highly inclined elliptical orbits (HIEO) cause the satellite’ssubsatellite ground trace to dwell at apogee at the same placeeach day,allowing several satellites to be phased to offer quasi-stationary satellite service with highelevation angles at highlatitudes [Maral and Bousquet, 2002]. The Sirius Satellite Radio constellation of threesatellites is inclined at 63.4– and has an orbit period of 24 hwith apogee at orbit radius of 42164 km (the same as aGEO satellite). However,the elliptical orbit has aperigee of 24,469 km, so that the satellites dwell over the northern hemisphere,providing highelevation angles seen from the ground (see www.ils.launch.com).Sirius claims that this reduces the need for terrestrial repeaters. Forfull Earth coverage from aconstellation of LEO satellites, circular polar constellations [Adams and Rider,1987] and constellations of orbit planes with different inclinations, e.g., Walker Orbits [Walker, 1977] havereceived attention.   The oblateness of the Earth causes the right ascension of the ascendingnode O (Figure 7.3) to move in time with the equatorial plane in the opposite direction to the satellite’s motion as seen from above the ascending node. This is called regression of the nodes. For inclination i 5 90– (prograde orbit), the ascending node rotates westward. For i . 90– (retrograde orbit), the ascending node rotates eastward.For i ¼ 90– ,the regression is zero. The orbit parameters maybechosen such that the nodal regression is 360– /365.24 ¼ 0.9856 degrees eastward per day.Inthat case, the orbit plane will maintain aconstant angle with the sun. The local solar time for the line of nodes is constant, that is, the satellite crosses agiven latitude at the same solar time and solar lighting conditions each day.This sun-synchronous orbit has advantages for certain applications such as weather and surveillance satellites [Roddy,2001].   Table 7.1 compares the geometry, coverage, and some parameters relevant to the communications links for some LEO,MEO,and  GEO systems. Referenceshould be made to Figure7.4 for the geometryand to the given equations for geometrical and link parameters.Satellites and Aerospace                                                           7 -13         TABLE 7.1 Comparison of Orbit and Link Parameters for LEO,MEO,and GEO for the Particular        Case of Circular Orbits (eccentricity, e ¼ 0) and for Elevation Angle, el ¼ 10–                 Orbit                    LEO              MEO             GEO         Example System                 Iridium1          ICO*            Intelsat        Inclination, i (deg.)          86.4              ^ 45            0        Altitude, h (km)               780               10,400          35,786        Semi-major axis radius, a(km)  7159              16,778          42,164        Orbit Period (minutes)100.5                      360.5           1,436.1         ( r e þ h )/r e                1.1222            2.6305          6.6107        Earth Central Angle, g (deg.)  18.658            58.015          71.433        Nadir Angle, y (deg.)          61.3              22              8.6        Nadir Spread Factor:        10 log(4p h 2 )(dB m 2 )128.8                    151.3           162.1         Slant Range, r s (km)          2,325             14,450          40,586        One-waytime delay (ms)         2.6               51.8            139.1        Maximum Spread Factor:               2     2        10 logð 4 p r s Þ (dB m )138.3                   154.2           163.2        20 log(r s /h) (dB)            9.5               2.9             1.1        Ground Coverage Area (km2 )13.433   · 106        120.2 · 106     174.2 · 106        Fraction of Earth Area         0.026             0.235           0.34                                                             6  2            –          Note: Earth radius, r e ¼ 6378.14 km, Earth surfacearea, a e ¼ 511.2 · 10 km ,elevation, e ¼ 10 .The        MEO system never materialized and is shown herefor comparison.  7.10     Access and Modulation  Satellites act as central relay nodes, which are visible to alarge number of users who must efﬁciently use the limited resources of powerand bandwidth. For detailed discussions of access issues, see Gagliardi [1991], Pritchardetal., [1993], Miya [1985], Roddy [2001], and Shimbo [1988]. Abrief summaryofissues speciﬁc to satellite systems is now given.   Frequency-division multiple access (FDMA) has been the most prevalent access for satellite systems until recently.Individual users assigned aparticular frequency band may communicate at anytime. Satellite ﬁlters subdivide abroad frequency band into anumber of transponder channels.For example, the 500 MHz uplink FSS band from5.925 to 6.425 GHz maybedivided into 12 transponder channels of 36 MHz bandwidth plus guard bands. This limits the interference among adjacent channels in the corresponding downlink band of 3.7 to 4.2 GHz.   FDMA   implies that several individual carriers coexist in the transmit ampliﬁers. In ordertolimit intermodulation products caused by nonlinearities, the ampliﬁers must be operated in a backed off condition relative to their saturated output power. Forexample, to limit third-order intermodulation powerfor two carriers in aconventional traveling wave tube (TWT)ampliﬁer to about   20 dB relativetothe carrier,its input power must be reduced ( input backoff )byabout 10 dB relative to the powerthat would drive it to saturation. The output powerofthe carriers is reduced by about 4to5dB(output backoff ). Ampliﬁers with ﬁxed bias levelswillconsume power even if no carrier is present. Therefore, DC-to-RF efﬁciency degrades as the operating point is backedoff.For ampliﬁers with manycarriers, the intermodulation products havea noise-likespectrum and the noise-powerratio is abetter measure of multicarrier performance.   When reusing the available frequency spectrum by multiple spatially isolated beams (SDMA), interference can result if the sidelobes of one beam receives or transmits substantial energyinthe direction of the other beams. Twobeams that point in the same direction may reuse frequenciesprovided that they are orthogonally polarized, for example, vertical and horizontal linear polarizations or right- and left-hand circular polarizations. Typical values of sidelobe or polarization isolation among beams reusing the same frequency bands are from27to35dB.   Time-division multiple access (TDMA) users share acommon frequency band and are each assigned a unique time slot for their digital transmissions. At anyinstant the DC-RF efﬁciency is highbecause thereis7 -14                                  Broadcasting and Optical Communication Technology  only one carrier in the transmit ampliﬁer,which maybeoperated near saturation. Adrawback is the system complexityrequired to synchronize widely dispersed users in ordertoavoid intersymbol interference caused by more than one signal appearing in agiven time slot. Also,the total transmission rate in aTDMA satellite channel must be essentially the sum of the users’rates, including overhead bits such as for framing, synchronization and clock recovery, and sourcecoding.   In code-division multiple access (CDMA) each carrier is modulated with aunique pseudo-random code, usually by means of either adirect sequence or frequency hopping spread spectrum modulation. Because the CDMA  users occupy the same frequency band at the same time, the aggregate signal in the satellite ampliﬁer is noise-like.Individual signals are extracted at the receiver by correlation processes. CDMA tolerates noise-like interference but does not tolerate large deviations fromaverage loading conditions. One or moreverystrong carriers could violate the noise-like interferencecondition and generate strong intermodulation signals. Careful powercontrol of each user’ssignal is usually required in CDMA systems.   User access is throughassignments of afrequency,time slot, or code. Fixed assigned channels allow a user unlimited access. However,this may result in poor utilization efﬁciency for the satellite resources and mayimply  higher user costs (analogous to aleased terrestrial line). Other assignment schemes include demand assigned multiple access (DAMA) and random access (e.g., for the Aloha concept). DAMA systems require the user to ﬁrst send achannel request over acommon control channel. The network controller (at another Earth station) seeks an emptychannel and instructs the sending unit to tune to it either in frequency or time slot. Alink is maintained for the call duration and then released to the system for other users to request. Random access is economical for lightly used burst trafﬁc such as data. It relies on random time of arrival of data packets, and protocols are in placefor repeat requests in the event of collisions [Gagliardi, 1991].   In practice, combinations of multiplexing and access techniques maybeused. Abroad band may be channelized or frequency-division multiplexed (FDM), and FDMA may be used in each sub-band, for example, FDM/FDMA.  7.11     Frequency     Allocations  Table 7.2 contains apartial list of frequency allocations for satellite communications. The International Telecommunications Union (ITU) holds periodic World Radiocommunications Conferences (WRC)related to frequency allocations. Auseful chart of U.S. allocations may be obtained at http://www.ntia.doc.gov/ osmhome/allochrt.pdf.            TABLE 7.2 APartial List of Satellite Frequency Allocations—Frequencies in GHz            Band           Uplink          Downlink            Satellite Service                         1.613.8–1.6605   1.6138–1.6265    Mobile                        1.215–1.240                       Radio navigation                                         1.227            GPS           S-Band       1.980–2.0102.170–2.200MSS                                         2.32–2.345       Broadcast Satellite (U.S.)                                         2.4835–2.500     Mobile           C-Band       5.85–7.075       3.4–4.2          Fixed (FSS)                        7.250–7.300      4.5–4.8          FSS           X-Band       7.9–8.4          7.25–7.75        FSS           Ku-Band      12.75–13.25      10.7–12.2        FSS                        14.0–14.8        12.2–12.7        Direct Broadcast (BSS) (U.S.)           Ka-Band      17.3–17.78                        FSS (BSS in U.S.)                                                          22.55–23.55 Intersatellite                        27–31            17–21            FSS           Q42.5–43.5,                   37.5–40.5        FSS, MSS                          47.2–50.2Satellites and Aerospace                                                           7 -15  7.12     Satellite Subsystems  The major satellite subsystems are described in, for example, Grifﬁn and French [2004]. They are: propulsion, power,antenna, communications repeater,structures, thermal, attitude determination and control, telemetry, tracking,and command. Thermal control is described in Gilmore [1994].   The satellite antennas typically are offset-fed paraboloids. Typical sizes are constrained by launch vehicles and haveranged from less than 1mto morethan 20 mfor some applications. The XM satellites use two 5-m shaped reﬂectors, each with asingle feed and highpowertransmitters to producedownlink EIRP of morethan 67 dBWoverthe   contiguous United States (CONUS). Amoretraditional approach to beam shaping werethe Intelsat 6satellites, which used aD¼ 3.2-m diameter antenna at 4GHz. Multiple feeds in the focal region. Each feed produces anarrow component beam whose beamwidth is < 65l /D and whose directions are established by the displacement of the feeds from the focal point. These beams are combined to produce ashaped beam withrelatively highgain over ageographical region. Multiple beams are also used to reuse frequencies on the satellite. Figure 7.2 suggests that asatellite mayhaveseveral beams for frequency reuse. In that case, the carriers occupying the same frequencies must be isolated from each other by either polarization orthogonalityorantenna sidelobe suppression. As long as the sidelobes of one beam do not radiate strongly in the direction of another,both may use the same frequency band and increase the satellite’s capacity.   The repeaters include the following main elements (see Figure 7.2): alow noise ampliﬁer (LNA) ampliﬁes the received signal and establishes the uplink noise. The G / T of the satellite receiver includes the effect of losses in the satellite antenna, the noise ﬁgure of the LNA, and the noise temperature of the Earth seen from space (from 150 to 290 Kdepending on the percentage of the beam area over oceans, land and clouds). In aconventional repeater,the overall frequency band is down-converted by alocal oscillator (LO) and mixer from the uplink band to the downlink band. It is channelized by an input multiplexer into a number (e.g., 12) of transponder channels. Aseparate high-power ampliﬁer is typically used for each of these channelized signals. Traveling wave tube ampliﬁers (TWTA) are typically used for very highpowers, e.g., up to . 200 Wfor aDBS. Solid-state ampliﬁers at can provide more than 15 WatC-and Ku-Bands.   The attitude determination and controlsystem (ADCS) must maintain the proper angular orientation of the satellite in its orbit in ordertokeepthe antennas pointed to the Earth and the solar arrays aimed toward the sun (for example). The twoprevalent stabilization methods are spin stabilization and body stabilization. In the former,the satellite body spins, and the angular momentum maintains gyroscopic stiffness. The latter uses momentum wheels to keep the spacecraft body orientation ﬁxed. Components of this subsystem include the momentum wheels, torquers (which interact with the Earth’s magnetic ﬁeld), gyros, sun and Earthsensors, and thrusters to maintain orientation.   The telemetry tracking and command (TT&C) subsystem receives data from the ground and enables functions on the satellite to be activated by appropriate codes transmitted from the ground. This system operates with low data rates and requires omni-directional antennas to maintain ground contact in the event the satellite loses its orientation.   The power subsystem comprises batteries and asolar array.The solar array must provide enoughpower to drivethe communications electronics as well as the housekeeping functions, and it must also haveenough capacity to charge the batteries that powerthe satellite during eclipse, that is, when it is shadowedand receives no power from the sun [Richharia, 1999; Grifﬁn and French,2004]. Typical batterytechnologyuses Nickel- Hydrogen cells, which can provide apower densityofmorethan 50 W-h/kg.Silicon solar cells can yield more than 170 W/m2 at asatellite’s beginning of life (BOL). Gallium Arsenide solar cells (GaAs) yield morethan 210 W/m2 .However,they are moreexpensivethan silicon cells.   The space environment including radiation, thermal, and debris issues are described in Wertzand Larson [1991], Grifﬁn and French [2004], Committee on Space Debris 1995, and Johnson and McKnight [1991]. The structuremust supportall the functional componentsand withstand the rigors of the launch environment. The thermal subsystem must control the radiation of heat to maintain arequired operating temperaturefor critical electronics [Gilmore, 1994].7 -16                                  Broadcasting and Optical Communication Technology  7.13     Trends  Satellite communications must now be viewed as amature industry. Satellites lost competitiveness for point- to-point voicetrafﬁc compared with ﬁber and terrestrial mobile systems. Satellites do best when they exploit their unique wide view of the Earth for such applications as broadcast, data services to wide areas, and mobile communications. Video will represent alarge component of satellite capacitydemand, and as HDTV increases, direct broadcast is expected to be the largest user of satellite capacity.   During the 1990s and early 2000s, considerable speculation of expensivesystems such as Iridium, Globalstar,ICO,and Teledesic, along with an excess of GEO capacityrelative to demand, forced the industry into adownturn. This was partly fueled by the growing perception that satellites were risky,thereby resulting in lack of favor among investors. Ka-band systems for broadband data did not materialize as expected and werefound to be costly relative to emerging cable and DSL. Yet, some services can be best provided by satellite systems, and satellites will ﬁnd applications, even for broadband data services such as satellite Internet using the Ku-and Ka-bands, perhaps even at higher frequencies. Satellite construction has evolved from acraft industrywith extensive custom design, long lead times, long test programs, and highcost towardreduced design and construction cycle times, althoughcaremust be taken to ensurereliability. Technologyadvances include development of light-weightsmall satellites for economical provision of data, communications services at low cost, and improvedcomponents.  Deﬁning   Terms Attitude: The angular orientation of asatellite in its orbit, characterized by roll(R), pitch (P), and yaw(Y).      The roll axis points in the direction of ﬂight, the yawaxis points toward the Earth’s center,and the pitch      axis is perpendicular to the orbit plane such that R · P ! Y. ForaGEO satellite, rollmotion causes      north-south beam pointing errors, pitch motion causes east-west pointing errors, and yawcauses a      rotation about the subsatellite axis. Backoff: Ampliﬁers are not linear devices when operated near saturation. To reduce intermodulation      products for multiple carriers, the drivesignal is reduced or backed off. Input backoffisthe decibel      difference between the input powerrequired for saturation and that employed. Output backoff refers to      the reduction in output powerrelative to saturation. Beam  and polarization isolation: Frequency reuse allocates the same bands to several independent      satellite transponder channels. The only waythese signals can be kept separate is to isolate the antenna      response for one reuse channel in the direction or polarization of another.The beam isolation is the      couplingfactor for each interfering path and is always measured at the receiving site, that is, the satellite      for the uplink and the Earth terminal for the downlink. Bus:  The satellite bus is the ensemble of all the subsystems that supportthe antennas and payload      electronics. It includes subsystems for electrical power, attitude control, thermal control, TT&C, and      structures. Frequency reuse: Away to increase the effective bandwidth of asatellite system when available spectrum is      limited. Dual polarizations and multiple beams pointing to different Earth regions may utilize the same      frequencies as long as, for example, the gain of one beam or polarization in the directions of the other      beams or polarization (and vice versa) is low enough. Isolations of 27 to 35 dB are typical for reuse      systems.  References W.S. Adams and L. Rider,‘‘Circular polar constellations providing continuous single or multiple coverage      aboveaspeciﬁed latitude,’’ J. Astronaut. Sci.,vol. 35, no.2,pp. 155–192, 1987. G. Bjo¨ rnstrom, ‘‘Digital payloads: enhancedperformancethroughsignal processing,’’ ESA J.,17, 1–29,      1993.Satellites and Aerospace                                                           7 -17  R.D.Briskman, ‘‘Satellite radio technology,’’ in 16th Int. Commun. Satell. Syst. Conf.,Washington,      DC: American Institute of Aeronautics and Astronautics, Febrauary25–29, 1996, pp.821–825. A. Chobotov, Orbital Mechanics,2nd ed., Washington, DC: American Institute of Aeronautics and      Astronautics, 1991. A.C. Clarke, ‘‘Extra terrestrial relays,’’ Wireless World,October,1945. S. De Gaudenzi, F. Gianetti, and M. Luise, ‘‘Advances in satellite CDMA transmission for mobile and personal      communications,’’ Proc. IEEE,vol. 84, no.1,pp. 18–39, 1996. Committee on SpaceDebris, National Research Council, Orbital Debris,Washington, DC: National Academy      Press, 1995. B. Ebert, The Satellite Communication Ground Segment and Earth Station Handbook,Wiley,2000. K. Feher, Digital Communications: Satellite/Earth Station Engineering,Englewood Cliffs, NJ: Prentice-Hall,      1983. K. Feher, Advanced Digital Communications,Englewood Cliffs, NJ: Prentice Hall, 1987. S. Finkelstein and S.H. Sanford, ‘‘Learning fromcorporate mistakes: The rise and fall of iridium,’’ Org.      Dynamics,29(2): 138–148, 2000. Futron Corporation, ‘‘The Transformation of the Satellite ServicesIndustry,’’ January2005, www.futron.com M. Gagliardi, Satellite Communications,New York: VanNostrand Reinhold, 1991. D.G. Gilmore, Ed., Satellite Thermal Control Handbook,ElSegundo,CA: The AerospaceCorporation Press,      1994. G. Gordon and W. Morgan, Principles of Communications Satellites,New York: Wiley,1993. M.D.Grifﬁn and J.R. French, Space Vehicle Design,2nd ed., Reston, VA:American Institute of Aeronautics and      Astronautics, 2004. A. Inkpen, M. Martin, and I. Fas-Pacheo,The Rise and Fall of Iridium, Thunderbird, the American Graduate      School of International Management: 2000. L.J.Ippolito, Radiowave Propagation in Satellite Communications,New York: VanNostrand Reinhold, 1986. L.J.Ippolito, Propagation Effects Handbook for Satellite Systems Design,NASA, 2000. J. Isakowitz, International Reference Guide to Space Launch Systems,3rd ed., Reston, VA:American Institute of      Aeronautics and Astronautics, 1999. ITV Handbook on Satellite Communications, International Communications Union, 2002. K.G. Johannsen, ‘‘Mobile P-service satellite system comparison,’’ Int.J.Satell. Commun.,13, 453–471,      1995. N.L. Johnson and D.S. McKnight, Artiﬁcial Space Debris,Malabar,FL: Krieger Publishing Co., 1991. G. Maral, VSAT Networks,2nd ed., NewYork: Wiley,2003. G. Maral and M. Bousquet, Satellite Communication Systems,4th ed., John Wiley &Sons, 2002. K. Miya, Ed., Satellite Communications Technology ,Tokyo:KDD Engineering and Consulting,Inc., 1985. W.L. Morgan and G.D.Gordon, Communications Satellite Handbook,New York: Wiley,1989. Ofﬁce of Scienceand Technology Policy,Interagency Report on Orbital Debris, (LibraryofCongress Catalog      No.95-72164), November,1995. J.J. Pocha, An Introduction to Mission Design for GeostationarySatellites,Dordrecht, The Netherlands:      D. Reidel, 1987. T. Pratt,C.W.Bostian, and J.E. Allnut, Satellite Communications,2nd ed., NewYork: Wiley,2000. W.L. Pritchard, H.G. Suyderhoud, and R.A. Nelson, Satellite Communications Systems Engineering,2nd ed.,      Englewood Cliffs, NJ: Prentice-Hall,1993. D.W.E. Rees, Satellite Communications: The First Quarter CenturyofService,New York: Wiley,1990. M. Richharia, Satellite Communications Systems,2nd ed., NewYork: McGraw-Hill,1999. D. Roddy, Satellite Communications,3rd ed., NewYork: McGraw-Hill,2001. M. Schwartz, Information Transmission, Modulation, and Noise,New York: McGraw-Hill,1990. A. Scott, Understanding Microwaves,New York: Wiley,1993. O. Shimbo, Transmission Analysis in Communications Systems,vol. 1&2, NewYork: Computer Science Press,      1988. B. Sklar, Digital Communications,2nd ed., Englewood Cliffs, NJ: Prentice Hall, 2001.7 -18                                  Broadcasting and Optical Communication Technology  J.G. Walker, Continuous Whole-Earth Coverage by Circular Orbit Satellite Patterns,Technical Report77044,      Royal Aircraft Establishment, Farnborough, Hants, U.K. 1977. J.R. Wertz, Ed., Spacecraft Attitude Determination and Control,Dordrecht, The Netherlands: D. Reidel      Publishing Co., 1978. J.R. Wertzand W.J. Larson, Eds., Space Mission Analysis and Design, 3rd ed., Dordrecht, The Netherlands:      Kluwer Academic Publishers, 1999.  Further  Information Extensivesummaries of satellites, frequencies, and orbital locations maybefound at www.lyngsat.com. Satmaster,apopular commerciallink budget program may be purchased from Arrowe Technical Services at www.satmaster.com. Aconvenient Webcalculator for Earth terminal pointing and polarization tilt adjustment maybefound  at http://www.sadoun.com/sat/installation/satellite-heading-calculator.htm. For abrief history of satellite communications see Satellite Communications: The First Quarter CenturyofService,byD.Reese, Wiley,1990. Descriptions of manysatellites can be found in D.H. Martin, Communications Satellites,4th ed., Reston, VA,American Institute of Aeronautics and Astronautics, 2000. An overview of trends may be found in T. Iida et al., Satellite Communications in the 21st Century: Trends and Technologies,Reston, VA,American Institute of Aeronautics and Astronautics, 2003. Basic information and analyses for satellite communications maybefound     in The ITV  Handbook  of Satellite Communications.The emerging digital video broadcast standard, DVB S2 is described in the European Telecommunications Standards Institute (ETSI) http://webapp.etsi.org/action/pv/20050215/tr_102376v0101.   Propagation issues are summarized in Propagation Effects Handbook for Satellite Systems Design,2000, NASA.  Manyofthe   organizations mentioned can be accessed throughthe Internet: (www.nasa.gov); International Telecommunications Union (ITU) (www.itu.ch); Inmarsat FCC (www.fcc.gov); Galileo: www.esa.int/export/esaNA/GGGMX650NDC_index_0.html.   Sirius: http://www.ilslaunch.com/launches/cbin/Mission_Overview/proton/sirius3_mo.pdf and www.sirius. com, XM: www.xmradio.com. For satellite communications to low proﬁle in-motion terminals on vehicles, see www.raysat.com.                                                                                8                               Digital           Video Processing                                 8.1 Introduction....................................................................... 8 -1                                    AHistorical Perspective * Video * Image Sequences as                                   Spatiotemporal Data                               8.2  Some Fundamentals............................................................. 8 -4                                    A3-D System * The 3-D Fourier Transform *                                    Moving Images in the Frequency Domain * 3-D Sampling                               8.3 The Perception of Visual Motion .......................................... 8 -10                                    Anatomyand Physiology of Motion Perception *                                    The Psychophysics of Motion Perception *                                   The Effects of EyeMotion                               8.4 Image Sequence Representation ............................................ 8 -14                                    What Does ‘‘Representation’’ Mean? * Spatial/Spatial-Frequency                                    Representations * The Gabor Representation *                                    Spatial/Scale Representations (Wavelets) * Resolution                               8.5 The Computation of Motion................................................ 8 -19                                    The Motion Field * Optical Flow * The Calculation of Optical Flow                               8.6  Image Sequence Compression.............................................. 8 -25                                    Motion CompensatedPrediction/Transform Coders * Todd R. Reed                      Perceptually Based Methods University of Hawaii          8.7 Conclusions ....................................................................... 8 -29  8.1     Introduction  Rapid increases in performanceand decreases in cost of computing platforms and digital image acquisition and displaysubsystems havemade digital images ubiquitous. Continued improvements promise to make digital video as widely used, opening abroad range of new application areas. In this chapter,some of the key aspects of this evolving data type are examined.  AHistorical Perspective The use of image sequences substantially predates modern video displays(see, e.g., [1]). As mightbeexpected, the primaryinitial motivation for using these sequences was the depiction of motion. One of the earlier approaches to motion picture display was invented by the mathematician William George Horner in 1834. Originally called the Daedaleum (after Daedalus, who was supposed to havemade ﬁgures of men that seemed to move), it was later called the Zoetrope(life turning) or the Wheel of Life. The Daedaleum works by presenting aseries of images, one at atime, through slits in acircular drum, as the drum is rotated.   Although this device is very simple, it illustrates some important concepts. First and foremost, the impression of motion conveyed by asequenceofimages is illusory. It is the result in partofaproperty of the human visual system (HVS) referred to as persistenceofvision. An image is perceived to remain for aperiod of time after it has been removed from view.This illusion is the basis of all motion picture displays. When the                                                                                      8 -18 -2                                   Broadcasting and Optical Communication Technology  drum in the device is rotated slowly, the images appear (as they are) adisjoint sequence of still images. As the speed of rotation increases (the images are displayed at ahigher rate), apoint is reached at which motion is perceived, even thoughthe images appear to ‘‘ﬂicker.’’Further increasing the speed of rotation, apoint is reached at which ﬂickerisnolonger perceived (the critical fusion frequency). Finally,the slits in the drum illustrate acritical aspect of this illusion. In order to perceivemotion fromasequenceofimages, the stimulus the individual images represent must be removedfor aperiod of time between each presentation. If not, the sequence of images simply merges into ablur,and no motion is perceived.   These concepts (rooted in the nature of human visual motion perception) are fundamental, and are reﬂected in all motion picture acquisition and display systems.  Video Unlike image sequencesonﬁlm, video is represented as a1-D signal, derived by scanning the camera sensor. The fact that the signal is derivedbyscanning imposes aparticular signal structure, an example of which is shown in Figure8.1 for anoninterlaced system.   In principle, scanning can be done in manyways. The simplest in concept is noninterlaced line continuous scanning (which yields the video signal just discussed). This approach is also referred to as progressive scanning. Viewed in the 2-D plane (either at the camera or display), this approach appears as shown in Figure8.2.   The bandwidth of the resulting video signal is relatively high. Transmitting aframe of 485 lines,1 with a4:3 aspect ratio (NTSC resolution), at 60 frames per second requires roughly twice the available channel bandwidth (6 MHz). Sixtyupdates per second are needed to avoid wide areaﬂicker, dictated by the temporal response of the HVS. One approach to reducing the signal bandwidth is to send half as manysamples (lines). This cannot be accomplished by reducing the frame rate to 30 frames per second, because an unacceptable degreeofﬂickerisintroduced. Reducing the spatial resolution of each frame results in unacceptable blurring. Interlaced scanning is acompromise between the twoapproaches.                                                Frame blanking                                             (for frame retrace)                          Line blanking                         (for line retrace)                              FIGURE 8.1 Anoninterlaced video signal.                                                            Retrace                                                          (not visible)                              FIGURE 8.2 Anoninterlaced scanning raster.    1 NTSC consists of 525 lines, but only , 485 lines are active.Digital Video Processing                                                            8 -3                           1                         3                         5                         7                         519                        521                        523                        525                  FIGURE 8.3 An NTSC frame, formed by interlacing two ﬁelds (2:1 interlace).                          Direction                 Serration due                        of motion                 to interlace                    FIGURE 8.4 The effect of interlace on an edge in horizontal motion.    As used in NTSC television, each completescan (a frame) contains 525 lines and occurs every1/30 s. The frame consists of two ﬁelds (even and odd), 262½ lines each. These ﬁelds are interlacedtoform the frame. Fields are scanned every1/60 s(reducing ﬂicker). Because twoﬁelds are interlaced to form one frame, this is called 2:1 interlace. Twointerlaced ﬁelds (NTSC) are shown in Figure8.3.   Image acquisition and displayvia scanning has several disadvantages. Nonideal aspects of the scanning system (e.g., nonzerospot size), and under some circumstances the act of scanning itself, lead to areduction in vertical resolution belowthat predicted by the sampling theorem. The ratio of the actual to ideal resolution is called the Kell factor k ,0# k # 1. Typical values of k are .6 , k , .8, with interlacedsystems having lower k . Scanning also causes distortion when objects in the scene are in motion. For example, avertical line in motion will result in atilted scanned image (not due to the tilt of the scan line, but because points on the line at the bottom of the screen are reached later than points at the top). Finally,different points in spacewithin the frame do not correspond to the same point in time. Viewed in the spatiotemporal volume, each frame is tilted, with the upper left corner of the frame corresponding to asigniﬁcantly earlier time than the lower right corner. This can makethe accurate analysis of the image sequence difﬁcult.   Interlaced scanning has additional disadvantages. Interlaced display systems suffer from interline ﬂicker (particularly in regions of the image with nearly horizontal structure). Interlacing results in reduced vertical resolution, which increases aliasing.Italso increases the complexityofsubsequent processing or analysis (such as motion estimation). Interoperabilitywithother systems, such as computer workstations (which use noninterlaced displays), is made difﬁcult. Still images extracted from interlacedvideo (‘‘freeze frames’’) are generally of poor quality.   Often only ‘‘freeze ﬁelds’’ are provided. This last point can be seen by considering the case of an edge in horizontal motion (Figure8.4). To merge twoﬁelds to get astill image of reasonable quality, or to get agood progressively scanned sequence from an interlacedone, is anontrivial problem.8 -4                                   Broadcasting and Optical Communication Technology    FIGURE 8.5 An image sequence represented as aspatiotemporal volume, raytraced to exhibit its internal structure.  Image  Sequences   as Spatiotemporal   Data As discussed previously,the scanning process makes the precise speciﬁcation of an image sequence difﬁcult (since everyspatial point exists at adifferent time). Interlacecomplicates matters further.Inthe remainder of this chapter,the simplifying assumption will be made that each point in aframe corresponds to the same point in time. This is analogous to the digitization of motion picture ﬁlm, or the sequencewhich results from aCCD camera with ashutter.Itisareasonable assumption in progressiveorinterlaced video systems when scene motion is slow compared to the frame rate. The series of frames are no longer tilted in the spatiotemporal domain and can be ‘‘stacked’’inastraightforward way to form aspatiotemporal volume(see Figure8.5).  8.2     Some   Fundamentals  Following are some notational conventions and basic principles used in the balanceofthis chapter. Acontinuous sequence is denoted as u ( x , y , t ), v ( x , y , t ), etc., where x , y are the continuous spatial variables and t is the continuous temporal variable. Similarly,adiscrete sequence is denoted as u ( m , n , p ), v ( m , n , p ), etc., where m , n are the discrete (integer) spatial variables and p is the discrete (integer) temporal variable.  A3-D   System As in 1-D and 2-D,a3-D discrete system can be deﬁned as                                  y ð m ; n ; p Þ¼H ½ x ð m ; n ; p Þ                ð 8 : 1 Þ  where H is the system function. In general, this function need be neither linear nor shift invariant. If the system is both linear and shift invariant (LSI), it can be characterized in terms of its impulse response h ( m , n , p ). The linear shift invariant system response can then be written as                             X 1   X 1  X 1               y ð m ; n ; p Þ¼            x ð m 0 ; n 0 ; p 0 Þ h ð m   m 0 ; n   n 0 ; p   p 0 Þ                           m 0 ¼ 1 n 0 ¼ 1 p 0 ¼ 1                          x ð m ; n ; p Þ h ð m ; n ; p Þð8                           : 2 ÞDigital Video Processing                                                            8 -5  where ‘*’ denotes (discrete) convolution. Similarly,for the continuous case,                          Z 1  Z 1 Z 1               g ð x ; y ; t Þ¼       f ð x 0 ; y 0 ; t 0 Þ h ð x   x 0 ; y   y 0 ; t   t 0 Þ d x 0 d y 0 d t 0 ð 8 : 3 Þ                             1   1   1   The 3-D Fourier   Transform The 3-D continuous Fourier transform can be expressed as                                Z 1 Z 1 Z 1                                                      j2p ð x x x þ y x y þ t x t Þ                 F ð x x ; x y ; x t Þ¼     f ð x ; y ; t Þ e    d x d y d t        ð 8 : 4 Þ                                  1   1    1  where j x , j y ,and j t are the spatiotemporal frequencyvariables and f ( x , y , t )isacontinuous spatiotemporal signal. As in the 2-D case, the 3-D Fourier transform is separable:                           Z 1    Z 1   Z 1                                                    j2p x x x   j2p y x y   j2p t x t             F ð x x ; x y ; x t Þ¼       f ð x ; y ; t Þ e d x e d y e    d t      ð 8 : 5 Þ                              1    1     1  Also as in the 1-D and 2-D cases, if                                g ð x ; y ; t Þ¼h ð x ; y ; t Þ * f ð x ; y ; t Þð8    : 6 Þ  then                            G ð x x ; x y ; x t Þ¼H ð x x ; x y ; x t Þ F ð x x ; x y ; x t Þð8 : 7 Þ  If h ( x , y , t )isthe LSI system impulse response, then H ( j x , j y , j t )isthe frequency response of the system.   The spatiotemporal discrete Fourier transform is deﬁned as                                   N X   1 N X   1 N X   1                                                        hm  kn  lp                        v ð h ; k ; l Þ¼     u ð m ; n ; p Þ W N W N W N            ð 8 : 8 Þ                                  m ¼ 0 n ¼ 0 p ¼ 0                                  j2p = N where0# h , k , l # N   1and W N ¼ e .   The inverse transform is                                  1  N X   1 N X   1 N X   1                     u ð m ; n ; p Þ¼         v ð h ; k ; l Þ W   hmW   knW   lp    ð 8 : 9 Þ                                 N 3                   N     N    N                                    h ¼ 0 k ¼ 0 l ¼ 0  where0# m , n , p # N   1.  Moving Images in the Frequency Domain Following the discussion in Ref. [2], amoving monochromeimage can be represented by an intensity distribution f ( x , y , t ). The image is static if f ( x , y , t ) ¼ f ( x , y ,0)for all t .The velocityofthe image can be expressed viathe image velocityvector                                        !                                        r ¼ðr x ; r y Þð8                            : 10Þ8 -6                                   Broadcasting and Optical Communication Technology  If the (initially static) image translates at aconstant velocity !r ,then                               f r ð x ; y ; t Þ¼f ð x   r x t ; y   r y t ; t Þð8    : 11Þ    Consider the case of asimple 2-D ‘‘image’’ f ( x , t ). Let                                                                                      !     x          !     x                               a ¼        and    b ¼    x                          ð 8 : 12Þ                                     t                 x t  where j x and j t are the spatial and temporal frequency variables. Then the transform pair can be written as                                         f ð ~aa Þ!F ð ~bb Þð8                        : 13Þ    Now, translation can be represented as acoordinate transformation                                                                           ! 0    x   r t     !                                  a  ¼       x   ¼  A a                            ð 8 : 14Þ                                           t  where                                                                                        1    r                                     A  ¼        x                                 ð 8 : 15Þ                                            01   and r x is the horizontal speed.   Using the expression for the Fourier transform after an afﬁne coordinate transformation (anycombination of scaling,rotation, and translation),                                                                                  0                                    f  ! a ! F ½ðA   1 Þ T ~bb  ð8                   : 16Þ  where                                                                                              10                                    ð A   1 Þ T ¼                                  ð 8 : 17Þ                                               r x 1  so that                                            F                                f ð x   r x t ; t Þ!ð x x ; x t þ r x x x Þð8        : 18Þ  Example Consider asimple static image with only twocomponents(Figure 8.6). As the image undergoes translation  with horizontal speed r x ,all temporal frequencies are shifted by   r x j x .Spatial frequency coordinates remain unchanged. That is, all frequency components of an image moving with velocity r x lie on aline through the origin, with slope   r x .                                                        !   Extending the analysis to the 3-D case ( f ( x , y , t )), let the velocity r ¼ðr x ; r y Þ .Then                                           F                        f ð x   r x t ; y   r y t ; t Þ!ð x x ; x y ; x t þ r x x x þ r y x y Þð8 : 19ÞDigital Video Processing                                                            8 -7                                           ξ                                          t (temporal frequency)                                          r x ξ                                           x 0                                                 ξ                                                 x 0                                                         ξ x                            − ξ                              x 0                 (spatial frequency)                                   − r ξ                                    x x 0                     FIGURE 8.6 Atwo-component, 1-D signal in translational motion.                                                ξ                                              y                                                       (,ξ ξ ,- r ξ - r ξ )                                                        x 0 y 0 x x 0 y y 0                                                       (,,ξ ξ 0)                                                       x 0 y 0                                                         ξ                                                         x                                (-ξ ,-ξ ,0)                                  x 0 y 0                       (-ξ ,-ξ , r ξ + r ξ )                        x 0 y 0 x x 0 y y 0                                    ξ                                    t                     FIGURE 8.7 Atwo-component, 2-D signal in translational motion.                                                                         ! Each temporal frequency is shifted by the dot product of the spatial frequency vector s ¼ðx x ; x y Þ and the                   ! image velocity vector r ¼ðr x ; r y Þ .Ifthe image was originally static, then                                      !  !                               x t ¼ r ·s ¼ ð  r x x x þ r y x y Þð8                 : 20Þ     Geometrically,the image motion changes the static image transform (which lies in the ( j x , j y )plane) into a spectrum in aplane with slope   r y in the ( j y , j t )plane and   r x in the ( j x , j t )plane. As in the 2-D case, the shifted points lie on aline through the origin. Note that this represents arelatively sparse occupation of the frequency domain (of interest for compression applications). A3-D volume of data has been ‘‘compressed’’ into aplane. This compactness is not observed in the spatiotemporal domain.    In summary, the spectrum of astationaryimage lies in the ( j x , j y )plane. When the image undergoes translational motion, the spectrum occupies an oblique plane which passes throughthe origin. The orientation of the plane indicates the speed and direction of the motion. It is, therefore, possible to associate energyinparticular regions of the frequency domain with particular image velocitycomponents. By ﬁltering speciﬁc regions in the frequency domain, these image velocitycomponents can be detected. As will be seen shortly,other effects (such as the visual impact of temporal aliasing) can also be understood in the frequency domain.  3-D Sampling In its simplest form (regular sampling on arectangular grid, the method used here), 3-D sampling is a straightforward extension of 2-D (or 1-D) sampling (Figure 8.8). Given abandlimited sequence                                            F                                  f ð x ; y ; t Þ!ð x x ; x y ; x t Þð8              : 21Þ8 -8                                         Broadcasting  and Optical Communication     Technology  with                                                                                                                                              F ð x ; x ; x Þ¼0whenever       jjx >  x  ;   x   > x ;  or  jjx 4  x          ð 8 : 22Þ                   x   y  t                       x     x 0  y     y 0          t    t 0  the continuous sequence can be reconstructed from adiscrete set of samples whenever                              x  >  2 x ;   x  >  2 x ;   and    x  >  2 x                      ð 8 : 23Þ                              x s    x 0    y s     y 0          t s    t 0  where x  ; x ; x are the sampling frequencies. Equivalently,the sequence can be reconstructed if the intervals        x s y s z s                             y                                                                             t                           ∆ y                                                                   ∆ t                                                            x                                     ∆ x                          f ( x , y , t )                         FIGURE 8.8  Asampled spatiotemporal signal (image sequence).             FIGURE  8.9  An image sequence with insufﬁciently highsampling in the temporal dimension.Digital Video Processing                                                            8 -9                                             ξ                                            t                                          r ξ    Aline with slope - r ,                                           x x 0 passing through the                                                 origin.                                                            ξ x                              - ξ                 ξ                                x 0                x 0                                      - r ξ                                      x x 0                FIGURE 8.10 Acontinuous, two-component, 1-D signal in translational motion.                 FIGURE 8.11 Areconstruction of asampled 1-D signal with temporal aliasing.  between samples are such that                               1            1                 1                         D x 5    ;   D y 5    ;  and   D t 5                      ð 8 : 24Þ                              2 x          2 x               2 x                                x 0          y 0               t 0   If anyofthe sampling frequencies fall below the speciﬁed rates, the neighboring spectra (replications of the continuous spectrum, produced by the sampling process) overlap,and aliasing results.Acase for which the temporal sampling frequencyistoo low is shown in Figure 8.9. The appearanceofaliasing in the spatial domain, whereitcommonly manifests as ajagged approximation of smooth highcontrast edges, is relatively familiar and intuitive. The effect of sampling at too low arate temporally is perhaps less so.    Consider the earlier simple example of a1-D image with only twocomponents, moving with velocity r x . The continuous case, as derived previously,isshown in Figure8.10. x is the frequency of the static image.                                                         x 0 Suppose this image is sampled along the temporal dimension at asampling frequency x less than the Nyquist                                                                      t s rate ð x ¼ 2 r x Þ ,and the image is reconstructed viaanideal lowpass ﬁlter with temporal cutofffrequencies      t N   x x 0 at plus and minus half the sampling frequency (Figure 8.11). What is the visual effect of the aliased components?8 -10                                  Broadcasting and Optical Communication Technology    As seen previously,the velocity of motion is reﬂected in the slope of the line connecting the components. Forthe situation shown, asinusoidal grid (of the same frequency as the original) moving in the opposite direction,withspeed r x  ðx   x Þ is observed. As the sampling frequency drops, the velocitydecreases,                   x x 0  t N t s eventually reaching zero.Continued reduction in x results in motion in the same direction as the original                                           t s image, increasing in velocityuntil (at x ¼ 0) the velocities of the two components are identical.                                 t s   In the simple example just considered, the image was spatially homogeneous, so that the effects of aliasing wereseen throughout the image. In general, this is not the case. As in the 1-D and 2-D cases, the temporal aliasing effect is seen in regions of the sequence withsufﬁciently hightemporal frequency components to alias.  Circumstances leading to hightemporal frequencies include highvelocity(large values of r x in our simple example) and highspatial frequency components with some degreeofmotion (high x in our example).                                                                        x 0 Higher spatial frequency componentsrequire slowerspeeds to cause aliasing.   Awell-known example of temporal aliasing is the so-called ‘‘wagon wheel’’effect, in which the wheels of a vehicle appear to moveinadirection opposite to that of the vehicle itself. The wheels haveboth highspatial frequency components (due to their spokes) and relatively highrotational velocity. Hence, aliasing occurs (the wheels appear to rotate in reverse). The vehicle itself, however,which is moving moreslowly and is also generally composed of lower spatial frequency components, moves forward (does not exhibit aliasing effects).  8.3     The   Perception    of Visual   Motion  Visual perception can be discussed at anumber of different levels: the anatomyorphysical structure of the visual system; the physiologyorbasic function of the cells involved; and the psychophysical behavior of the system (the response of the system to various stimuli). Following is abrief discussion of visual motion perception. Amoreextensivetreatment can be found in Ref. [3].  Anatomy    and Physiology  of Motion   Perception The retina(the hemispherical surface at the back of the eye) is the sensor surface of the visual system, consisting of two major types of sensor elements. The rods are long and thin structures, numbering approximately 120 million. They provide scotopic (‘‘low-light’’) vision and are highly sensitive to motion. The conesare shorter and thicker,and substantially fewer in number (approximately 6million per retina). They are less sensitivethan the rods, providing photopic (‘‘high-light’’) and color vision. The cones are much less sensitive to motion.   The rodsand cones are arranged in aroughly hexagonal array.However,they are not uniformly distributed over the retina. The cones are packed in the fovea (hence color vision is primarily foveal). The rodsare primarily outside the fovea. As aresult, motion sensitivityishigher outside the fovea, corresponding to the peripheryofthe visual ﬁeld.   Visual information leaves each eyevia the optic nerve.The nerves fromeach eyesplit at the optic chiasma, pass through the lateral geniculate nucleus, and continue to the visual cortex. Information is retinotopically mapped on the cortex (organized as in the original scene, but reversed). Note, however,that the mapping is not one-to-one (one retinal rod or cone to one cortical cell). As mentioned previously,approximately 120 million rods and 6million conesare found in each eye, but only 1million ﬁbers in the associated optic nerve. This 126:1, apparently visually lossless compression, is one of the motivations for studying perceptually inspired image and video compression techniques, as discussed later in this chapter.   To achieve this compression, each cortical cell receives information fromaset of rods and/or cones. This set makes up the receptive ﬁeld for that cell. The response of acortical cell to stimuli at different points in this ﬁeld can be measured (e.g., viaamoving spot of light) and plotted just as one mightplot the impulse response of a 2-D ﬁlter.   Physiologically,nothing mentioned so far seems speciﬁcally adapted to the detection (or measurement) of motion. It mightbereasonable to expect to ﬁnd cells, which respond selectively to,e.g., the direction of motion. There appear to be no such cells in the human retina (althoughother species do haveretinalDigital Video Processing                                                           8 -11                                                     Input ReceptiveField                                                 Directionally Sensitive                                               Subunits                             Spatial and Temporal                                Integration               FIGURE 8.12 Acommon organizational structure for modeling complex cell behavior.                                  Motion                                  ∆ x                        RF1                RF2                                                "Combine" -multiplicative                                                or additive(comparator)                                                depending on the theory.                               ∆ t          C                 FIGURE 8.13 Amechanism for the directionally sensitive detection of motion.   cells that respond in this way); however, cells in the mammalian striate cortex exhibit this behavior (the complex cells).   Howthese cells come to act this wayremains under study.However,most current theories ﬁt acommon organizational structure[4], shown in Figure 8.12. The input receptiveﬁelds are sensitiveboth to the spatial location and spatial frequencyofthe stimulus. The role,ifany,oforientation is not widely agreed upon. The receptiveﬁeld outputs are combined, most likely in anonlinear fashion, in the directionally sensitivesubunits to produceanoutput highly dependent on the direction and/or velocityofthe stimulus. The output of these subunits are then integrated both spatially and temporally.   Consider the hypothetical directionally sensitive mechanism in more detail for the case of rightward moving patterns (Figure 8.13). Forexample, suppose the receptive ﬁelds are symmetric, and Cisacomparator which requires both inputs to be hightooutput ahighvalue. If apattern, which stimulates receptiveﬁeld 1(RF1), movesadistance D x in time D t so that it falls within receptive ﬁeld 2(RF2), then the comparator will ‘‘ﬁre.’’   Although it is simple, such amodel establishes abasic link between moving patterns on the retina and the perception of motion. Additional insight can be obtained by consideringthe problemfrom asystems perspective.  The Psychophysics of Motion Perception Spatial Frequency Response In the case of spatial vision, much can be understood by modeling the visual system as shown in Figure8.14.  The characteristics of the ﬁlter H ( j x , j y )havebeen estimated by determining the threshold visibility of sine wave gratings. The resulting measurements indicate visual sensitivityasafunction of spatial frequency that is8 -12                                  Broadcasting and Optical Communication Technology                                                   Contrast                                                  c ( x , y )                        Luminance                              Brightness                                  g ()           H ( ξξx , y )                      f ( x , y )                            b ( x , y )                                 Nonlinear function Contrast sensitivity                               (typically     function (CSF),                               logarithmic or sometimes called the                               powerlaw).     modulation transfer                                              function (MTF).                      FIGURE 8.14 Asimple block diagram modeling spatial vision.  approximately lowpass in nature. The response peaks in the vicinityof5cycles/degree, and falls offrapidly beyond 10 cycles/degree.    If it were separable (that is, H ( j x , j y )could be determined by ﬁnding H ( j x )and H ( j y )independently), with H ( j x ) ¼ H ( j y ), or isotropic, the spatial response could be characterized viaasingle 1-D function. Althoughthe assumption of separabilityisoften useful, the spatial CSF of the human visual system is not, in fact, separable. It has been shown that visual sensitivityisreduced at orientations other than vertical and horizontal. This may be due to the predominanceofvertical and horizontal structures in the visual environment, leading to the development or evolution of the visual system to be particularly sensitiveat(or conversely,less sensitive away from) these orientations. This is referred to as the ‘‘oblique effect.’’  Temporal Frequency Response The most straightforward approach to extending the abovespatial vision model to include motion is to  modify the CSF to include temporal frequency sensitivity, so that H ( j x , j y )becomes H ( j x , j y , j t ).   One way to estimate the temporal frequency response of the visual system is to measure the ﬂicker response. Althoughthe ﬂicker response varies withintensityand withthe spatial frequency of the stimulus, it is again generally lowpass, with apeak in response in the vicinityof10Hz. The attenuation of the response above 10 Hz increases rapidly,sothat at 60 Hz (the ﬁeld rate of NTSC television) the ﬂicker response is very low.    It is natural, as in the 2-D case, to ask whether the spatiotemporal frequency response H ( j x , j y , j t )is separable with respect to the temporal frequency.There is evidencetobelieve that this is not the case. The ﬂickerresponse curves for highand low spatial frequency patterns do not appear consistent with aseparable spatiotemporal response.  Reconstruction Error To aﬁrst approximation, the data discussed aboveindicate that the HVS behaves as a3-D lowpass ﬁlter,with bandlimits (for bright displays) at 60 cycles/degreealong the spatial frequency axes, and 70 Hz temporally. This approximation is useful in understanding errors, which may occur in reconstructing acontinuous spatiotemporal signal from asampled one. Consider the case of an image undergoing simple translational motion. This spatiotemporal signal occupies an oblique plane in the frequency domain. With sampling,the spectrum is replicated (with periods determined by the sampling frequencies along the respectivedimensions) to ﬁll the inﬁnite 3-D volume. The spectrum of asufﬁciently sampled (aliasing free) image sequence produced in this wayisshown in Figure8.15.   The 3-D lowpass reconstruction ﬁlter (the spatiotemporal CSF) can be approximated as an ideal lowpass ﬁlter,asshown in Figure 8.16. As long as the cube in Figure8.16 completely encloses the spectrum centered at DC, without including neighboring spectra, thereisnoreconstruction error.This case included no aliasing.If aliasing is included (the sample rate during acquisition is too low), the aliased componentswillbevisible only if they fall within the passband of the CSF ﬁlter.Digital Video Processing                                                           8 -13                                             x y                                         x y                                         s      -x t                                                  s                                                                   x x                             -x x                     x x                               s                       s                                     x       -x y                                    t        s                                    s                          x t            FIGURE 8.15 The spectrum of asampled image undergoing uniform translational motion.   FIGURE 8.16 An ideal 3-D,lowpass reconstruction ﬁlter,with cutofffrequencies determined by the spatiotemporal contrast sensitivity funtion.     The above frequency domain analysis explains some important aspects of human visual motion perception. Other observations are not as easily explained in this way, however.AsobservedinRef. [5], perceived motion is local (different motions can be seen in different areas of the visual ﬁeld) and spatial-frequency speciﬁc (individual motion sensors respond differently (selectively) to different spatial frequencies). These two observations suggest an underlying representation that is local in both the spatiotemporal and spatiotemporal- frequency domains. Examples of such representationswillbediscussed in the following subsection.  The Effects of Eye Motion The analysis of motion perception described previously assumed a‘‘passive’’ view.That is, anychange in the pattern of light on the retinal surface is due to motion in the scene. That this is not the case can be seen by8 -14                                  Broadcasting and Optical Communication Technology  considering the manner in which static images are viewed. They are not viewed as awhole, but in aseries of ‘‘jumps’’ from position to position. These ‘‘jumps’’ are referred to as saccades (French for ‘‘jolt’’or‘‘jerk’’).   Even at the positions wherethe eye is ‘‘at rest’’itisnot truly static. It undergoes verysmall motions (microsaccades) of 1–2 min of arc. In fact, the eye is essentially neveratrest. It has been shown that if the eyeis stabilized, vision fades away after about asecond. The relevanceofthis to the current discussion is that althoughthe eye is in constant motion, so that the intensitypatterns on the retinaare constantly changing, when viewing astatic scene no motion is perceived. Similar behavior is observedwhen viewing dynamic scenes [6]. Obviously,however,inthe case of dynamic scenes motion is often perceived (even thoughthe changes in intensitypatterns on the retina are not necessarily greater than for static images).   Twohypotheses might explain these phenomena. The ﬁrst is that the saccades are so fast that they are not sensed by the visual system; however,this does not account for the fact that motion is seen in dynamic scenes, but not static ones. The second is that the motion sensing system is ‘‘turned off’’under some circumstances (the theoryofcorollarydischarge). The basic idea is that the motor signals that control eyemovement are also involved in the perception of motion, so that when intensitypatterns on the retinachange and thereisamotor signal present, no motion is perceived. When intensity patterns change but there is no motor signal, or if there is no change in intensitypatterns but thereisamotor signal, motion is perceived. The latter situation corresponds to the tracking of moving objects (smooth pursuit). The ﬁrst hypothesis (the less plausible of the two) can be easily modeled with temporal linear ﬁlters. The second, moreinteresting behavior can be modeled with asimple comparator network.  8.4     Image   Sequence     Representation  What  Does   ‘‘Representation’’Mean? The term ‘‘representation’’ may require some explanation. Perhaps the best waytodosoistoconsider some examples of familiar representations. For simplicity, 2-D examples will be used. Extension to 3-D is relatively straightforward. The Pixel Representation The pixel representation is so common and intuitivethat it is usually considered to be ‘‘the image.’’ More precisely,however,itisalinear sum of weighted impulses:                                   N X   1 N X   1                        u ð m ; n Þ¼      u ð m 0 ; n 0 Þ d ð m   m 0 ; n   n 0 Þð8  : 25Þ                                  m 0 ¼ 0 n 0 ¼ 0  where u ( m , n )isthe image, u ð m 0 ; n 0 Þ are the coefﬁcients of the representation (numerically equal to the pixel values in this case), and the d ð m   m 0 ; n   n 0 Þ play the role of basis functions. The DFT The next most familiar representations (at least to engineers) is the DFT,inwhich the image is expressed in terms of complex exponentials:                                      1 N X   1 N X   1                           u ð m ; n Þ¼       v ð h ; k Þ W   hmW   kn             ð 8 : 26Þ                                    N 2               N     N                                        h ¼ 0 k ¼ 0  where0#  m , n # N   1and                                                j 2 p = N                                      W N ¼  e                                     ð 8 : 27Þ                                                                                 hm   kn In this case v ( h , k )are the coefﬁcients of the representation and the 2-D complex exponentials W N W N are the basis functions.Digital Video Processing                                                           8 -15    The choice of one representation over the other (pixel vs. Fourier) for agiven application depends on the image characteristics that are of most interest. The pixel representation makes the spatial organization of intensities in the image explicit. Because this is the basis of the visual stimulus, it seems more‘‘natural.’’ The Fourier representation makes the composition of the image in terms of complex exponentials (‘‘frequency components’’) explicit. The two representationsemphasize their respective characteristics (spatial vs. frequency) to the exclusion of all others. If amixtureofcharacteristics is desired, different representations must be used.  Spatial/Spatial-Frequency Representations Anatural mixtureistocombine frequency analysis  with spatial location. An example of a1-D representation of this type (a time/frequency representation) is amusical score. The need to knownot only what the frequency content of asignal is, but whereinthe signal the frequency components exist is common to manysignal, image, and image sequence processing tasks [7]. Avarietyofapproaches [8,9] can be used to develop arepresentationtofacilitate these tasks. The most intuitiveapproach is the ﬁnite- supportFourier transform. The Finite-Support Fourier Transform This approach to local frequency decomposition has been used for manyyearsfor the analysis of time varying signals. In the 2-D continuous case,                                   Z 1 Z 1                                                          0  0                                               0  0   j 2 p ð x x x þ x y y Þ 0 0                     F x ; y ð x x ; x y Þ¼ f x ; y ð x ; y Þ e d x d y            ð 8 : 28Þ                                     1    1  where                                  0  0      0 0       0     0                             f x ; y ð x ; y Þ¼f ð x ; y Þ h ð x   x ; y   y Þð8     : 29Þ  f ð x 0 ; y 0 Þ is the original image, and h ð x   x 0 ; y   y 0 Þ is awindow centered at ( x , y ).   The properties of the transform depend agreat deal on the properties of the window function. Under certain circumstances (i.e., for certain windows) the transform is invertible. The most obvious case is for nonoverlapping (e.g., rectangular) windows.   The windowed transform idea can, of course, be applied to other transforms, as well. An example that is of substantial practical interest is the discrete cosine transform, with arectangular nonoverlapping window:                                                                                  N X   1 N X   1  ð 2 m þ 1 Þ h p ð 2 n þ 1 Þ k p              F ð h ; k Þ¼a ð h Þ a ð k Þ f ð m ; n Þ cos   cos                    ð 8 : 30Þ                               m ¼ 0 n ¼ 0          2 N             2 N  where h , k ¼ 0, 1, ... , N   1,                                      8 r ﬃﬃﬃ                                      >   1                                      >        for  h ¼ 0                                      <>  N                               a ð h Þ¼                                            ð 8 : 31Þ                                      >   r ﬃﬃﬃ                                      >     2                                      :>       otherwise                                           N  a ( k )isdeﬁned similarly,and the windowdimensions are N · N .This transform is the basis for the well known JPEG and MPEG compression algorithms.8 -16                                  Broadcasting and Optical Communication Technology  The  Gabor  Representation This representation was ﬁrst proposed for 1-D signal analysis by Dennis Gabor in 1946 [10]. In 2-D [11], an image can be represented as the weighted sum of functions of the form                                             j 2 p ½ x ð x   x Þþx ð y   y Þ                             g ð x ; y Þ¼gg^ ð x ; y Þ e x 0 0 y 0 0                ð 8 : 32Þ where                                                 hi  2    2                                        1      1 x   x 0 þ y   y 0                             gg^ ð x ; y Þ¼  e 2  s x   s y                        ð 8 : 33Þ                                      2 psx s y   is a2-D Gaussian function, s x and s y determine the extent of the Gaussian along the respective axes, ( x 0 , y 0 )is the center of the function in the spatial domain, and ð x ; x Þ is the center of supportinthe frequency                                                x 0 x 0 domain. Arepresentativeexample of aGabor function is shown in Figure8.17.   Denoting the distancebetween spatial centers as Dand the distancebetween their centers of supportinthe frequency domain as W ,the basis is complete if WD ¼ 2 p .These functions haveanumber of interesting aspects. They achievethe lower limits of the Heisenburguncertaintyinequalities:                                          1              1                               D x D x >   ;  D y D x >                            ð 8 : 34Þ                                     x  4 p         y  4 p   where D x , D y , D j x ,and D j y are the effective widths of the functions in the spatial and spatial-frequency domains. By this measure, then, these functions are optimally local. Their real and imaginaryparts also agree reasonably well with measured receptiveﬁeld proﬁles.The basis is not orthogonal, however.Speciﬁcally,the Gabor transform is not equivalent to the ﬁnite-supportFourier transform with aGaussian window. For a cross-section of the state of the artinGabor transform-based analysis, see [12].  The Derivative of Gaussian Transform In 1987, Young [13] proposed areceptive ﬁeld model based on the Gaussian and its derivatives. These functions, like the Gabor functions, are spatially and spectrally local and consist of alternating regions of          FIGURE 8.17 The real (top) and imaginary(bottom) parts of arepresentative 2-D Gabor function.Digital Video Processing                                                           8 -17  excitation and inhibition in adecaying envelope. Young showedthat Gaussian derivative functions more accurately model the measured receptiveﬁeld data than do the Gabor functions [14].   In Ref. [15], aspatial/spatial-frequency representation based on shifted versions of the Gaussian and its derivatives was introduced (the derivative of Gaussian transform (DGT)). As with the Gabor transform, althoughthis transform is nonorthogonal, with asuitably chosen basis it is invertible. The DGT has signiﬁcant practical advantage over the Gabor transform in that both the basis functions and coefﬁcients of expansion are real-valued.   The family of 2-D separable Gaussian derivativescentered at the origin can be deﬁned as                                  g 0 ; 0 ð x ; y Þ¼g 0 ð x Þ g 0 ð y Þ                                          ¼ e  ðx 2 þ y 2 Þ = 2 s 2                ð 8 : 35Þ                               g m ; n ð x ; y Þ¼g m ð x Þ g n ð y Þ                                         d ð m Þ   d ð n Þ                                      ¼       g ð x Þ  g ð y Þð8                     : 36Þ                                         d x ð m Þ 0 d y ð n Þ 0  This set can then be shifted to anydesiredlocation. The variance s deﬁnes the extent of the functions in the spatial domain. There is an inverse relationship between the spatial and spectral extents, and the value of this variable maybeconstant or mayvarywithcontext.   The 1-D Gaussian derivative function spectra are bimodal (except for that of the original Gaussian, which is  itself aGaussian) with modes centered at ^ O m rad/pixel:                                              p ﬃﬃﬃ                                                m                                        O  ¼                                       ð 8 : 37Þ                                         m     s  where m is the derivative order. The order of derivative necessarytocenter amode at aparticular frequency is therefore                                                 2                                       m ¼ðO  m s Þ                                ð 8 : 38Þ  The Wigner Distribution The previous examples indicate that alocal frequency representation need not haveanorthogonal basis. In fact, it need not even be linear.The Wigner distribution was introduced by Eugene Wigner in 1932 [16] for use in quantum mechanics (in 1-D). In 2-D,the Wigner distribution can be written as                                                           Z 1 Z 1        a     b        a     b                                                                 j 2 p ð axx þ bxy Þ      W f ð x ; y ; x x ; x y Þ¼ fxþ   ; y þ  f   x    ; y    e          d a d b   ð 8 : 39Þ                          1    1      2     2        2     2  wherethe asterisk denotes complex conjugation. The Wigner distribution is real valued, so does not havean explicit phase component (as seen in, e.g., the Fourier transform). Anumber of discrete approximations to this distribution (sometimes referred to as pseudo-Wigner distributions) havealso been formulated.  Spatial/ScaleRepresentations (Wavelets) Scale is aconcept that has provenverypowerful in manyapplications, and may under some circumstancesbe considered as fundamental as frequency.Given aset of (1-D) functions                                                 j                                   W jkð x Þ¼W ð 2 x   k Þð8                         : 40Þ8 -18                                  Broadcasting and Optical Communication Technology  wherethe indices j and k correspond to dilation (change in scale) and translation, respectively, asignal decomposition                                         X  X                                  f ð x Þ¼     b jkW jkð x Þð8                       : 41Þ                                          j  k  emphasizes the scale (or resolution) characteristics of the signal (speciﬁed by j )atspeciﬁc points along x (speciﬁed by k ), yielding amultiresolution description of the signal.    Aclass of functions W jk( x )that haveprovenextremelyuseful are referred to as wavelets. Adetailed discussion of wavelets is beyond the scope of this chapter (see [17–19] for excellent treatments of this topic); however, an importantaspect of anyrepresentation (including wavelets) is the resolution of the representation,and howitcan be measured.  Resolution In dealing with joint representations, resolution is averyimportant issue. It arises in anumber of ways. In discussing the Gabor representation, it was noted that the functions minimized the uncertaintyinequalities, e.g.,                                                  1                                       D x D x >                                   ð 8 : 42Þ                                            x   4 p  Note that it is the product that is minimized. Arbitrarily highresolution cannot be achievedinboth domains simultaneously,but can be traded between the two domains at will. The proper balancedepends on the  application. It should be noted that the ‘‘effectivewidth’’ measures D x , D j x ,etc. (normalized second moment measures) are not the only way to deﬁne resolution. Forexample, the degree of energyconcentration could be used (leading to adifferent ‘‘optimal’’set of functions, the prolate spheroidal functions). The appropriateness of the various measures again depends on the application. Their biological (psychophysical) relevance remains to be determined.   All the previously mentioned points are relevant for both spatial/spatial-frequency and spatial/scale representations(wavelets). Wavelets, however,present some special considerations. Suppose one wishes to compare the resolutions of time/frequency and wavelet decompositions? Speciﬁcally,what is the resolution of amultiresolution method? This question can be illustrated by considering the 1-D case, and examining the behavior of the twomethods in the time-frequency plane (Figure 8.18).    In the time/frequency representation,the dimensions D t and D j t remain the same throughout the time- frequency plane. In wavelet representations the dimensions vary, but their product remains constant. The resolution characteristics of wavelets maylead one to believe that the uncertaintyofawavelet decomposition  mayfall below the bound in Equation (8.42). This is not the case. The tradeoffbetween D t and D j t simply varies. The fundamental limit remains.   FIGURE 8.18 The resolution of atime/frequency representation and awavelet representation in the time-frequency plane.Digital Video Processing                                                           8 -19    Aﬁnal point relates more speciﬁcally to the representation of image sequences. The HVS has aspeciﬁc (bandlimited) spatiotemporal frequency response. Beyondindicating the maximum perceivable frequencies (setting an upper bound on resolution) it seems feasible to exploit this point further,toachieveamore efﬁcient representation. Recalling the relationship between motion and temporal frequency,asurfacewith highspatial frequency components, moving quickly,has hightemporal frequency components. When it is static, it does not. The characteristics of the spatiotemporal CSF may lead us to the conclusions that static regions of an image require little temporal resolution, but highspatial resolution, and that regions in an image undergoing signiﬁcant motion require less spatial resolution (due to the lowered sensitivityofthe CSF), but require hightemporal resolution (for smooth motion rendition).   The ﬁrst conclusion is essentially correct (althoughnot trivial to exploit). The second conclusion, however, neglects eyetracking.Ifthe eyeistracking amoving object, the spatiotemporal frequency characteristics experienced by the viewer are very similar to those in the static case, i.e., visual sensitivitytospatial structureis not reduced signiﬁcantly.  8.5     The Computation of Motion  Many approaches are used for the computation of motion (or,moreprecisely,the estimation of motion based on image data). Before examining some of these approaches in more detail, it is worthwhile to review the relationship between the motion in ascene and the changes observedinanimage of the scene.  The Motion Field The motion ﬁeld [20] is determined by establishing acorrespondencebetween the motion of points in the scene (the real world) and the motion of points in the image plane. This correspondence is found geometrically,and is independent of the brightness patterns in the scene (e.g., the presence or absence of surfacetextures, changes in luminance, etc.).    Consider the situation in Figure8.19. At aparticular instant in time, apoint P image in the image corresponds to some point P object on the surface of an object. The two points are related viathe perspective projection equation. Now, suppose the object point P object has velocity(v x , v y , v z )relative to the camera. The result is a         0  0 velocity ð v x ; v y Þ for the point P image in the image plane. The relationship between the velocities can be found by differentiating the perspective projection equation with respect to time. In this way, avelocityvector can be assigned to each image point, yielding the motion ﬁeld.  Optical Flow Usually,the intensitypatterns in the image move as the objects to which they correspond move. Optical ﬂowis the motion of these intensitypatterns. Ideally,optical ﬂowand the motion ﬁeld correspond; but this is not                   FIGURE 8.19 The motion ﬁeld based on asimple pinhole camera model.8 -20                                  Broadcasting and Optical Communication Technology  always the case. Foraperfectly uniform sphere rotating in front of an imaging system, thereisshading over the surface of the sphere (due to the shape of the sphere), but it does not change with time. The optical ﬂowis zeroeverywhere,while the motion ﬁeld is not. For aﬁxedsphere illuminated by amoving light source, the shading changes with time, althoughthe sphere is not in motion. The optical ﬂowisnonzero, while the motion ﬁeld is zero.   Furthermore,optical ﬂow is not uniquely determined by local information in the changing image. Consider, for example, aregionwith uniform brightness which does not varywith time. The ‘‘most likely’’optical ﬂow value is zero, but (as long as thereare corresponding points of equal brightness in both images) thereare many ‘‘correct’’ ﬂow vectors. What we would likeisthe motion ﬁeld, but what we haveaccess to is optical ﬂow. Fortunately,the optical ﬂowisusually not too different from the motion ﬁeld.  The  Calculation  of Optical Flow Wide varietyofapproaches are used for the calculation of optical ﬂow. The ﬁrst, below, is aconceptually simple yetverywidely used method. This approach is particularly popular for video compression, and is essentially that used in MPEG-1 and 2.  Optical Flow by Block Matching The calculation of optical ﬂowbyblock-matching is the most commonly used motion estimation technique. The basic approach is as follows. Given two successiveimages from asequence, the ﬁrst image is partitioned into nonoverlapping blocks (e.g., 8 · 8pixels in size, Figure8.20(left)). To ﬁnd the motion vector for each block, the similarity(e.g., viamean-squarederror)between the block and the intensities in the neighborhood of that block in the next frame (Figure 8.20(right)) is calculated. The location that shows the best match is considered the location to which the block has moved. The motion vector for the block is the vector connecting the center of the block in frame ntothe location of the best match in frame n þ 1.   The approach is simple, but anumber of things must be consider.The size of the searchneighborhood must be established, which in turn determines the maximum velocitythat can be estimated. The searchstrategy must be decided, including the need to evaluate everypotential match location and the precision withwhich the match locations must be determined (e.g., is each pixel apotential location? Is subpixel accuracy required?). The amount of computation time/power available is acritical factor in these decisions. Even at its simplest, block matching is computationally intensive.Ifmotion estimates must be computed at frame rate (1/30 s) this will haveastrong effect on the algorithm design. Adetailed discussion of these and related issues can be found in Ref. [21].  Optical Flow via Intensity Gradients The calculation of optical ﬂowvia intensitygradients, as proposed by Horn and Shunck [22], is aclassical approach to motion estimation.                                                    Search neighborhood                                                   forthe blockinthe                                                    previous frame                                Frame n              Frame n +1                          FIGURE 8.20 Motion estimation by block matching.Digital Video Processing                                                           8 -21     Let f ( x , y , t )bethe intensityattime tfor the image point ( x , y ), and let r x ( x , y )and r y ( x , y )bethe x and y components of the optical ﬂowatthat point. Then for asmall time interval d t ,                           f ð x þ r d t ; y þ r d t ; t þ d t Þ¼f ð x ; y ; t Þð8    : 43Þ                                |{z}x    |{z}y                                  d x     d y   This single equation is not sufﬁcient to determine r x and r y .Itcan, however,provide aconstraint on the solution. Assuming that intensity varies smoothly with x , y ,and t ,the left hand side of Equation (8.43) can be expanded in aTaylor’sseries:                           q f     q f    q f              f ð x ; y ; t Þþd x þ d y þ d t þ higher-order terms ¼ f ð x ; y ; t Þð8 : 44Þ                           q x     q y    q t  Ignoring the higher order terms, canceling f ( x , y , t ), dividing by d t and letting d t ! 0,                                   q f dx q f dy  q f                                       þ       þ    ¼ 0                            ð 8 : 45Þ                                  q x dt q y dt  q t  or                                     f x r x þ f y r y þ f t ¼ 0                    ð 8 : 46Þ   where f x , f y ,and f t are estimated from the image sequence.   This equation is called the optical ﬂow constraint equation,since it constrains r x and r y of the optical ﬂow. The values of ( r x , r y )which satisfy the constraint equation lie on astraight line in the ( r x , r y )plane. Alocal brightness measurement can identify the constraint line, but not aspeciﬁc point on the line. Note that this problem cannot really be solved via, e.g., adding an additional constraint. It is afundamental aspect of the image data. A‘‘true’’solution cannot be guaranteed, but asolution can be found.   To view this limitation in another way, the constraint equation can be rewritten in vector form, as                                     ð f x ; f y Þ · ð r x ; r y Þ¼  f t             ð 8 : 47Þ                                                                        T so that the component of optical ﬂowinthe direction of the intensitygradient ( f x , f y ) is                                             f                                         q ﬃﬃﬃﬃﬃﬃﬃﬃ1 ﬃ                             ð 8 : 48Þ                                           2    2                                          f x þ f y  However,the component of the optical ﬂowperpendicular to the gradient (along isointensitycontours) cannot be determined. This is amanifestation of the aperture problem.Ifthe motion of an oriented element is detected by aunit that is small compared with the size of the moving element, the only information that can be extracted is the component of motion perpendicular to the local orientation of the element. For example, looking at amoving edge through asmall aperture(Figure 8.21), it is impossible to tell whether the actual motion is in the direction of a or of b .   One waytowork around this limitation is to impose an explicit smoothness constraint. Motion was implicitly assumed smooth earlier,when aTaylor’sexpansion was used and when the higher order terms were ignored. Following this approach, an iterativescheme for ﬁnding the optical ﬂow for the image sequence8 -22                                  Broadcasting and Optical Communication Technology                                                      b                                     Edge                                                                a                                               Aperture                           FIGURE 8.21 An instanceofthe aperture problem.  can be formulated:                                           2      n             n                        n þ 1       n   l f x r x ð k ; l Þ þ l f x f y r y ð k ; l Þ þ l f x f t                  r x ð k ; l Þ ¼ r x ð k ; l Þ         2    2                                                1 þ l ð f x þ f y Þ                                                  n          n                                    n      f x r x ð k ; l Þ þ f y r y ð k ; l Þ f t                            ¼ r x ð k ; l Þ   l f x   2   2                        ð 8 : 49Þ                                              1 þ l ð f x þ f y Þ  and                                                      n           n                            n þ 1      n      f x r x ð k ; l Þ þ f y r y ð k ; l Þ f t                      r y ð k ; l Þ ¼ r y ð k ; l Þ   l f y 2 2                    ð 8 : 50Þ                                                  1 þ l ð f x þ f y Þ  wherethe superscripts n and n þ 1indicate the iteration number, l is aparameter allowing atradeoffbetween  smoothness and errors in the ﬂowconstraint equation, and r x ð k ; l Þ and r y ð k ; l Þ are local averages of r x and r y . The updated estimates are thus the average of the surrounding values, minus an adjustment (which in velocity spaceisinthe direction of the intensitygradient).   The previous discussion relied heavily on smoothness of the ﬂowﬁeld. However,thereare places in image sequenceswhere discontinuities should occur.Inparticular,the boundaries of moving objects should exhibit discontinuities in optical ﬂow.One approach taking advantage of smoothness but allowing discontinuities is to apply segmentation to the ﬂowﬁeld. In this way,the boundaries between regions with smooth optical ﬂow can be found, and the algorithm can be prevented from smoothing over these boundaries. Because of the ‘‘chicken-and-egg’’nature of this method (a good segmentation depends on agood optical ﬂow estimate, which depends on agood segmentation ... ), it is best applied iteratively.  Spatiotemporal-Frequency-Based Methods It was shown in section ‘‘Some Fundamentals’’ that motion can be considered in the frequency domain, as well as in the spatial domain. Anumber of motion estimation methods have been developed with this in mind. If the sequence to be analyzed is very simple (has only asingle motion component,for example) or if motion detection alone is required, the Fourier transform can be used as the basis for motion analysis, as examinedDigital Video Processing                                                           8 -23  in Refs. [23–25]; however,due to the global nature of the Fourier transform, it cannot be used to determine the location of the object in motion. It is also poorly suited for cases in which multiple motions exist (i.e., when the scene of interest consists of morethan one object moving independently), sincethe signatures of the different motions are difﬁcult (impossible, in general) to separate in the Fourier domain. As aresult, although Fourier analysis can be used to illustrate some interesting phenomena, it cannot be used as the basis of motion analysis methods for the majorityofsequences of practical interest.   To identify the locations and motions of objects, frequency analysis localized to the neighborhoods of the objects is required. Windowed Fourier analysis has been proposed for such cases [26], but the accuracyofa motion analysis method of this type is highly dependent on the resolutionofthe underlying transform, in both the spatiotemporal and spatiotemporal-frequency domains. It is known that the windowed Fourier transform does not perform particularly well in this regard. Filterbank-based approaches to this problem have also been proposed, as in Ref. [27]. The methods examined beloweach exploit the frequency domain characteristics of motion, and provide spatiotemporally localized motion estimates. Optical Flow via the 3-D Wigner Distribution. Jacobson and Wechsler [28] proposed an approach to spatiotemporal-frequency,based derivation of optical ﬂowusing the 3-D Wigner distribution (WD). Extending the 2-D deﬁnition given earlier,the 3-D WD can be written as                       Z 1 Z 1  Z 1                                               a      b     t          a     b     t W f ð x ; y ; t ; x x ; x y ; x t Þ¼ fxþ  ; y þ ; t þ   ·f  x    ; y   ; t                            1   1   1      2      2     2          2     2     2                     · e   j2p ð axx þ bxy þ txt Þ d a d b d t                      ð 8 : 51Þ   It can be shown that the WD of alinearly translating image with velocity ~ rr ¼ðr x ; r y Þ is              W f ð x ; y ; t ; x x ; x y ; x t Þ¼d ð r x x x þ r y x y þ x t Þ ·Wf ð x   r x t ; y   r y t ; x x ; x y Þð8 : 52Þ  which is nonzeroonly when r x x x þ r y x y þ x t ¼ 0.   Foralinearly translating image, then, the local spectra W ð x ; x ; x Þ contain energyonly in aplane (as in                                                f x ; y ; t x y t the Fourier case) the slope of which is determined by the velocity. Jacobson and Wechsler proposed to ﬁnd this plane by integrating over the possible planar regions in these local spectra (viaaso-called ‘‘velocitypolling function’’), using the plane of maximum energytodetermine the velocity. Optical Flow Using 3-D Gabor Filters. Heeger [29] proposed the use of 3-D Gabor ﬁlters to determine this slope. Following the deﬁnition discussed for 2-D,a3-D Gabor ﬁlter has the impulse response                                           j2p ½ x ð x   x Þþx ð y   y Þþx ð t   t Þ                        g ð x ; y ; t Þ¼gg^ ð x ; y ; t Þ e x 0 0 y 0 0 t 0 0       ð 8 : 53Þ  where                                                 hi  2    2     2                                      1         1 x   x 0 þ y   y 0 þ t   t 0                      gg^ ð x ; y ; t Þ¼     e 2  s x    s y   s t                 ð 8 : 54Þ                                    3 = 2                                ð 2 p Þ s x s y s t  To detect motion in different directions, afamily of these ﬁlters is deﬁned, as shown in Figure 8.22.   In order to capture velocities at different scales (highvelocities can be thought of as occurring over large scales, because alarge distanceiscovered per unit time), these ﬁlters are applied to aGaussian pyramidal decomposition of the sequence. Given the energies of the outputs of these ﬁlters, which can be thought of as sampling spatiotemporal/spatiotemporal-frequency space, the problem is analogous to that shown in Figure8.23. The slope of the line (corresponding to the slope of the plane which characterizes motion) must be found viaaﬁnite set of observations. In this method, this problem is solved under the8 -24                                  Broadcasting and Optical Communication Technology                                             ξ y                                                                      ξ x                       ξ t                   FIGURE 8.22 The (stylized) power spectra of aset of 3-D Gabor ﬁlters.                                              ξ t                                                                ξ x         FIGURE 8.23 Velocityestimation in the frequency domain viaestimation of the slope of the spectrum.  assumption of arandom textureinput (the plane in the frequency domain consists of asingle constant value).  Optical Flow via the 3-D Gabor Transform.  One shortcoming of aﬁlterbank approach (if the ﬁlters are not orthogonal or do not provide acompletebasis) is the possibilityofloss. Using the 3-D Gabor functions as the basis of atransform resolves this problem. Asequence of dimension N · M · P can then be expressed at  each discrete point ð x m ; y n ; t p Þ as                       XJ   1 KX   1 LX   1 QX   1 RX   1 XS   1         f ð x m ; y n ; t p Þ¼          c x q ; y ; t ; x ; x ; x ·gx ; y ; t ; x ; x ; x ð x m ; y n ; t p Þð8 : 55Þ                                            r s x j y k t l q r s x j y k t l                      j ¼ 0 k ¼ 0 l ¼ 0 q ¼ 0 r ¼ 0 s ¼ 0   where J · K · L · Q · R · S ¼ N · M · P for completeness, the functions g x ; y ; t ; x ; x ; x ð x m ; y n ; t p Þ denote the Gabor                                                        q r s x j y t basis functions with spatiotemporal and spatiotemporal-frequency centers k of l ( x , y , t )and ð x ; x ; x Þ                                                                    q r  s       x j y k t l  respectively,and c x q ; y ; t ; x ; x ; x are the associated coefﬁcients. Note that these coefﬁcients are not found by                  r s x j y k t lDigital Video Processing                                                           8 -25  convolving with the Gabor functions, since the functions are not orthogonal. See [30] for asurveyand comparison of methods for computing this transform.   In the case of uniform translational motion, the slope of the planar spectrum is sought, yielding the optical ﬂowvector !r .Astraightforward approach to estimating the slope of the local spectra [31,32] is to form vectors  of the j x , j y ,and j t coordinates of the basis functions that havesigniﬁcant energyfor each point in the sequence at which basis functions are centered. From equation 20, the optical ﬂowvector and the coordinate        ! !      ! vectors x x ; x y ; and x t at each point are related as                                 ~       ~      ~        !                                xx t ¼ ð r x xx x þ r y xx y Þ¼  Sr                ð 8 : 56Þ             !  ! where S ¼ðx x j x y Þ .AnLMS estimate of the optical ﬂowvector at agiven point can then be found using the pseudo inverse of S:                                                    !                                   !         T   1 T                                   r est ¼ ð S S Þ S x t                           ð 8 : 57Þ    In addition to providing ameans for motion estimation, this approach has also provenuseful in predicting the apparent motion reversal associated with temporal aliasing [33]. Wavelet-Based  Methods.  Anumber of wavelet-based approaches to this problem havealso been proposed. In Refs. [34–37], 2-D wavelet decompositions are applied frame-by-frame to produce multi-scale feature images. This view of motion analysis exploits the multiscale properties of wavelets, but does not seek to exploit the frequency domain properties of motion. In Ref. [38], aspatiotemporal (3-D) wavelet decomposition is employed, so that some of these frequency domain aspects can be utilized. Leduc et al. explorethe estimation of translational, accelerated, and rotational motion viaspatiotemporal wavelets in Refs. [39–44]. Decompositions designed and parameterized speciﬁcally for the motion of interest (e.g., rotational motion) are tuned to the motion to be estimated.  8.6     Image Sequence Compression  Image sequencesrepresent an enormous amount of data (e.g., a2-hour movie at the US HDTV resolution of 1280 · 720 pixels, 60 frames/second progressive, with 24 bits/pixel results in 1194 Gbytes of data). This data is highly redundant, and much of it has minimal perceptual relevance. One approach to reducing this volume of data is to apply still image compression to each frame in the sequence (generally referred to as intraframe coding). Forexample, the JPEG still image compression algorithm can be applied frame by frame (sometimes referred to as Motion-JPEG or M-JPEG). This method, however,does not take advantage of the substantial correlation, which typically exists between frames in asequence.Compression techniques which seek to exploit this temporal redundancy are referred to as interframe coding methods.  Motion Compensated Prediction/Transform        Coders Predictive coding is based on the idea that to the degreethat all or partofaframe in asequence can be predicted, that information need not be transmitted. As aresult, it is usually the case that the better the prediction, the better the compression that can be achieved. The simplest possible predicator is to assume that successive frames are identical (differential coding); however,the optical ﬂow, which indicates the motion of intensity patterns in the image sequence, can be used to improvethe predictor. Motion compensated prediction uses optical ﬂowinformation, together with areconstruction of the previous frame, to predict the content of the current frame.   Quantization (and the attendant loss of information) is inherent to lossy compression techniques. This loss, if introduced strategically,can be exploited to produceahighly compressed sequence,with good visual8 -26                                  Broadcasting and Optical Communication Technology                      f          f                             DCT(f )+n                    N      +   D N                              D N  N                             Σ       DCT       Quantizer                            --                           f                            P N                           Inverse                                                          Quantizer                                   f                                   R N -1                          Motion        Frame   +                                                  Σ                          Compensation  Delay                                               f  +                                                R N        IDCT                            Motion                           Estimation                                      Motion Vectors                         FIGURE 8.24 Ahybrid (predictive/transform) encoder.                 DCT(f )+n                  f  + n ′                  D N N                    D N N                                              +                      Inverse                                     IDCT      Σ                         f R                      Quantizer                                           N                                              +                                            f                                             P N                                  Motion    Motion          Frame                                 Vectors   Compensation    Delay                                                     f                                                     R N-1                            FIGURE 8.25 Apredictive/transform decoder.  quality. Transforms (e.g., the DCT), followed by quantization, provide aconvenient mechanism to introduce (and control) this loss. Following this approach, ahybrid (motion compensated prediction/ transform) encoder and decoderare shown in Figures 8.24 and 8.25. This hybrid algorithm (with the addition of entropy coders and decoders) is the essenceofthe H.261, MPEG-1, MPEG-2, and US HDTV compression methods [45].  Perceptually  Based  Methods Althoughalgorithms such as MPEG exploit the properties of visual perception (principally in the formulation of quantization matrices), it is not especially central to the structureofthe algorithm. There is, for example, no explicit model of vision underlying the MPEG-1 and 2algorithms. In perceptually-based (sometimes called second generation) methods, knowledge of the HVS takesamuch morecentral role. This view of the problem is particularly effective (and necessary) when designing compression algorithms intended to operate at very highcompression ratios (e.g., over 200:1).   The methods in this subsection are inspired by speciﬁc models of visual perception. The ﬁrst is an approach based on averycomprehensivevision model, performing spatial and temporal frequency decomposition via ﬁlters designed to reﬂect properties of the HVS. The second and third are techniques using visually relevant transforms (the Gabor and derivative of Gaussian transforms, respectively) in an otherwise conventional hybrid (predictive/transform) framework. Finally,amethod based on spatiotemporal segmentation (following the contour/texture model of vision) will be discussed. The Perceptual Components  Architecture The perceptual components architecture [46] is aframework for the compression of color image sequences based on the processing thought to take placeinthe early HVS. It consists of the following steps. The input RGBimage sequenceisconverted into an opponent color space (white/black (WB), red/green (RG), and blue/Digital Video Processing                                                           8 -27  yellow (BY)). The sequence is ﬁlteredspatially with aset of frequency and orientation selectiveﬁlters, inspired by the frequency and orientation selectivityofthe HVS. Filters based on the temporal frequency response of the visual system are applied along the temporal dimension. The ﬁlteredsequencesare then subsampled using ahexagonal grid, and subsampled by afactor of twointhe temporal dimension. Uniform quantization is applied within each subband, with higher frequency subbands quantized more coarsely.The WB (luminance) component is quantized less coarsely overall than the RG and BY (chrominance) components. The ﬁrst-order entropy of the result provides an estimate of the compression ratio.   Note that thereisnoprediction or motion compensation. This is a3-D subband coder,wheretemporal redundancy is exploited viathe temporal ﬁlters. For a256 · 256, 8frame segment of the ‘‘football’’sequence (a widely used test sequence depicting aplayfromanAmerican football game), acceptable image quality was achieved for about 1bit/pixel (from 24 bits/pixel). Althoughthis is not veryhighcompression, the sequence used is more challenging than most. Another contributing factor is that the subsampled representation is 8/3 the size (in terms of bits) of the original, which must be overcome before any compression is realized.  Very-Low-Bit-Rate Coding Using the Gabor Transform In discussing the Gabor transform previously,itwas stated that the basis functions of this transform are optimally (jointly) local. In the context of coding,there are three mechanisms that can be exploited to achievecompression, all of which depend on locality: the local correlation between pixels in the sequence; the bounded frequency response of the human visual system (as characterized by the CSF); and visual masking (the decrease in visual sensitivitynear spatial and temporal discontinuities). To take advantage of local spatial correlation, the image representation upon which acompression method is based must be spatially local (which is whyimages are partitioned into blocks in JPEG, MPEG-1&2, H.261, etc.). If the CSF is to be exploited (e.g., by quantizing highfrequency coefﬁcients coarsely) localization in the spatial-frequency domain is required. To exploit visual masking, spatial locality(of afairly highdegree) is required.   The Gabor transform is inherently local in space, so the partitioning of the image into blocks is not required (hence no blocking artifacts are observed at highcompression ratios). Its spatial localityalso provides a mechanism for exploiting visual masking,while its spatial-frequency localityallows the bandlimited nature of the HVS to be utilized.   An encoder and decoder based on this transform are shown in Figures 8.26 and 8.27 [47].   Note that they are in the classic hybrid (predictive/transform) form. This codec does not include motion compensation, and is for monochromeimage sequences.   Applying this method to a128-by-128, 8bit/pixel version of the Miss America sequence resulted in reasonable image qualityatacompression ratio of approximately 335:1.1 At 24 frames per second, the associated bit rate is 9.4 kbits/s (a bitrate consistent, e.g., with wireless videotelephony).  Video Coding Using the Derivative of Gaussian Transform As mentioned previously,the derivativeofGaussian transform (DGT)has properties similar to the Gabor transform, but with the practical advantage that it is real-valued. This makes it particularly well-suited to video compression. In Ref. [48] the hybrid codec structure shown in Figures 8.24 and 8.25 is adapted to the DGT,replacing the DCT (and IDCT), and adapting the quantization scheme to ﬁt the visibility of the DGT basis, viaasimple quantization mask.   Comparable results to those of the standardH.261 (DCT-based) codec are obtained for bitrates around 320 kbits/s (5 channels in the p*64 model). Object-Based Coding by Split and Merge Segmentation Object-based coding reﬂects the fact that scenes are largely composedofdistinct objects, and that these objects are perceived as boundaries surrounding ﬁelds of shading or texture (the contour/texturetheoryofvision).    1 Notincluding the initial frame, which is intracoded to 9.1 kbits (a compression ratio of 14).8 -28                                  Broadcasting and Optical Communication Technology                     f N         f D                          Gabor(f )+n                           +    N   Gabor                       D N N                             Σ     Transform    Quantization                            --                           f                            P N                           Inverse                                                          Quantizer                                      Frame   +                                               Σ                                     Delay                                                           Inverse                                            f R +                                             N             Gabor                                                          Transform                         FIGURE 8.26 AGabor transform-based video encoder.                     Gabor(f D )+n N          f  + n ′                        N                    D N N                                       Inverse                           Inverse             +                                       Gabor    Σ                  f R                           Quantizer                                N                                      Transform +                                              f                                               P N                                                       Frame                                                      Delay                       FIGURE 8.27 The associated Gabor transform-based decoder.                                   FIGURE 8.28 The split phase.  Encoding an image or sequence in this wayrequires segmentation to identify the constituent objects. This view of compression, which also facilitates interaction and editing,underlies the MPEG-4 video compression standard[49]. Although the method that will be described is different in detail from MPEG-4, as one of the earliest documented object-based systems, it illustrates manyimportantaspects of such systems.   In this approach [50], 3-D (spatiotemporal) segmentation is used to reduce the redundant information in a sequence (essentially identifying objects within the sequence),while retaining information critical to theDigital Video Processing                                                           8 -29  human observer. The sequenceistreated as asingle 3-D data volume, the voxels of which are groupedinto regions viasplit and merge. The uniformitycriterion used for the segmentation is the goodness-of-ﬁt to a3-D polynomial. The sequenceisthen encoded in terms of region boundaries (a binarytreestructure) and region interior intensities (the coefﬁcients of the 3-D polynomial).   The data volume is ﬁrst split such that each region is aparallelepiped over which the gray level variation can be approximated within aspeciﬁed mean squarederror (Figure 8.28). Regions are split by quadrants, following the octree strategy. Aregionadjacency graph is constructed, with nodes corresponding to each region and links between the nodes assigned acost indicating the similarityofthe regions. Ahighcost indicates low similarity. Regions are merged, starting with regions withthe lowest cost, and the region adjacency graph is updated. The resulting regions are represented using apyramidal (binarytree) structure, with the regions labeled so that adjacent regions havedifferent labels.   Using 16 frames from the ‘‘Secretary’’sequence,the compression ratio achieved was 158:1 (a bitrate of 83 kbits/s). Atotal of 5740 parallelepipeds (1000 regions) wereused.  8.7     Conclusions  In this chapter,some of the fundamental aspects and algorithms in the processing of digital video were examined. Continued improvements in computing performancemakemanymethods that previously required specialized platforms (or were primarily of research interest due to computational requirements) practical. In addition to bringing high-end applications to the desktop,numerous new applications are thus enabled, in areas as diverse as medical imaging, entertainment, and human-computer interaction.  References  1.  J. Wyver, The Moving Image —AnInternational Histoy of Film, Television and Video,London: BFI      Publishing,1989.  2.  A.B.Watson and A.J.Ahumada, ‘‘Alook at motion in the frequency domain,’’ SIGGRAPH/SIGART      InterdisciplinaryWorkshop MOTION: Representation and Perception,Toronto,Canada, 1983, pp.1–10.  3.  A.T.Smith and R.J.Snowden, Eds., Visual Detection of Motion,San Diego,CA: Academic Press, 1994.  4.  K. Nakayama, ‘‘Biological image motion processing: areview,’’ Vision Res. ,vol. 25, no.5,pp. 625–660,      1985.  5.  A.B.Watson and A.J.Ahumada, ‘‘Model of human visual-motion sensing,’’ J. Opt. Soc. Am.A,vol.2,      no.2,1985.  6.  L.B.Stelmach, W.J. Tam, and P. Hearty,‘‘Static and dynamic spatial resolution in image coding: an      investigation of eyemovements,’’ Proc. SPIE/SPSE Symp.Electron. Imaging Sci. Technol., San Jose, CA,      1995.  7.  T.R. Reed, ‘‘Local frequency representations for image sequence processingand coding,’’in Digital      Images and Human Vision,A.B.Watson, Ed., Cambridge, MA: MIT Press, 1993.  8.  L. Cohen, Time–Frequency Analysis,Englewood Cliffs, NJ: Prentice-Hall, 1995.  9.  R. Tolimieri and M. An, Time–Frequency Representations,Boston, MA: Birkha¨ user,1998. 10.  D. Gabor,‘‘Theoryofcommunication,’’ Proc. Inst. Electr. Eng.,vol. 93, no.26, pp.429–457, 1946. 11.  J.G. Daugman, ‘‘Complete discrete 2-D Gabor transforms by neural networks for image analysis and      compression,’’ IEEE Trans. Acoust. Speech Signal Process.,vol.36, no.7,pp. 1169–1179, 1988. 12.  H.G. Feichtinger and T. Strohmer Eds., Gabor Analysis and Algorithms,Boston, MA: Birkha¨ user,1998. 13.  R.A. Young,‘‘The Gaussian derivative model for spatial vision: I. Retinal mechanisms,’’ Spatial Vision,      vol. 2, pp.273–293, 1987. 14.  R.A. Young,‘‘Oh say can you see? The physiologyofvision,’’ Proc. SPIE-Human Vision, Vis. Process.      Digit. DisplayII ,vol. 1453, 1991, pp.92–123. 15.  J.A. Bloom and T.R. Reed, ‘‘AGaussian derivative-based transform,’’ IEEE Trans. Image Process.,vol. 5,      no.3,pp. 551–553, 1996. 16.  E. Wigner,‘‘On the quantum correction for thermodynamic equilibrium,’’ Phys. Rev. ,40, 749–759, 1932.8 -30                                  Broadcasting and Optical Communication Technology  17.  M. Vetterli and J. Kovac˘ vic, Wavelets and Subband Coding,Englewood Cliffs, NJ: Prentice-Hall,      1995. 18.  G. Strang and T. Nguyen, Wavelets and Filter Banks,Wellesley,MA: Wellesley-Cambridge Press, 1996. 19.  S. Mallat, AWavelet Tour of Signal Processing,San Diego,CA: Academic Press, 1998. 20.  B.K.P.Horn, Robot Vision,Cambridge, MA: MIT Press, 1986. 21.  G. Tziritas and C. Labit, Motion Analysis for Image Sequence Coding,Amsterdam:Elsevier,1994. 22.  B.K.P.Horn and B.G. Shunck, ‘‘Determining optical ﬂow,’’ Artif. Int.,vol. 17, no.1–3, pp.185–203,      1981. 23.  H. Gafni and Y.Y. Zeevi, ‘‘Amodel for separation of spatial and temporal information in the visual      system,’’ Biol. Cybern. ,28, 73–82, 1977. 24.  A. Kojima, N. Sakurai, and J. Kishigami, ‘‘Motion detection using 3D-FFT spectrum,’’ Proc. IEEE Int.      Conf. Acoust., Speech Signal Process., Minneapolis, MN, vol. V, 1993, pp.213–216. 25.  B. Porat and B. Friedlander,‘‘A frequency domain algorithm for multiframe detection and estimation of      dim targets,’’ IEEE Trans. PatternAnal. Mach. Int. ,vol. 12, no.4,pp. 398–401, 1990. 26.  H. Gafni and Y.Y. Zeevi, ‘‘Amodel for processing of movement in the visual system,’’ Biol. Cybern. ,32,      165–173, 1979. 27.  D.J. Fleet and A.D.Jepson, ‘‘Computation of component image velocityfromlocal phase information,’’      Int. J. Comput. Vision,vol. 5, no.1,pp. 77–104, 1990. 28.  L. Jacobson and H. Wechsler,‘‘Derivation of optical ﬂow using aspatiotemporal-frequency approach,’’      Comput. Vision, Graph. Image Process.,38, 29–65, 1987. 29.  D.J. Heeger,‘‘Optical ﬂow using spatiotemporal ﬁlters,’’ Int. J. Comput. Vision,vol. 1, no.4,pp. 279–302,      1988. 30.  T.T. Chinen and T.R. Reed, ‘‘Aperformanceanalysis of fast Gabor transforms,’’ Graph. Models Image      Process.,vol. 59, no.3,pp. 117–127, 1997. 31.  T.R. Reed, ‘‘The analysis of motion in natural scenes using aspatiotemporal/spatiotemporal-frequency      representation,’’ Proc. IEEE Int.Conf. Image Process., Santa Barbara, CA, 1997, pp.I-93–I-96. 32.  T.R. Reed, ‘‘On the computation of optical ﬂow using the 3-D Gabor transform,’’ Multidimens. Sys.      Signal Process.,vol.9,no. 4, pp.115–120, 1998. 33.  T.R. Reed, ‘‘Aspatiotemporal/spatiotemporal-frequency interpretation of apparent motion reversal,’’      Proc. 16th Int.Joint Conf. Artif. Int., Stockholm, Sweden, vol.2,1999, pp.1140–1145. 34.  J. Magarey and N. Kingsbury, ‘‘Motion estimation using acomplex-valued wavelet transform,’’ IEEE      Trans. Signal Process.,vol.46, no.4,pp. 1069–1084, 1998. 35.  Y.-T.Wu, T. Kanade, J. Cohn, and C.-C. Li, ‘‘Optical ﬂow estimation using wavelet motion model,’’ Proc.      IEEE Int. Conf. Comput. Vis., Bombay,India, 1998, pp.992–998,. 36.  G.Van der Auwera, A. Munteanu, G. Lafruit, and J. Cornelis, ‘‘Video coding based on motion estimation      in the wavelet detail image,’’ Proc. IEEE Int.Conf. Acoust. Speech Signal Process., Seattle, WA,vol. 5, 1998,      pp.2801–2804. 37.  C.P.Bernard, ‘‘Discrete wavelet analysis: anew frameworkfor fast optic ﬂowcomputation,’’ Proc. ﬁfth      Eur. Conf. Comput. Vision,Freiburg,Germany, vol. 2, 1998, pp.354–368. 38.  T.J. Burns, S.K. Rogers, M.E. Oxley,and D.W. Ruck, ‘‘Discrete, spatiotemporal, wavelet multiresolution      analysis method for computing optical ﬂow,’’ Opt. Eng.,vol.33, no.7,pp. 2236–2247, 1994. 39.  J.-P.Leduc, ‘‘Spatio-temporal wavelet transforms for digital signal analysis,’’ Signal Process.,vol.60,      no.1,pp. 23–41, 1997. 40.  J.-P.Leduc, J. Corbett, M. Kong,V.Wickerhauser,and B. Ghosh, ‘‘Accelerated spatio-temporal wavelet      transforms: an iterative trajectoryestimation,’’ Proc. IEEE Int.Conf. Acoust. Speech Signal Process.,      Seattle, WA,vol.5,1998, pp.2781–2784. 41.  J.-P.Leduc, J.R. Corbett, and M.V.Wickerhauser,‘‘Rotational wavelet transforms for motion analysis,      estimation and tracking,’’ Proc. IEEE Int. Conf. Image Process., Chicago,IL, vol.2,1998, pp.195–199. 42.  J.-P.Leduc and J.R. Corbett, ‘‘Spatio-temporal continuous wavelets for the analysis of motion      on manifolds,’’ Proc. IEEE-SP Int.Symp.Time–Freq. Time–Scale Anal., Pittsburgh, PA,1998,      pp.57–60.Digital Video Processing                                                           8 -31  43.  M. Kong,J.-P.Leduc, B.K. Ghosh, J. Corbett, and V.M. Wickerhauser,‘‘Wavelet-based analysis of      rotational motion in digital image sequences,’’ Proc. IEEE Int. Conf. Acoust. Speech Signal Process.,      Seattle, WA,vol.5,1998, pp.2777–2780. 44.  J. Corbett, J.-P.Leduc, and M. Kong,‘‘Analysis of deformational transformations with spatio-temporal      continuous wavelet transforms,’’ Proc. IEEE Int. Conf. Acoust. Speech Signal Process., Phoenix, AZ, vol. 6,      1999, pp.3189–3192. 45.  A.N. Netravali and B.G. Haskell, Digital Pictures—Representation, Compression, and Standards ,      NewYork: Plenum Press, 1995. 46.  A.B. Watson and C.L.M. Tiana, ‘‘Color motion video coded by perceptual components,’’ in SID1992      Digest Tech. Pap.,vol. XXIII, 1992, pp.314–317. 47.  T.R. Reed and A.E. Soohoo,‘‘Very-low-bit-rate coding of image sequences using the Gabor transform,’’      J. Soc. Inf. Display ,vol.3,no. 2, 1995. 48.  J.A. Bloom and T.R. Reed, ‘‘On the compression of video using the derivativeofGaussian transform,’’      Proc. 32nd Ann. Asilomar Conf. Signals Syst. Comput., Paciﬁc Grove, CA, 1998, pp.865–869. 49.  R. Koenen, Ed., MPEG-4 Overview,ISO/IEC JTC1/SC29/WG11 N3747, La Baule, 2000. 50.  P. Willemin, T.R. Reed, and M. Kunt, ‘‘Image sequence coding by split and merge,’’ IEEE Trans.      Commun.,vol.39, no.12, 1991.This page intentionally left blank                                                                                 9                                        Low Sample Support                                          Adaptive Parameter                                                     Estimation and                                                            Packet-Data                                       Detection for Mobile                                              Communications1                                 9.1 Introduction........................................................................ 9 -2                               9.2 Basic Signal Model................................................................ 9 -3                               9.3 Data Processing with Known Input Statistics ............................ 9 -5                                    Optimum MMSE/MVDRFilter * Generalized Sidelobe                                   Canceller (GSC)                               9.4 Auxiliary-Vector (AV) Filters .................................................. 9 -7 Haoli Qian                    9.5 Disjoint Parameter Estimation and Packet-Data Detection........ 9 -10 Marvell Semiconductor Inc.                                   Known Channel * Unknown Channel Stella N. Batalama            9.6 Joint Parameter Estimation and Packet-Data Detection ............ 9 -21                                   GLRTDetection: Known Channel * GLRTDetection: SUNY at Buffalo                                   Unknown Channel * ImplementationIssues * Dimitri Kazakos                   SimulationStudies University of Idaho           9.7 Concluding Remarks ........................................................... 9 -27   In this chapter,weconsider the problemofdesigning low sample support(packet-rate) adaptivereceivers for mobile communications. We examine both disjoint and joint conﬁgurations whereestimations of receiver parameters and packet-data detection are performed either in two separate stages or in acoupled, joint manner,respectively. Forthe ﬁrst case, we focus on minimum mean squareerror/minimum variance distortionless response (MMSE/MVDR)-type receivers, while for the second case, we develop generalized likelihood ratio test (GLRT)-typedetectors. Complete adaptive antenna-array DS-CDMA receiver structures are developed. Receiver performanceismeasured in terms of output signal-to-interference-plus-noise-ratio and bit-error-rate.    1 Part of this work appeared in Ref. [1] and is reprinted with permissionofJohnWiley &Sons Inc.                                                                                      9 -19 -2                                   Broadcasting and Optical Communication Technology  9.1    Introduction  The effectiveness of areceiver designed for arapidly changing multiple-access (multiuser) communications environment depends on the following design attributes: (i) system adaptivity under limited data support, (ii) multiple-access-interference resistance, and (iii) low computational complexity. Short- data-recordadaptivedesigns appear as the natural next step for amatureddiscipline that has extensively addressed the other twodesign objectives, (ii) and (iii), in ideal setups (perfectly known or asymptotically estimated statistical properties). System adaptivitybased on shortdata records is necessaryfor the development of practical adaptive receivers that exhibit superior signal-to-interference- plus-noise ratio (SINR) or bit-error-rate (BER) performancewhen they operate in rapidly changing communications environments that limit substantially the input data supportavailable for adaptation and redesign.   In modern packet data transmission systems wherethe basic information ﬂow unit is the packet (a group of bits that includes the actual information bits as well as other coding and networkcontrol bits), the main measureoflink qualityisthe throughput (either packet throughput or information throughput) that is directly related to the packet-error-rate (PER). Real-time voicecommunications impose stringent delayconstraints and require aguaranteed upper bound on PER of about 10  2 .Onthe other hand, data packets can tolerate reasonable delays, but may require alowerPER bound. Packet throughput improvements can be achieved as aresult of bit-error-rate (BER) improvements. On the other hand, BER improvements can be achievedbymeans of advanced receiver designs that exploit both the characteristics of the transmitted signal and the current state of the environment. In dynamic environments, adaptivereceiver designs can reacttovariations, as opposed to static receivers that remainunchanged regardless of the changes in the environment. Inherently, amajor consideration in the design of successful adaptivereceivers is their adaptation rate must be commensurate to the rate of change of the environment.   An example of asystem that can beneﬁt from modern advanced adaptive receiver technologyisthe direct-sequence code-division-multiple-access (DS-CDMA) radio frequency (RF) system. In such asystem, the transmitted signal is aspread-spectrum (SS) signal that is obtained by multiplying each information bit by aunique code(or signature)waveform dedicated to each user.The SS characteristics of the transmitted signal allow intelligent temporal (code) processing at the receiver (unmasking of the signature). During RF transmission, the signal undergoes aprocess known as multipath-fading that is dictated by the physical characteristics of the communication channel. As aresult, the received signal consists of multiple faded and delayed copies of the transmitted signal. At the receiver,the multiple copies, instead of being discarded as interference,can be processed in an advantageous manner (a procedureknown as RAKE processing). Further performanceimprovements can be obtained by exploiting the spatial characteristics of the transmitted signal; such processing requires that antenna-array(‘‘smart-antenna’’)technologyis employed at the receiver.This general DS-CDMA signal model example will be revisited manytimes throughout our discussion, and acomplete adaptiveantenna-arrayDS-CDMA receiver will be developed as an illustration.   Returning to the main topic of our discussion, an adaptivereceiver consists of aset of building blocks that are re-evaluated (estimated) everytime thereisasigniﬁcant change in the statistics of the environment. Packet-rateadaptive receivers that aim at the detection of the information bits within adata packet and assume limited or no knowledge of the time varying environment can be categorized as follows: (i) Receivers that perform estimation and detection disjointly,and (ii) receivers that perform estimation and packet-data detection jointly.Ineither case, both estimation and detection are based on the same data packet (it is assumed that the data packet size is commensurate to the rate of change of the environment, which implies that within adata packet the environment remains unchanged).   Forclass (i), the design of the receiver building blocks is initially formulated mathematically as asolution to an optimization problem, under the assumption that all statistical quantities are perfectly known.This is known as the ideal or optimum solution. Then, the statistical quantities that are present in the optimum solution areLow Sample Support Adaptive Parameter Estimation and Packet-Data Detection          9 -3  substituted by corresponding estimates that are based on the received data packet, to generate an estimate of the optimum solution. The short-data-record performanceofanestimation algorithm directly determines the performanceofthe receiver in atime varying channel. In class (ii), receivers are implemented by solving ajoint, two-step,coupled optimization problem. Forexample, optimization with respect to the unknown parameters is carried out ﬁrst, and results in asolution that is parameterized by the data bits. Then, the parameterized solution is substituted into the original criterion function. Optimization is nowperformed with respect to the data bits only.Such an approach avoids aseparate estimation step for the unknown parameters, but often results in a receiver algorithm that exhibits higher computational complexity. We note that the time varying nature of the channel frequently necessitates fast (short-data-record) adaptiveoptimization through the use of small input data sets that can ‘‘catch up’’ with the channel variations. In the following sections, we will study the short-data- recordperformanceofvarious adaptivereceiver structures that perform either disjoint or joint parameter estimation and packet-data detection. Examples include sample-matrix-inversion (SMI), least-mean-square (LMS), recursive-least-square(RLS) and auxiliary-vector (AV)-typeadaptivelinear receivers, as well as generalized likelihood ratio test (GLRT)-typepacket data receivers.   To explain further the developments presented in this chapter,let us consider aDS-CDMA system with 5-element antenna-array reception, system processing gain 64, and 3resolvable multipaths for the user signal of interest (usually the number of resolvable multipaths is between 2and 4, including the direct path, if any). Forsuch asystem, we will see later that jointly optimal space-time (S-T)processing at the receiver under the the minimum-mean-square-error(MMSE) criterion requires processing in the 5 ð 64 þ 3   1 Þ¼330 space- time product space. That is, optimization needs to be carried out in the complex C 330 vector space. We know that adaptiveSMI implementations of the MMSE receiver require data samples manytimes the space-time product to approach the performancecharacteristics of their ideal counterpart(RLS/LMS implementations behave similarly). In fact, theoretically,system optimization with data samples less than the space-time product may not even be possible, as we will explain later in our discussion. With CDMA chip rates at 1.25 MHz, processing gain 64, and typical fading rates of 70 Hz or morefor vehicle mobiles, the fading channel ﬂuctuates decisively at least every280 data symbols. In this context, conventional SMI/RLS/LMS adaptiveﬁlter optimization in the C 330 vector spacebecomes an unrealistic objective.   The goal of our presentation is to introduceand then elaborate on the underlying principles of short-data- recordadaptivereceivers. Throughillustrativeexamples from the mobile communications literature, we will observe that well-designed short-data-record (packet-rate) disjoint or joint parameter estimation and packet- data detection schemes result in improvedchannel BER, which translates to higher packet success probability and higher user capacityfor agiven PER upper bound qualityofservice (QoS) constraint.  9.2    Basic Signal Model  Forillustration purposes, we consider throughout this presentation amultiuser communications system wherebinaryantipodal information symbols from user 0 , user 1 , ···, user Q-1 are transmitted at arate 1 = T by modulating (being multiplied by)asignal waveform, d q ð t Þ ,ofduration, T , q ¼ 0 ; ... ; Q   1, that uniquely identiﬁes each user and is assumed to be approximately bandlimited or havenegligible frequency  components outside acertain bandwidth. If H 1 ð H 0 Þ denotes the hypothesis that the information bit, b 0 ¼þ1 ð b 0 ¼ 1 Þ ,ofthe user of interest, say user 0 ,istransmitted during acertain bit period, T ,then the corresponding equivalent lowpass compositereceived waveform over the bit interval, T ,may be expressed in general as                  p ﬃﬃﬃ                                p ﬃﬃﬃ   H 1 : x ð t Þ¼ðþ1 Þ E 0 v 0 ð t Þþz ð t Þ and H 0 : x ð t Þ¼ð 1 Þ E 0 v 0 ð t Þþz ð t Þ ; 0 < t < T ð 9 : 1 Þ   With respect to the user of interest, user 0 , E 0 denotes transmitted energyper bit, v 0 ð t Þ represents the channel processed version of the original waveform, d 0 ð t Þ (w.l.o.g.the signal waveform d 0 ð t Þ is assumed to be normalized to unit energyoverthe bit period T ), and z ð t Þ identiﬁes comprehensively the channel disturbance9 -4                                   Broadcasting and Optical Communication Technology  and includes one, some, or all of the following forms of interference: ( i )MAI, ( ii)inter-symbol interference (ISI), and ( iii)additive Gaussian or non-Gaussian noise.   The continuous-time waveform, x ð t Þ ,is‘‘appropriately’’sampled, and the discrete samples are grouped to form vectors of ‘‘appropriate’’length, P (both the sampling method and the length value P are pertinent to the speciﬁcs of the application under consideration). Let x denote such adiscrete-time complex,ingeneral, received signal vector in C P .That is,                        p ﬃﬃﬃ                       p ﬃﬃﬃ                                                                          P              H 1 : x ¼þ  E 0 v 0 þ z and H 0 : x ¼  E 0 v 0 þ z ; x ; v 0 ; z 2 C  ð 9 : 2 Þ  where P identiﬁes the dimension of the discrete-time complex observation space, v 0 is the signal vector that corresponds to v 0 ð t Þ ,and z denotes the discrete-time comprehensivedisturbancevector [2]. Our objective is to detect b 0 (that is, to decide in favor of H 1 or H 0 )bymeans of alinear ﬁlter w as follows:                                                                                          ^             H                                   bb 0 ¼ sgn ð Ref w x g                           ð 9 : 3 Þ  wheresgnð · Þ is the ^ 1hard-limiter,Re f · g extracts the real partofacomplex number,and ð · Þ H denotes the Hermitian operation. In other words, we ﬁx the structure of the receiver to that given by Figure 9.1. Our discussion will be focused on the design of the linear ﬁlter w according to the MMSE or minimum- variance-distortionless-response (MVDR) optimization criteria that are presented in the following section. We  note that under the assumption of perfectly known input statistics and colored Gaussian noise (parameter z in Equation (9.2)), Figure9.1 that utilizes the ideal MMSE/MVDR ﬁlter w is indeed the optimum ML  scheme.   The speciﬁc illustrativeexample of amultipath-fading AWGN DS-CDMA packet data communication link with narrowband linear antenna-arrayreception that we considered earlier is certainly covered by the above general basic signal model. The transmitted signal waveform of aparticular user is obtained as follows. The user is assigned aunique binaryantipodal signature (code) sequence, that is asequence withelements, þ 1 or   1, of length L ( L is also called system processing gain). The bits of the user code multiply abasic signal pulse  (e.g., square pulse or raised cosine) of duration, T c ,known as chip.This wayweobtain the signature waveform, d q ð t Þ ,ofduration, T ¼ LTc .The transmitted signal waveform that corresponds to asingle information bit is the product of the information bit itself and the signature waveform. The corresponding received waveform is the convolution of the transmitted waveform and the impulse response of the multipath fading channel (when the latter is modeled as alinear tap delayline), and is assumed to be bandlimited by the chip rate. Discretization of the continuous received waveform at each antenna element of the array can be achieved by chip-matched ﬁltering of the received waveform and sampling at the chip rate (or by low-pass ﬁltering,commensurate Nyquist sampling, and chip-rate accumulation) over the multipath-extended symbol period. The discrete vector outputs from all antenna elements are stacked together (one on top of the other) to create asuper-vector known as space-time (S-T)received vector.This waythe data are prepared for processing by the linear ﬁlter, w ,extraction of the real partofthe ﬁlter output, and ﬁnally,sign detection as shown in Figure9.1 —aprocess termed ‘‘one-shot’’detection, i.e. detection on asymbol-by-symbol (information bit) basis, as opposed to simultaneous detection of all information bits of the user of interest. If M is the number of antenna elements of the array, L is the length of the signature(code) vector,and N is the number of resolvable multipaths (w.l.o.g.weassume that N is the same for all users) then the discrete-time, S-T complex received vector x is of dimension, M ð L þ N   1 Þ ;where L þ N   1isexactly the length of what we referred to earlier as                                                                      ^                  x(t)   Sampling/  x  Linear filter H                                                 wxRe{    }     <> 0 b                       discretization      w  FIGURE 9.1 General receiver structure for the (one-shot) detection of binaryantipodal information symbols of the user of interest.Low Sample Support Adaptive Parameter Estimation and Packet-Data Detection          9 -5   the multipath-extended symbol period. ‘‘Inside’’ x in Equation (9.2), v 0 corresponds to the channel-processed (also known as ‘‘effective’’) S-T signature vector of the user of interest while z corresponds to the discrete-time  disturbancevector that accounts for MAI, ISI, and AWGN. Speciﬁcally, v 0 can be expressed as afunction of the transmitted signal, channel and receiver structureparameters:                                       2                 3                            r ﬃﬃﬃ                          T                              E N X   1                       v  ¼    0    c  4 0 ... 0 d T 0 ... 0 5   a                  ð 9 : 4 Þ                        0     L      0 ; n |ﬄ{zﬄ} 0 |ﬄ{zﬄ}      0 ; n                                n ¼ 0       n      N   n   1  where c 0 ; n , n ¼ 0 ; ... ; N   1denote the path coefﬁcients of the channel of the user of interest.The coefﬁcients, c 0 ; n , n ¼ 0 ; ... ; N   1, are frequently modeled as independent zero-mean complex Gaussian random variables (which exhibit Rayleighdistributed amplitude and auniformly distributed phase that ﬁts experimental measurements) and are assumed to remain constant over the entirepacket duration. In arealistic environment, the coefﬁcients mayvaryapproximately every300 symbols [3]. Thus, keeping the packet size less than 300 validates the assumption of constant multipath coefﬁcients over the duration of apacket. In Equation (9.4),                      T d 0 ¼½d 0 ½ 0   ; ... ; d 0 ½ L   1    is the binarysignature vector (spreading sequence) of the user of interest, d 0 ½ l   2 f 6 1 g , l ¼ 0 ; ... ; L   1, a 0 ; n is the array response vector that corresponds to the n th path of the user of interest and   denotes the Kronecker tensor product. The array response vector of the n th path of the user of interest is deﬁned by                                        d                                  j 2 p ð m   1 Þ sin y 0 ; n                       a 0 ; n ð m Þ¼e  l      ;   m ¼  1 ; 2 ; ... ; M             ð 9 : 5 Þ  where y 0 ; n identiﬁes the angle of arrival of the corresponding path, l is the carrier wavelength, and d is the                                           l element spacing of the antenna array (usually d ¼ 2 ). More details on the DS-CDMA S-T received signal model in Equation (9.4) and the operational characteristics of an antenna-arraysystem can be found in Refs. [4,5]. Finally,the noise vector z represents the comprehensivedisturbance effect of AWGN and all other user signal contributions that are again of the form of Equation (9.4), yet withdifferent in general energy values, signature vectors, multipath coefﬁcients, and angles of arrival.  9.3    Data Processing with Known Input Statistics  Optimum    MMSE/MVDR Filter MVDR (minimum-variance-distortionless-response) receiver design refers to the problem of identifying a linear ﬁnite-impulse-response ﬁlter that minimizes the varianceatits output, while at the same time the ﬁlter maintains a‘‘distortionless’’ response toward aspeciﬁc input vector direction of interest. In mathematical terms, if x is arandom, 0 -mean (without loss of generality) complex input vector of dimension, P , x 2 C P ,that is processed by a P -tap ﬁlter, w 2 C P ,then the ﬁlter output varianceis E fjw H x j 2 g¼w H Rw,where R ¼ E f xxH g is the input autocorrelation matrix ( E f · g denotes the statistical expectation operation). The MVDR ﬁlter           H                                               H minimizes w Rw and simultaneously satisﬁes an equation of the form, w v 0 ¼ r ; where v 0 is the given input signal vector direction to be protected. In this set-up,MVDR ﬁltering is astandardlinear constraint optimization problem. The conventional Lagrange multipliers constraint optimization technique leads to the solution (the Lagrange multipliers optimization technique is presented in detail in Ref. [5])                                                    1                                                R  v 0                                   w MVDR ¼ r * H    1                              ð 9 : 6 Þ                                               v 0 R v 0  where ð · Þ * denotes conjugation. Extensivetutorial treatments of MVDR ﬁltering can be found in many sources, for example in Ref. [5].   MVDR ﬁltering has long been aworkhorse for blind (unsupervised) communications and signal processing applications whereadesired(pilot) scalar ﬁlter output, y 2 C ,cannot be identiﬁed or cannot be assumed9 -6                                   Broadcasting and Optical Communication Technology  available for each input, x 2 C P .Prime examples include radar and array processing problems where the  constraint vector, v 0 ,isusually referred to as the ‘‘target’’or‘‘look’’ direction of interest. It is interesting to observe the close relationship between the MVDR ﬁlter and the MMSE (‘‘Wiener’’) ﬁlter.Indeed, if the  constraint vector, v 0 ,ischosen to be the statistical cross-correlation vector between the desired output and the input vector,then the MMSE ﬁlter obtained by minimizing the mean square (MS) error between the ﬁlter output and the desiredoutput is given by                                          1                                     c R  v 0 ; c > 0                               ð 9 : 7 Þ  The MMSE   ﬁlter becomes apositive scaled version of the MVDR ﬁlter and exhibits identical output SINR performance. Forthis reason, in the rest of our discussion we refer comprehensively to both ﬁlters as MMSE/MVDR    ﬁlters [5].   Conventionally,the computation of the MMSE/MVDR ﬁlter in Equation (9.6) or Equation (9.7) begins with the calculation of the inverse of the ideal input autocorrelation matrix, R   1 (assuming that the Hermitian matrix R is strictly positive deﬁnite, henceinvertible). The calculation of the inverse is usually based on numerical iterative diagonalization linear algebra procedures [6]. Then, the matrix, R   1 ,isused for the linear                                                                   H    1 transformation (left multiplication) of the constraint vector, v 0 ,followed by v 0 R v 0 normalization, as necessary.   Linear transformations that involve the inverse of ahigh-dimension matrix are computationally intensive. In addition, and most importantly,severecomplications arise at the adaptiveimplementation stage when the estimate of such ahigh-dimension matrix is inverted (particularly when the estimate is based on asmall set of data/observations and is obtained, possibly,bysome form of sample averaging). One extreme example of such acomplication is the fact that the inverse may not even exist. Thus, when the data that are available for adaptation and redesign are limited, use of inverses of (sample-average)estimated high-dimension matrices is not viewed favorably (this issue will be discussed in detail in the next section). In such cases, it is preferred to proceed with alternativemethods that approximate the optimum solution and, hopefully,avoid implicit or explicit use of inverses. Then, at the adaptive implementation stage, we may utilize estimates of the approximate solutions. Algorithmic designs that aim at approximating the optimum MMSE/MVDR ﬁlter include: (i) The generalized sidelobe canceller (GSC) and its variations, and (ii) the auxiliary-vector (AV) ﬁlter.The relative performanceofthe above methods in limited data supportenvironments is examined in Section ‘‘Auxiliary-Vector (AV) Filters’’.  Generalized  Sidelobe  Canceller  (GSC)                                                      P                                P Foragiven (not necessarily normalized) constraint vector, v 0 2 C ,any ‘‘distortionless’’linear ﬁlter w 2 C ,               H                                          r *                     P that satisﬁes w v 0 ¼ r can be expressed/decomposed as w ¼  2 v 0   u for some u 2 C such                                                         k v 0 k      H that v 0 u ¼ 0, as shown in Figure9.2 (this decomposition is an application of the projection theorem in linear algebra). There are two general approaches for the design of the ﬁlter part, u : (i) eigen-decomposition based approaches and (ii) non-eigen-decomposition-based approaches.   Algorithmic eigen-decomposition based designs that focus on the MMSE/MVDR ﬁlter part, u ,which is  orthogonal to the constraint vector,or‘‘look’’ direction, v 0 ,include the Applebaum/Howells arrays, beam- spacepartially adaptive processors, or generalized sidelobe cancellers (GSC). Recent developments havebeen inﬂuencedbyprincipal component analysis reduced-rank processing principles. The general goal of these designs is to approximate the MMSE/MVDR ﬁlter part u by utilizing different rank reducing matrices as explained below. The approximation is of the general form (Figure 9.3):                                                         GSC                              u P · 1 . B P · ð P   1 Þ T ð P   1 Þ · p w p · 1     ð 9 : 8 Þ                              H where B is amatrix that satisﬁes B v 0 ¼ 0 P   1 and is, thus, called ‘‘blocking matrix’’sinceitblocks signals in the direction of v 0 ( B is afull column-rank matrix that can be derivedbyGram-Schmidt orthogonalization ofLow Sample Support Adaptive Parameter Estimation and Packet-Data Detection          9 -7                           W Px1                                                w Px1       X       ρ *                            x        ρ *                  V                                       v 0                 2 0 Px1                               v 2Px1               V 0                                      0                                                                                    H                              W H X                                                w x                                                                          u Px1                                                                        GSC                                                   B Px(P− 1) T (P− 1)xp w px1                U Px1   FIGURE 9.2 General decomposition of alinear ﬁlter, FIGURE 9.3 Generalized sidelobe canceller structure.                 H w ,that satisﬁes w v 0 ¼ r to two orthogonal            H components, ð u v 0 ¼ 0 Þ .                                             H                                         v 0 v 0 a P · P orthogonal projection matrix such as I   2 where I is the identitymatrix). T is the rank reducing                                        k v 0 k matrix with 1 < p < P   1columns that havetobeselected, and w GSC is avector of weights of the p columns of T                                                                 8    !           9                                                                 <             H   2 =                                                                     r *           that is designed to minimize the variance at the output of the ‘‘overall’’ﬁlter w , E   v   u x   .                                                                 :      2 0        ;                                                                    k v 0 k   The solution to the latter optimization problem (assuming that T is given) is                              GSC    r *   H  H       1 H H                           w    ¼      2 ½ T B RBT   T  B  Rv0                      ð 9 : 9 Þ                                   k v 0 k We note that the rank-reducing matrix T ‘‘reduces’’ the dimension of the linear ﬁlter (number of parameters to be designed) from P (dimension of ﬁlter, w )to p (dimension of ﬁlter, w GSC), 1 5 p 5 P   1. The p columns of the rank reducing matrix, T ,can be chosen in various ways. We can choose the p columns to be the eigenvectors that correspond to the P maximum eigenvalues of the disturbance-only autocorrelation matrix. This choice is mean-square (MS) optimum under the assumption that the disturbance-only eigenvectors are not rotated by the blocking matrix being used (i.e., when the disturbancesubspaceisorthogonal to the  constraint vector, v 0 ), which is not true in general. We can address this concern by choosing alternatively the p columns of T to be the eigenvectors that correspond to the p maximum eigenvalues of the blocked data autocorrelation matrix, B H RB.If, however,the columns of the rank reducing matrix, T , have to be eigenvectors of the blocked-data autocorrelation matrix (there is no documented technical optimalityto  this approach), then the best way in the minimum output variance sense is to choose the p eigenvectors q i of                                                      j v H RB j 2 B H RB with corresponding eigenvalues l ,that maximize the ratio, 0 q i , i ¼ 1 ; ... ; p ,[7] (also known as                                 i                       l ‘‘cross-spectral metric’’).                              i  9.4    Auxiliary-Vector (AV) Filters  Auxiliary-vector (AV)-ﬁlters are non-eigen-decomposition based ﬁlters that approximate the optimum MMSE/MVDR solution [4,8,11]. The AV algorithm is astatistical optimization procedurethat generates a sequence of ﬁlters (AV-ﬁlters). Each ﬁlter in the sequence has the general structure described in Figure9.2, wherethe vector, u ,isapproximated by aweighted sum of auxiliaryvectors that maintain orthogonality only  with respect to the distortionless direction, v 0 (and they are, in general, non-orthogonal,toeach other). The number of auxiliaryvectors used to approximate the ﬁlter part, u ,inFigure 9.2 is increasing with the ﬁlter index in the sequence. Both the auxiliaryvectors and the corresponding weights are subject to design (they are designed according to the maximum cross-correlation and minimum variance criterion, respectively,as explained in detail below). An important characteristic of the AV-algorithm (besides the non-orthogonalityof the auxiliaryvectors) is that it is aconditional optimization procedure.That is, each ﬁlter in the sequence is a9 -8                                   Broadcasting and Optical Communication Technology  function of the previously generated ﬁlter.Furthermore, AV-ﬁlters do not requireany explicit or implicit matrix inversion, eigendecomposition or diagonalization. Finally,under ideal setups (perfect known input autocovariance matrix) the AV ﬁlter sequence converges to the MMSE/MVDR optimum solution [9].   Apictorial presentation of generation of the sequence of AV-ﬁlters is given by Figure9.4(a). The sequenceis                                                   r * initialized at the appropriately scaled constraint vector, w 0 ¼ 2 v 0 ,which is MMSE/MVDR optimum only                                                  k v 0 k                                         2 when the vector inputs are white (i.e., when R ¼ s I , s > 0). Next, we incorporate in w 0 an ‘‘auxiliary’’vector                                                                        P component, g 1 ,that is orthogonal to v 0 ,and we form w 1 ¼ w 0   m 1 g 1 where g 1 2 C  f0 g , m 1 2 C ,and  H g 1 v 0 ¼ 0. We assume for amoment that the orthogonal auxiliaryvector, g 1 ,isarbitrarybut nonzeroand ﬁxed. We concentrate on the selection of the scalar, m 1 .The value of m 1 that minimizes the variance of the                                                                  H  2 output of the ﬁlter, w 1 ,can be found by direct differentiation of the variance, E fjw 1 x j g ,orsimply as the value                                 H       * H                  H      H that minimizes the MS error between w 0 x and m 1 g 1 x .This leads to m 1 ¼ g 1 Rw0 = g 1 Rg1 .   Since g 1 is set to be orthogonal to v 0 ,the expression of m 1 shows that if the vector, Rw0 ,happens to be ‘‘on v 0 ’’                H                        H                                   H (that is if Rw0 ¼ðv 0 Rw0 Þ v 0 or equivalently ð I   v 0 v 0 Þ Rw0 ¼ 0), then m 1 ¼ 0. Indeed, if Rw0 ¼ðv 0 Rw0 Þ v 0 then w 0 is already the MMSE/MVDR ﬁlter.Toavoid this trivial case and continue with our presentation, we assume            H that Rw0 6¼ðv 0 Rw0 Þ v 0 .Byinspection, we also observe that for the MS-optimum value of m 1 the product, m 1 g 1 ,is independent of the norm of g 1 .Hence, so is w 1 .Atthis point, we set the auxiliaryvector, g 1 ,tobeanormalized                                                         H      H                  H vector that maximizes the magnitude of the cross-correlation between w 0 x and g 1 x (i.e., g 1 ¼ arg max j w 0 Rgj )                         H          H                                          g subject to the constraint that g 1 v 0 ¼ 0and g 1 g 1 ¼ 1. Forthe sake of mathematical accuracy,wenote that both                     H the criterion function j w 0 Rgj to be maximized as well as the orthogonalityconstraint are phase invariant. Without loss of generality, to avoid anyambiguityinour presentation and to haveauniquely deﬁned auxiliary  vector,wechoose the one and only auxiliaryvector, g 1 ,that satisﬁes the maximization problemand places the                                         H cross-correlation value on the positive real line, ð w 0 Rg1 > 0 Þ .   The general inductive step is as follows: At step k þ 1, we deﬁne the AV ﬁlter, w k þ 1 ¼ w k   m k þ 1 g k þ 1 ,where g k þ 1 and m k þ 1 are to be conditionally optimized given the previously identiﬁed AV ﬁlter, w k .The auxiliaryvector, g k þ 1 ,ischosen as the vector that maximizes the magnitude of the cross-correlation between the output of the previous ﬁlter, w k ,and the output of g k þ 1 (Figure 9.4.(a)), again subject to g k þ 1 being orthonormal to v 0 only (we note that the choice of the norm does not affect the solution since m k g k , k ¼ 1 ; 2 ; ... ; is g -norm invariant; we also                         (a)                                          (b)                                                           Auxiliary-Vector (AV) algorithm                                             w k+1                                        w                                         k             Input:                              w 2                           Autocovariance matrix R, constraint vector v 0 ,                       w 1                                            H                                                           desired response w v 0 = ρ .                 w 0                                                       Initialization:  x           ρ *    H     H     H   H      H     H                    wx0   wx1   wx2 ...wxk-1 wxk wxk+1         *               2 v 0    +     +         +     +            w := ρ v .             v 0                                            0   2 0                       -     -          -     -                v 0                                                       Iterative computation:             g 1   μ 1                                                           For k=1,2,... do                                                           begin             g                                                      v v H              2    μ 2                                       g      0 0                                                              k := (I− 2 )Rwk − 1                                                                    v 0                                                              if g k =0 then EXIT                                                                   H                                                                  g k Rwk − 1                                                             μ k :=                                                                   H                                                                  g k Rgk             g                                               w := w −μ g              k    μ k                                        k    k − 1 k k                                                           end                  μ                                    Output:             g k+1 k+1                                                           Filter sequence w 0 ,w1 ,w2 ,... .   FIGURE 9.4 (a) Block diagram representation and (b) algorithmic description/generation of the auxiliary-vector (AV)  ﬁlter sequence, w 1 ; w 2 ; ... .Low Sample Support Adaptive Parameter Estimation and Packet-Data Detection          9 -9   emphasize that g 1 , g 2 , g 3 , g 4 ... , are not necessarily orthogonal to each other). The value of m k þ 1 minimizes the                                                                                  H output variance of w k þ 1 given w k and g k þ 1 (or equivalently minimizes the MS error between w k x and      H m k þ 1 * g k þ 1 x ). The solution for g k þ 1 and m k þ 1 is given below, while the iterativealgorithm for the generation of the inﬁnite sequence of AV-ﬁlters, w 0 ; w 1 ; w 2 ; ... ,ispresented in Figure9.4(b) (in Figure 9.4(b), we dropped the unnecessarynormalization of g 1 , g 2 , ... since m k g k is independent of the norm of g k ):     (i) The scalar m k þ 1 that minimizes the varianceatthe output of w k þ 1 or equivalently minimizes the MS                     H           H        error between w k x and m k þ 1 * g k þ 1 x is                                      H                                     g k þ 1 Rwk                             m k þ 1 ¼ H      ;  k ¼ 0 ; 1 ; 2 ; ...               ð 9 : 10Þ                                    g k þ 1 Rgk þ 1                    !                          H                       v 0 v 0    (ii) Suppose that I   2  Rwk 6¼ 0 ð w k 6¼ w MVDRÞ .Then, the auxiliaryvector                       k v 0 k                                        v H Rw                                 Rw      0   k v                                     k   k v k 2 0                          g   ¼            0      ;  k ¼ 0 ; 1 ; 2 ; ...           ð 9 : 11Þ                           k þ 1         H                                               v Rw                                        Rw    0   k v                                       k       2 0                                           k v 0 k                                                             H       H        maximizes the magnitude of the cross-correlation between w k x and g k þ 1 x (which is equal to          H                              H            H                      H        j w k Rgk þ 1 j ), subject to the constraints g k þ 1 v 0 ¼ 0and g k þ 1 g k þ 1 ¼ 1. In addition, w k Rgk þ 1 is real                H        positive ð w k Rgk þ 1 > 0 Þ .                                                                               R   1 v   With respect to the convergence of the ﬁlter sequence, w ; w ; w ; ... ,tothe MVDR ﬁlter r * 0 ,we                                                 0  1  2                      v H R   1 v can show [9]                                                                  0    0      (i) The generated sequence of auxiliary-vector weights, f m k g , k ¼ 1 ; 2 ; ... ,isreal-valued, positive,and                    1         1        bounded: 0 5   < m k <  ; k ¼ 1 ; 2 ; ... ,where l max and l min are the corresponding maximum and                   l max     l min        minimum eigenvalues of R ;     (ii) The sequence of auxiliaryvectors, f g k g , k ¼ 1 ; 2 ; ... ,convergestothe 0vector,i.e., limk ! 1 g k ¼ 0;    (iii) The sequence of AV  ﬁlters, f w k g , k ¼ 1 ; 2 ; ... ,converges to the MVDR ﬁlter,i.e.,                         1                      R  v 0        limk ! 1 w k ¼ r * H   1 .                     v 0 R v 0   We conclude this section with afew comments on AV ﬁltering.Fromageneral input spacesynthesis/ decomposition point of view,the key features of the AV algorithm are: non-orthogonal AV synthesis and conditional statistical optimization.Non-orthogonal synthesis allows the designer to growan inﬁnite sequence of AV ﬁlters on a‘‘best effort’’basis that takes into account the whole interference space at everystep. Conditional optimization results in estimators that do not require anyimplicit or explicit matrix inversion or decomposition operation and, thus, plays akey roleindeveloping superior adaptiveﬁltering schemes in short-data-recordenvironments, as illustrated in the next section.   Figure9.5 presents an illustrativeexample of the relative merits of various receiver designs in terms of BER. The example is based on asimple single-path synchronous DS-CDMA signal model, ð P ¼ L Þ .The  BER performanceofthe receiver, w 3 ,(which utilizes threeauxiliaryvectors, g 1 , g 2 ,and g 3 )iscompared with the BER performanceofthe ‘‘maximum eigenvector’’(Max EV)receiver and the ‘‘cross-spectral- metric’’(CSM) receiver (both of which use threeeigenvectors). As anumerical example that illustrates the convergence of the AV-ﬁlter sequence to the ideal MMSE/MVDR solution under perfectly known (ideal) autocorrelation matrix, R ,inFigure 9.6, we plot the squarednorm error between the AV ﬁlter of the user of  interest, w k ,and w MMSE= MVDR as afunction of k (i.e., the number of auxiliaryvectors used or equivalently the index of the AV ﬁlter in the sequence).9 -10                                        Broadcasting  and Optical Communication     Technology                            10− 1                          Bit–Error–Rate                                      Max EV                                     CSM                                     AV                          10− 2                                50      100      150     200      250     300                                              Data Record Size  FIGURE  9.5  BER as afunction of the data record size for the AV,‘‘max, eigenvector’’(MaxEV),and ‘‘cross-spectral metric’’(CSM) receivers of the same order (3 auxiliaryvectors, 3eigenvectors). Operational environment:Synchronous DS- CDMA   system, user of interest at 12 dB, 12 interferers at 10–14 dB, processing gain L ¼ 32, and arbitrarynormalized signatures (cross-correlation with the signature of the user of interest about 0.2).                              2.5                           2    2                          ||                             1.5                             MMSE/MVDR, IDEAL 1                          –w                           k                           ||w 0.5                               0                               0   5   10   15  20   25  30   35  40   45  50                                Numerical Iterations (Number of auxiliary vectors) k  FIGURE  9.6 Convergenceofthe AV ﬁlter sequence to the ideal MMSE/MVDR solution as afunction of the number of iterations k in Figure 9.4(b). The sequence of conditionally optimized AV ﬁlters (which utilize non-orthogonal auxiliary  vectors) converges to the w MMSE= MVDR optimum solution for aperfectly known input autocorrelation matrix R .  9.5      Disjoint    Parameter       Estimation       and   Packet-Data       Detection Known     Channel SMI,  GSC,  and  Auxiliary-Vector  Estimators We  recall that the MMSE/MVDR  ﬁlter is afunction of the true input autocorrelation matrix, R ,and the true  constraint vector, v 0 .However,inalmost everypractical adaptiveﬁltering application, neither R nor v 0 is known  to the receiver.Inthe ﬁrst part(partA)ofthis section we present various estimates of the optimum MMSE/MVDR      ﬁlter when R is unknown and sample-average estimated from adata packet (data record)ofLow Sample Support Adaptive Parameter Estimation and Packet-Data Detection         9 -11   size, J ,that is x 0 ; x 1 ; ... x J   1                                            1 XJ   1                                     RR ^ ð J Þ¼ x x H                             ð 9 : 12Þ                                            J     j j                                             j ¼ 0   Throughout the ﬁrst partofthis section, v 0 is assumed to be known (since v 0 is afunction of the channel parameters, we label partAas the ‘‘known channel’’case); aprocedurefor the estimation of v 0 fromthe same data packet (data record), x 0 ; x 1 ; ... x J   1 ,willbepresented in the second part(Part B: Unknown Channel).   When R is unknown, the most widely used MMSE/MVDR ﬁlter estimator is obtained from Equation (9.6) by using the sample average estimate RR ^ ð J Þ in place of R .This estimator is known as the sample-matrix- inversion (SMI) ﬁlter.Ifwechoose to work with the approximate solutions presented in Section ‘‘Data Processing with Known Input Statistics’’and utilize the sample average estimate of the autocorrelation matrix RR ^ ð J Þ instead of R in Equation (9.9), Equation (9.10), and Equation (9.11), we obtain aGSC and AV-type estimator of the MMSE/MVDR solution, respectively.Wenote that, for Gaussian inputs, RR ^ ð J Þ is amaximum- likelihood (ML), consistent, unbiased estimator of R .Onthe other hand, the inverse of RR ^ ð J Þ ,which is utilized explicitly by the SMI ﬁlter and implicitly by GSC, is not always deﬁned. We can guarantee (with probability1) that RR ^ ð J Þ is invertible only when the number of observations, J ,isgreater than or equal to the dimension of the input space (or ﬁlter dimension), P ,and the input distribution belongs to aspeciﬁc class of multivariate, elliptically contoured distributions that includes the Gaussian. Based on the convergence properties of the AV ﬁlter sequencediscussed in the previous section, we can show that the corresponding sequence of AV ﬁlter  estimates, ww ^ k ð J Þ ,converges, as k ! 1 ,tothe SMI ﬁlter [9]                                                        ^      1                                                      ½ RR ð J Þ  v 0                        ww ^ k ð J Þ ! ww ^ 1 ð J Þ¼ww ^ SMI ¼ r *                 ð 9 : 13Þ                              k ! 1                   H  ^     1                                                     v 0 ½ RR ð J Þ  v 0  Properties of the Sequence of AV Estimators                                                    r * The AV-ﬁlter sequence of estimators begins with ww ^ 0 ð J Þ¼ 2 v 0 ,which is a 0 -variance,ﬁxed-valued,                                                   k v 0 k                                                                               2 estimator that maybeseverely biased ð ww ^ 0 ð J Þ 6¼ w MMSE= MVDRÞ unless the input is white (i.e., R ¼ s I ,for some s > 0). In the latter trivial case, ww ^ 0 ð J Þ is already the perfect MMSE/MVDR ﬁlter.Otherwise, the next ﬁlter estimator in the sequence, ww ^ 1 ð J Þ ,has asigniﬁcantly reduced bias due to the optimization procedure employed, at the expense of non-zero estimator (co)variance. As we moveupinthe sequence of ﬁlter estimators, ww ^ k ð J Þ ,                                         1 k ¼ 0 ; 1 ; 2 ; ... ,the bias decreases rapidly to zero while the variance rises slowlytothe SMI ð ww ^ 1 ð J ÞÞ levels (cf. Equation (9.13)). To quantify these remarks, we plot in Figure9.7 the norm-squarebias,                    2                                                                 H f ww ^ k ð J Þg   w MMSE= MVDRk ,and the traceofthe covariance matrix, E f½ww ^ k ð J Þ E f ww ^ k ð J Þg ½ww ^ k ð J Þ E f ww ^ k ð J Þg  g , as afunction of the iteration step (ﬁlter index) k ,for the same signal model as in Figure9.6 and data packet (data record) size J ¼ 256. Bias and covariance trace values are calculated from 100,000 independent ﬁlter estimator realizations for each iteration point, k .That is, we generate 100,000 independent data packets  ( J received random vectors per packet). Foreach packet, we evaluate ww ^ 1 ð J Þ , ww ^ 2 ð J Þ ; ... .Then, we evaluate expectations as sample averages over 100,000 data packets.    Since ww ^ 1 ð J Þ is unbiased, the trace of the covariance matrix is the MS ﬁlter estimation error.Itisimportant to observe that the covariance matrix and, therefore, the MS ﬁlter estimation error depend on the data record  size J ,the ﬁlter length P ,aswell as the speciﬁcs of the signal processing problem at hand (actual R and v 0 ). From the results in Figure 9.7 for J ¼ 256,wesee that the estimators ww ^ 1 ð J Þ , ww ^ 2 ð J Þ ; ... ,uptoabout ww ^ 20ð J Þ are particularly appealing.Incontrast, the estimators ww ^ k ð J Þ for k > 20 do not justify their increased covariance tracecost since they havealmost nothing to offer in terms of further bias reduction.    We emphasize that sincethe AV-ﬁlters, w 1 , w 2 , w 3 ; ...,can be considered as approximations of the MMSE/ MVDR optimum ﬁlter under ideal set-ups, the AV-ﬁlter estimates, ww ^ 1 ð J Þ , ww ^ 2 ð J Þ , ww ^ 3 ð J Þ ; ... ,havebeen viewed                                                                                         1   1                                                                                R v 0   The SMI estimator is unbiasedfor multivariate elliptically contouredinput distributions E f ww ^ 1 ð J Þg ¼ w MMSE= MVDR ¼ r * H   1 .                                                                                    v 0 R v 09 -12                                  Broadcasting and Optical Communication Technology                         2                       1.8                       1.6                       1.4                       1.2                       ov–Trace 1                       &C 0.8                        Bias 0.6                                                 ^                                                 w k Bias                       0.4                       ^                                                 w k Cov− Trace                                                     ^                       0.2                       SMI (w∞ )Cov –Trace                        0                         0   20  40  60  80  100 120 140 160 180  200                            Numerical Iteration (Number of auxiliary vectors) k   FIGURE 9.7 Norm-square bias and covariance trace for the sequence of estimators, ww ^ k ð J Þ , k ¼ 0 ; 1 ; ....The signal model is as in Figure 9.6; data record size J ¼ 256.                                            (a) J=256                         2.5                       }                       2  2                       ||                          1.5                                                          w ^                        MMSE/MVDR 1                       ^ k                       w                                  w ∞ (SMI)                       −                        k                                 LMS                      <  0.5                        ||w                                RLS                        E{ 0                           0  10  20  30  40  50  60  70  80 90  100                              Numerical Iteration (Number of auxiliary vectors) k                                           (b) J=2048                         2.5                       }                       2                         ||                                 ^                          2                               w                                                          ^ k                                                          w ∞ (SMI)                         1.5                              LMS                                                          RLS                        MMSE/MVDR                        w  1                       −                       < k                        ||w 0.5                       E{                          0                           0  10  20  30  40  50  60  70  80 90  100                              Numerical Iteration (Number of auxiliary vectors) k   FIGURE 9.8 MS estimation error for the sequence of estimators, ww ^ k ð J Þ , k ¼ 0 ; 1 ; ....(a) Data record size J ¼ 256. (b) J ¼ 2048.   so far not only as estimates of the ﬁlters, w 1 , w 2 , w 3 ; ... ,but also,and most importantly,asestimates of the MMSE/MVDR    optimum ﬁlter in Equation (9.6) and Equation (9.7). In this context, the mean-square                                            2 estimation error expression E fkww ^ k ð J Þ w MMSE= MVDRk g captures the bias/variancebalanceofthe individual members of the estimator sequence, ww ^ k ð J Þ , k ¼ 0 ; 1 ; 2 ; ... .InFigure9.8, we plot the MS estimation errorasa function of the iteration step k (or ﬁlter index) for the same signal model as in Figure9.6, for J ¼ 256 (part [a]) and J ¼ 2048 (part[b]). As areference, we also include the MS-error of the constraint-LMS estimator andLow Sample Support Adaptive Parameter Estimation and Packet-Data Detection         9 -13  the RLS estimator.The constraint-LMS estimator is given by the following recursion               !                      H                   v 0 v 0                 H                r *    ww ^ LMSð j Þ¼ I   2 ½ ww ^ LMSð j   1 Þ m x j x j ww ^ LMSð j   1 Þ  þ 2 v 0 ; j ¼ 1 ; ... ; J ð 9 : 14Þ                   k v 0 k                                 k v 0 k               r * with ww ^ LMSð 0 Þ¼ 2 v 0 and some m > 0. The recursion of the RLS estimator can be obtained from the SMI              k v 0 k formula in Equation (9.13) by utilizing the following iterative estimation of R   1 that is based on the matrix- inversion-lemma                                    RR ^   1 ð j   1 Þ x x H RR ^   1 ð j   1 Þ                RR ^   1 ð j Þ¼RR ^   1 ð j   1 Þ  j j      ;  j ¼ 1 ; ... ; J     ð 9 : 15Þ                                            H ^   1                                       1 þ x j RR ð j   1 Þ x j              1                                                                    1 where RR ^   1 ð 0 Þ¼ I for some E > 0. Theoretically,the LMS gain parameter m > 0has to be less than ,                         0                                                         blocked              E 0                                                               2 · l max         blocked wher !e l max  !is the maximum    eigenvalue of the  ‘‘blocked-data’’ autocorrelation matrix        H          H     v 0 v 0    v 0 v 0  I      2 RI       2 .While this is atheoretical upper bound, practitioners are well aware that empirical,     k v 0 k    k v 0 k data-dependent ‘‘optimization’’ or ‘‘tuning’’ofthe LMS gain m > 0orthe RLS initialization parameter E 0 > 0is                                                             1 necessarytoachieveacceptable performance(in our study we set m ¼  and E ¼ 20, respectively).                                                              blocked   0                                                         200 · l max This data speciﬁc tuning frequently results in misleading,over-optimistic conclusions about the short-data-  recordperformanceofthe LMS/RLS algorithms. In contrast, when the AV ﬁlter estimators, ww ^ k ð J Þ , generated by the algorithm of Figure9.4(b) are considered,tuning of the real-valued parameters, m and  E 0 ,isvirtually replaced by an integer choice among the ﬁrst several members of the f ww ^ k ð J Þg sequence.In Figure9.8(a), for J ¼ 256 all estimators, ww ^ k ð J Þ ,from k ¼ 2uptoabout k ¼ 55 outperform in MS-error their RLS, LMS, and SMI ð ww ^ 1 ð J ÞÞ counterparts. ww ^ 8 ð J Þ ( k ¼ 8auxiliaryvectors) has the least MS-error of all (best bias/variancetrade-off).When the data recordsize is increased to J ¼ 2048 (Figure 9.8(b)), we  can affordmoreiterations and ww ^ 13ð J Þ offers the best bias/variancetrade-off(lowest MS-error).All ﬁlter estimators, ww ^ k ð J Þ ,for k > 8outperform the LMS/RLS/SMI ð ww ^ 1 ð J ÞÞ estimators. For such large data record sets, ð J ¼ 2048Þ ,the RLS and the SMI ð ww ^ 1 ð J ÞÞ MS-error are almost identical. Figure 9.9 offers a                  } 3.5               2                ||                   3                  2.5                   2                MMSE/MVDR                  1.5                –w                   1                (J)              < k                w 0.5                ||                   0                E{ 0                                                       0                                                                    500                            5                                                              1000                                    10                                                       1500 Data Record Size J                 Number of aux. vectors k                                             15 2000  FIGURE 9.9 MS estimation error versus number of auxiliaryvectors, k and sample support, J (the signal model is the same as in Figure 9.6).9 -14                                  Broadcasting and Optical Communication Technology  three-dimensional plot of the MS estimation errorasafunction of the sample support J used in forming  ww ^ k ð J Þ and the number of auxiliaryvectors k (or ﬁlter index). The dark line that traces the bottom of the MS estimation error surfaceidentiﬁes the best number of auxiliaryvectors (or the index of the best ﬁlter) for anygiven data record size, J .  How  to Choose the Best AV Estimator We recall that, when the auto-covariance matrix is sample-average estimated, the sequence of AV estimators converges to the SMI ﬁlter.Evidently,early,non-asymptotic elements of the sequence offer favorable bias/variancebalancecharacteristics and outperform in mean-square ﬁlter estimation error the unbiased SMI ﬁlter-estimator,aswell as the (constraint)-LMS, and RLS. In the context of digital wireless communication receivers, superior mean-square ﬁlter estimation error translates to superior bit-error-rate performanceunder shortdata record receiver adaptation. Selecting the most successful (in some appropriate sense) AV estimator in the sequence for agiven data record is acritical problem. Belowwepresent twodata-dependent selection criteria [12]. The ﬁrst criterion minimizes the cross-validated sample average variance of the AV-ﬁlter output and can be applied to general ﬁlter estimation problems; the second criterion maximizes the estimated J -divergence of the AV-ﬁlter-output conditional distributions and is tailoredtogeneral hypothesis testing (detection) applications.   In particular,the cross-validated minimum-output-variance (CV-MOV) rule is motivated by the fact that minimization of the output variance of ﬁlters that are constrained to be distortionless in the vector direction of asignal of interest is equivalent to maximization of the output SINR. Cross-validation is awell-known statistical method. In the context of AV-ﬁltering,cross-validation is used to select the ﬁlter parameter of interest (number of auxiliaryvectors, k )that minimizes the output variancethat is estimated based on observations (training data) that havenot been used in the process of building the ﬁlter itself. Aparticular case of this general method used in this presentation is the ‘‘leave-one-out’’method. The following criterion outlines the CV-MOVAV-ﬁlter selection process. Criterion 9.1 Foragiven data packet (data record) of size J, the cross-validated minimum-output-variance AV-ﬁlter selection rule chooses the AV-ﬁlter estimator ww ^ ð J Þ that minimizes the cross-validated sample average                                             k 1 output variance, i.e.,                                      8                       9                                      < X J                   =                                                 H   H                          k 1 ¼ arg min   ww ^ k ð J n j Þ x j x j ww ^ k ð J n j Þ ð 9 : 16Þ                                   k  :                       ;                                       j ¼ 1  where ð J n j Þ identiﬁes the AV-ﬁlterestimator that is evaluated from the available data recordafter removing the jth sample.                                                                           A   While the CV-MOVcriterion can be applied to general ﬁlter estimation problems, the second criterion, i.e., the maximum J-divergence criterion, is tailoredtoapplications that can be formulated as binaryhypothesis  testing problems on AV-ﬁltered data. Forany scalar binaryhypothesis testing problem, if f 0 and f 1 denote the conditional distributions of the detector input under hypothesis H 0 and H 1 ,respectively,then the J -divergence distancebetween f 0 and f 1 is deﬁned as the sum of the Kullback–Leibler (K–L) distances between f 0 and f 1                                      D                             Dðf 0 ; f 1 Þ¼KLðf 1 ; f 0 ÞþKLðf 0 ; f 1 Þð9           : 17Þ                                                       1                                                   D R       f 1 ð x Þ wherethe K–L distanceof f 1 from f 0 is deﬁned as KLðf 1 ; f 0 Þ¼ f 1 ð x Þ log d x .                                                      1      f 0 ð x Þ   The choice of the output J -divergence as one of the underlying rules for the selection of the AV ﬁlter is motivated by the fact that the probabilityoferror of the optimum (Bayesian) detector for anyscalar binaryLow Sample Support Adaptive Parameter Estimation and Packet-Data Detection         9 -15  hypothesis testing problemislower bounded by                                 P e > p 0 p 1 expf D ð f 0 ; f 1 Þ = 2 gð9           : 18Þ   where p 0 , p 1 are the apriori probabilities of H 0 and H 1 ,respectively.The right-hand side of Equation (9.18) is amonotonically decreasing function of the J -divergence between the conditional distributions of the detector  input. When the conditional distributions under H 0 and H 1 are Gaussian with the same variance, Equation (9.18) is satisﬁed with equality. The latter implies that the larger the J -divergence the smaller the probabilityoferror or,equivalently,the larger the J -divergence the easier the detection problem. Thus, maximization of the J -divergence implies minimization of the probabilityoferror.Due to the above properties and its relationship with the probabilityoferror of the optimum detector, J -divergence has been extensively used in the detection literatureasahypothesis discriminant function. In the context of AV ﬁltering,  we denote the AV scalar ﬁlter-output conditional distributions under H 0 and H 1 by f 0 ; k ð · Þ and f 1 ; k ð · Þ , respectively, wherethe index k indicates the dependenceofthe distributions on the speciﬁc AV ﬁlter ww ^ k used from the available sequence ww ^ 1 , ww ^ 2 ; ... .Then, the J -divergence between f 0 ; k ð · Þ and f 1 ; k ð · Þ is also afunction of k ; for this reason, in the rest of our presentation it will be denoted as Dðk Þ .Tothe extent that the conditional  distributions of the AV-ﬁlter output under H 0 and H 1 are approximated by Gaussian distributions with opposite means and equal variances(which is areasonable, in general, assumption), we can show in a straightforward manner that                                           2       H                                        4 E f b 0 Re½ ww ^ k ð J Þ x  g                                Dðk Þ <           H                                ð 9 : 19Þ                                        Varf b 0 Re½ ww ^ k ð J Þ x  g  whereVarð · Þ denotes variance.The following criterion outlines the J -divergence AV-ﬁlter selection process. Criterion 9.2 Foragiven data packet (data record) of size J , the J - divergence AV-ﬁlterselection rule chooses the AV-ﬁlterestimator ww ^ ð J Þ that maximizes the estimated J - divergence DD ^ ð k Þ between the AV-ﬁlter output                     k 2 conditional distributions, i.e.,                                                  ^                                   k 2 ¼ arg maxf DD ð k Þg                        ð 9 : 20Þ                                             k                                                                                       A                                       ^           H   If we substitute b 0 in Equation (9.19) by bb 0 ¼ sgnð Re½ ww ^ k ð J Þ x  Þ and evaluate expectations by sample averaging,then we can obtain ablind estimate of the J -divergence                                        "#2                                          P J                                         1        H                                      4  J   j Re½ ww ^ k ð J Þ x j  j                     ^                    j ¼ 1                    DD B ð k Þ¼                 "#2                                ð 9 : 21Þ                              P J              2   P J                             1        H           1        H                             J     Re½ ww ^ k ð J Þ x j       J j Re½ ww ^ k ð J Þ x j  j                              j ¼ 1                j ¼ 1  wherethe subscript ‘‘B’’identiﬁes the blind version of the J -divergence function. Then, we can evaluate             ^ k 2 ¼ arg maxf DD B ð k Þg.Werecall that in Equation (9.21) x denotes the received signal vector of the general form      p ﬃﬃﬃ k                             P x ¼ b 0 E 0 v 0 þ z where(cf. Equation (9.2)) v 0 2 C is aknown deterministic signal vector, E 0 > 0represents the unknown energyscalar, z 2 C P is azero-mean disturbance vector (i.e., it may incorporate ISI, MAI, and  additivenoise effects), and b 0 is þ 1or   1with equal probability.Wealso recall (cf. Equation (9.3)) that the                                                                             H decision on H 0 ð b 0 ¼ 1 Þ or H 1 ð b 0 ¼þ1 Þ is based on the real partofthe AV ﬁlter output, Re½ ww ^ k ð J Þ x   ,where ww ^ k ð J Þ is the AV estimator that utilizes k auxiliaryvectors.9 -16                                  Broadcasting and Optical Communication Technology  Performance Illustrations We illustrate the overall short-data-recordadaptiveﬁlter performanceinFigure9.10 and Figure9.11 for a multipath fading DS-CDMA system that employs antenna array reception. We consider processing gain 31, 20 users, 5antenna elements, and 3resolvable multipaths per user with independent zero-mean complex Gaussian fading coefﬁcients of varianceone. The maximum cross-correlation between the assigned user signatures  reaches 30%. The total SNRs (over the threepaths) of the 19 interferers are set at SNR2   6 ¼ 6dB, SNR7   8 ¼ 7dB, SNR9   13 ¼ 8dB, SNR14  15 ¼ 9dB, SNR16  20 ¼ 10dB. The space-time product (ﬁlter length) is P ¼ð31 þ 2 Þ 5 ¼ 165. The experimental results are averages over 1000 runs (100 different channel realizationsand 10 independent data record generations per channel). In Figure9.10, we plot the BER1 of the AV estimators ww ^ ð J Þ                                                                                    k 1 and ww ^ ð J Þ as afunction of the SNR of the user of interest for data records of size J ¼ 230: We also plot the BER      k 2 curve of the ‘‘genie’’assisted BER-optimum ﬁlter ww ^ ð J Þ as well as the corresponding curvesofthe ideal MMSE/                                         k opt MVDR  ﬁlter w MMSE= MVDR,the SMI ﬁlter estimator ww ^ 1 ð J Þ ,and the S-T RAKE matched-ﬁlter (MF) ww ^ 0 ð J Þ .We observe that both ww ^ ð J Þ and ww ^ ð J Þ are veryclose to the ‘‘genie’’ BER-optimum AV ﬁlter estimator choice and                 k 1      k 2 outperform signiﬁcantly the SMI ﬁlter estimator and the matched ﬁlter.Wealso observe that for moderate to highSNR of the user of interest, the J -divergence selection rule is slightly superior to the CV-MOVselection rule. Figure 9.11 repeats the study of Figure 9.10 as afunction of the data record size. The SNR of the user of interest is ﬁxed at 8dB.   Concluding our discussion in the ﬁrst partofthis section, we note that the key for asuccessful solution (in the sense of superior ﬁlter output SINR or BER performance) to the problemofadaptivereceiver design under limited data supportistoemployreceivers with varying bias/variancecharacteristics and to effectivelycontrol these characteristics in adata-driven manner.For this reason, operations and ﬁlter design/optimization criteria that suffer from‘‘data starvation’’ (e.g., implicit or explicit matrix inversion and/or eigendecomposi- tion) should be avoided. From ageneral input spacesynthesis/decomposition point of view,the non- orthogonal synthesis and the conditional statistical optimization are twoprinciples that allowtogrowan inﬁnite sequence of AV-ﬁlters on a‘‘best effort’’basis that takesinto account the whole interference space at each step.These twofeaturesplayakey roleleading to superior adaptiveﬁltering performanceinshort-data- recordenvironments. In particular,under short-data-record adaptation, early,non-asymptotic elements of the                        100                        10− 1                        10− 2                        10− 3                        Bit–Error–Rate Ideal MMSE/MVDR                              SMI                         − 4                       10     MF                              AV "Genie"                              AVJ divergence                              AV CV− MOV                       10− 5                          4    5    6    7    8    9    10   11   12                                            SNR0 (dB)  FIGURE 9.10 BER versus SNR for the user signal of interest for amultipath-fading antenna-array received signal model ð L ¼ 31; K ¼ 20; M ¼ 5 ; N ¼ 3 Þ with P ¼ 165 and J ¼ 230.                                                       1                                            p ﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃ    The BER of each ﬁlter under consideration is approximated by Q SINRout ,since the computational complexityofthe BER expression for this antenna array CDMA system prohibits exact analytic evaluation [13].Low Sample Support Adaptive Parameter Estimation and Packet-Data Detection         9 -17                          100                                                     Ideal MMSE/MVDR                                                     SMI                                                     MF                                                     AV "Genie"                                                     AVJ divergence                                                     AV CV− MOV                       10− 1                           − 2                      Bit–Error–Rate 10                         10− 3                             180 200 220 240 260 280 300 320 340  360                                        Date Record Size J        FIGURE 9.11 BER versus data record size (the signal model is the same as in Figure 9.10, SNR0 ¼ 8dB).   sequence of AV estimators are mildly biased, but exhibit much lowervariancethan other alternatives (for digital communications applications the latter implies superior BER performance). As the available data recordincreases, we can afford to go higher and higher in the sequence of generated estimators. In the limit, if we are given inﬁnitely manyinput data we can go all the wayuptothe convergence point of the algorithm, which is the ideal MMSE/MVDR receiver.Asafew concluding notes, an online version of the AV algorithm is presented in Ref. [14]. Application of AV-ﬁltering to the problem of rapid synchronization and combined demodulation of DS-CDMA  signals is considered in Ref. [15]. Detailed results on data record size requirements to achieve agiven output SINR (or BER) performancelevel can be found in Ref. [16].  Unknown Channel  The second partofthis section is devotedtothe estimation of the channel-processed constraint vector, v 0 , from the same data packet (data record), x 0 , x 1 ; ..., x J   1 .Tobeconsistent with our discussion and illustrative studies presented in the previous sections, we consider the general case of S-T DS-CDMA signal model described by Equation (9.1) to Equation (9.5). We recall that Q , L , N , M ,and J denote the number of active users in the system, the processing gain, the number of paths experiencedbythe transmitted signal of each  user,the number of antenna elements, and the data packet (data record) size, respectively.Wealso recall that v 0 is the S-T RAKE ﬁlter of the user of interest ( user 0 ), deﬁned by v ¼ D E f x b g wherethe statistical expectation                                                     0   b 0 0 operation E f · g is taken with respect to the bit of the user of interest b only.Clearly, v consists of shifted          b 0                                              0            0 versions of the S-T matched ﬁlter multiplied by the corresponding channel coefﬁcients (cf. Equation (9.4)):1                                      2                 3 T                              N X   1                                     4         T       5                         v 0 ¼    c 0 ; n |ﬄ{zﬄ}0 ... 0 d 0 |ﬄ{zﬄ}0 ... 0   a 0 ; n ð 9 : 22Þ                               n ¼ 0      n       N   n   1   Hence, v 0 is afunction of the binarysignature vector (spreading sequence) of the user of interest, d 0 ,the channel coefﬁcients, c 0 ; 0 ; c 0 ; 1 ; ... ; c 0 ; N   1 ,and the corresponding angles of arrival, y 0 ; 0 ; y 0 ; 1 ; ... ; y 0 ; N   1 ,                                          2            3                                  q ﬃﬃﬃ                 T                     r ﬃﬃﬃ                                    P                                          E 0   1 For the sakeofmathematical accuracy, v ¼ E 0 N   1 c 4 0 ... 0 d T 0 ... 0 5   a .The positivemultiplier is dropped                               0   L n ¼ 0 0 ; n |ﬄ{zﬄ} 0 |ﬄ{zﬄ} 0 ; n         L in Equation (9.22) as inconsequential.      n     N   n   19 -18                                  Broadcasting and Optical Communication Technology  (cf. Equation (9.5)). While the spreading sequence is assumed to be known to the receiver,the channel coefﬁcients and the angles of arrival are unknown.  Subspace Channel and Angle-of-Arrival Estimation                                                 D                T In this section, we explain how the channel coefﬁcients, c 0 ¼ ½ c 0 ; 0 ; c 0 ; 1 ; ... ; c 0 ; N   1   ,and the angles of arrival,    D                 T y 0 ¼ ½ y 0 ; 0 ; y 0 ; 1 ; ... ; y 0 ; N   1   ,for the user of interest, user 0 ,can be estimated by subspace-based techniques from the S-T  data packet (data record), x 0 , x 1 ; ... ; x J   1 .Wenote that while adaptivesubspace (eigendecomposition)-typeMMSE/MVDR ﬁltering is not afavorable approach under limited data support (the resulting estimates exhibit highvariance), subspace-type channel estimation techniques do not suffer from ‘‘data starvation,’’ as illustrated later.   Let the binarydata of each user be organized in identically structuredpackets of J bits. The channel  estimation procedurethat we employutilizes J p pilot bits (bits that are known to the receiver). Thus, the q th user data packet, f b q ð 0 Þ ; b q ð 1 Þ ; ... ; b q ð J   1 Þg; q ¼ 0 ; 1 ; ... ; Q   1 ; contains J   J p information bits and J p pilot bits. The J p known bits will be utilized later for the supervised recoveryofthe phase of the subspacechannel estimates, sinceblind second-order channel estimation methods return phase-ambiguous estimates. An  example of the data packet structure is shown in Figure9.12, wherethe J p pilot bits appear as a mid-amble in the transmitted packet [17]. Without loss of generality, we assume that each user transmits one data packet per  slot and the slot duration is T s seconds. Therefore, the data packet size J is the number of information bits transmitted by each user in one time slot, i.e., T s ¼ JT where T is the duration of each information bit transmission.    The rank, r s ,ofthe signal subspace of the received data vectors, x ,can be controlled by consideringone-sided or two-sided truncations of x (the latter eliminates ISI). The possible values of r s ,depending on the data format of choice, are as follows      (i) No truncation: Data dimension ¼ M ð L þ N   1 Þ ,2Q þ 1 < r s < 3 Q .    (ii) One-sided truncation: Data dimension ¼ ML,2Q < r s < 3 Q   1.    (iii) Two-sided truncation: Data dimension ¼ M ð L   N þ 1 Þ , Q < r s < 2 Q   1.   To haveaguaranteed minimum  rank of the noise subspace of M ð L   N þ 1 Þ ð 2 Q   1 Þ ,wechoose to truncate x from both sides (case [iii]) as shown in Figure9.13, and we form the ‘‘truncated’’received vector x tr of length M ð L   N þ 1 Þ as follows                                       2              3                                         x ððN   1 Þ T c Þ                                       6              7                                       6    x ð NTc Þ 7                                  x tr ¼ 6     .      7                            ð 9 : 23Þ                                       4       .      5                                          x ððL   1 Þ T c Þ                                                 tr Then, with respect to the j th information bit of user 0 , x j can be expressed as                                     p ﬃﬃﬃ                                       E                           x tr ¼ b ð j Þ 0 A B ð y Þ c þ MAI þ n                  ð 9 : 24Þ                            j    0    L    0   0 0       j    j                                              J                            Information Bits          Information Bits                                            J p                                          Pilot Bits          FIGURE 9.12 Data packet structure of total length J bits that contains amid-amble of J p pilot bits.Low Sample Support Adaptive Parameter Estimation and Packet-Data Detection         9 -19                                      X j of length M(L+N− 1)                b (j− 1)                  b (j)                    b (j+1)                 b (j− 1)                   b (j)                    b (j+1)                   b (j− 1)                  b (j)                    b (j+1)                                     X tr of length M(L− N+1)                                      j                            ISI                            FIGURE 9.13 Data collection and ISI trimming.   whereMAIj accounts comprehensively for multiple-access-interference of rank r s   1 ; B ð y 0 Þ is ablock diagonal                       D                                s matrix of the form B ð y 0 Þ¼diagð a 0 ; 0 ; a 0 ; 1 ; ... ; a 0 ; N   1 Þ ,and A 0 ¼ A 0   I M ,where I M is the M · M identity matrix and                           2                                      3                             d 0 ½ N   1   d 0 ½ N   2   ... d 0 ½ 0                             6   d ½ N    d ½ N   1   ...    d ½ 1   7                       s   6    0         0                 0     7                      A 0 ¼ 6    .           .               .    7                ð 9 : 25Þ                           4     .           .               .    5                              d 0 ½ L   1   d 0 ½ L   2   ... d 0 ½ L   N               tr tr H                          tr Let R tr ¼ E f x x g be the autocorrelation matrix of x .Weform asample-average estimate                                           1 XJ   1                                    RR ^ ¼     x trx tr H                          ð 9 : 26Þ                                      tr  J     j j                                            j ¼ 0                                          tr                  ^    ^ ^ ^ H based on the truncated J available input vectors x j , j ¼ 0 ; 1 ; ... ; J   1. If RR tr ¼ QQ LLLLLLLLLL QQ represents the eigen-                ^                    ^                  ^      ^ decomposition of RR tr,wherethe columns of QQ are the eigenvectors of RR tr and LL is adiagonal matrix consisting                    ^ of the eigenvalues of RR tr,then we use the eigenvectors that correspond to the M ð L   N þ 1 Þ ð 2 Q   1 Þ                                                                 ^ smallest eigenvalues to deﬁne our estimated noise subspace. Let the matrix UU n of size ½ M ð L   N þ 1 Þ  · ½ M ð L   N þ 1 Þ ð 2 Q   1 Þ  consist of these ‘‘noise eigenvectors.’’ We estimate c 0 and y 0 indirectly through an estimate of the MN · 1vector                                           D                                       h 0 ¼ B ð y 0 Þ c 0                         ð 9 : 27Þ  We estimate h 0 as the vector that minimizes the norm of the projection of the signal of the user of interest,                                        ^ user 0 , A 0 h 0 ,onto the estimated noise subspace UU n                                                                    ^                  H ^                 ^                     hh 0 ¼ arg min   ð A 0 h 0 Þ UU n   subject to k hh 0 k¼1     ð 9 : 28Þ                               h 0  The solution to this constrained minimization problem is the eigenvector that corresponds to the minimum             H ^ ^ H               ^                                    ^ eigenvalue of A 0 UU n UU n A 0 .After obtaining hh 0 ,wemay extract the desiredvectors cc^ 0 and yy 0 by applying least-                    ^ squares (LS) ﬁtting to hh 0 .Then, the estimate vv^ 0 is completely deﬁned by Equation (9.22).9 -20                                  Broadcasting and Optical Communication Technology    Sincethe above channel estimation method is based on ablind second-order criterion, the phase  information is absorbed, which means that the estimate, vv^ 0 ,isphase ambiguous. Inherently,adaptiveﬁlter estimators that utilize aphase ambiguous estimate of v 0 are also phase ambiguous. Next, we consider the recovery(correction) of the phase of linear ﬁlters when the vector, v 0 ,isknown within aphase ambiguity. Phase Recovery  Without loss of generality,let vv~ 0 denote aphase ambiguous version of v 0 ,i.e.,                                           j c                                        vv~ 0 e ¼ v 0                              ð 9 : 29Þ where c is the unknown phase. We consider the class of linear ﬁlters, w 2 C M ð L þ N   1 Þ that are functions of the  S-T RAKE vector, v 0 ,and share the following property                                                    j c                                     w ð v 0 Þ¼w ð vv~ 0 Þ e                       ð 9 : 30Þ  Such ﬁlters include: (i) the S-T RAKE ﬁlter itself, v 0 , (ii) the S-T MMSE/MVDR ﬁlter of Equation (9.6), and (iii) the auxiliary-vector sequence of S-T ﬁlters, f w k g .   As seen by Equation (9.30), for this class of ﬁlters the phase ambiguity of vv~ 0 leads to aphase ambiguous linear ﬁlter, w ð vv~ 0 Þ .Phase ambiguity in digital communications can be catastrophic sinceitmay result in receivers that exhibit BER equal to 50%. Given vv~ 0 ,weattempt to correct the phase of w ð vv~ 0 Þ as follows. We choose the value of the phase correction parameter c that minimizes the mean-square-error (MSE) between                                       j c H the output of the phase corrected ﬁlter, ½ w ð vv~ 0 Þ e   x ,and the desiredinformation bit b 0 (Figure9.14)                       ^                    j c H     2                     cc ¼ arg min E fj½w ð vv~ 0 Þ e   x   b 0 j g ; c 2 ½ p ; p Þð9 : 31Þ                              c  The optimum phase correction value according to the above criterion is given by                                  ^              H                                 cc ¼ anglef w ð vv~ 0 Þ E f x b 0 gg              ð 9 : 32Þ  Essentially,Equation (9.32) suggests projectingthe phase ambiguous w ð vv~ 0 Þ ﬁlter onto the ideal S-T RAKE ﬁlter v 0 ¼ E f x b 0 g .However, E f x b 0 g is certainly not known.Since we haveassumed that apilot information bit sequence of length J p is included in each packet, the expectation E f x b 0 g can be sample-average estimated by   P 1  J p    j ¼ 1 x j b 0 ð j Þ ,where b 0 ð j Þ , j ¼ 1 ; 2 ; ... ; J p ,isthe j th pilot information bit and x j is the corresponding input J p data vector.Then, the phase-corrected adaptiveﬁlter estimate is given by                                          8          2          3 9                                          <            X J p     =                      w ð vv^ ; RR ^ Þ ; cc ^ ¼ angle w ð vv^ ; RR ^ Þ H 4 x b ð j Þ 5 ð 9 : 33Þ                          0               :     0          j 0   ;                                                       j ¼ 1   Since j represents the packet size of the DS-CDMA system and J p is the number of mid-amble pilot                                    J information bits per packet, then the ratio p quantiﬁes the wasted bandwidth due to the use of the pilot bit                                    J                                         w(~v ) H e − j ψ x                        x     ~            0                   b 0                            w(v0 ) e j ψ              sgn(Re{·})                                                  +                                                   −                                                  b 0              FIGURE 9.14 Supervised (pilot-assisted) phase correction for the S-T linear ﬁlter, w ð vv~ 0 Þ .Low Sample Support Adaptive Parameter Estimation and Packet-Data Detection         9 -21                J sequence.Ideally, p is to be kept small. As we will see in the next section, afew pilot bits (on the orderof5                J bits) are sufﬁcient for effective recoveryofthe ﬁlter phase. As anumerical example, when the packet size is set                                J at J ¼ 256 and J ¼ 5ischosen, then p . 2 % only.              p                 J 9.6    Joint Parameter Estimation and Packet-Data Detection  Forconvenience in notation we introduce the variable S 0 to denote the P · N known signal waveform matrix                                                            s of the user of interest. Forasingle antenna receiver, S 0 takes the form of A 0 given in Equation (9.25), while for an antenna-arrayreceiver it is given by A 0 B ð y 0 Þ ( y 0 is assumed known in this section). Thus, the j -th received vector of the user of interest within the received data packet can be written as follows                                   p ﬃﬃﬃ                             x j ¼ b j E 0 S 0 c 0 þ z j ; j ¼ 1 ; ... ; J         ð 9 : 34Þ                      N where, we recall, c 0 2 C is the vector of the multipath channel coefﬁcients that are assumed to remain constant during the transmission of the data packet (quasi-static fading), and z j , j ¼ 1 ; ... ; J ,isasequenceof independent identically distributed zeromean Gaussian vectors with unknown covariance matrix R z ,that represent comprehensively channel interferenceand noise that is independent of the data sequence of user of  interest, b j , j ¼ 1 ; ... ; J .Wenote that the Gaussian distribution assumption for the general interference plus noise component is only introduced heretofacilitate the development of the packet-data receiver.Inthe simulation studies, the performanceofthe packet-data receiver will be examined under arealistic communication system setup with non-Gaussian distributed MAI.                                                         D   The probabilitydensity function (pdf)ofthe observations, X ¼ ½ x 1 ; x 2 ; ... ; x J   ,conditioned on the                              T transmitted bits, b ¼½b 1 ; b 2 ; ... ; b J   ,can be expressed in the following compact form                                                 p     ﬃﬃ        p ﬃﬃ                                      1             1        T          T H              f ð X j b ; E ; S ; c ; R Þ¼ e trace   R z ðÞX   E 0 S 0 c 0 b ðÞX   E 0 S 0 c 0 b ð 9 : 35Þ                      0  0  0  z     PJ   J                                   p  j R z j  GLRTDetection: Known Channel In this section, we treat receiver design as ajoint optimization problem wherereceiver parameter estimation is coupled with packet-data detection. In particular,for each hypothesis (information bit combination) we maximize the conditional likelihood with respect to the unknown receiver parameters and then choose the most likely hypothesis. The solution to this optimization problemisageneralized likelihood ratio test (GLRT)-typereceiver (it is alikelihood ratio scheme that utilizes maximum likelihood (ML) estimates of the unknown parameters). Formal derivation of the GLRTpacket-data detector is provided by the next proposition [20]. Proposition 9.1 The GLRTtest for the detection of the data packet b of size Jinthe presence of complex  Gaussian disturbance with unknowncovariance matrix R z is                                    n                o                    ^                    bb GLRT ¼ arg max max f ð X j b ; v 0 ; R z Þ ¼ arg max l 1 ð b Þð9 : 36Þ                                 b    R z                     b  where               D   T H ^   1        H ^   1        T H ^   1      H ^   1        l 1 ð b Þ¼J b X RR ð J Þ v 0 þ J v 0 RR ð J Þ Xb þðb X RR ð J Þ XbÞðv 0 RR ð J Þ v 0 Þ                    T H ^   1     H ^                 ðb  X  RR ð J Þ v 0 Þðv 0 RR ð J Þ XbÞð9                            : 37Þ     p ﬃﬃﬃ                 ^   D 1  H v 0 ¼ E 0 S 0 c 0 and RR ð J Þ¼J XX is the sample average received data correlation matrix . We note that direct implementation of test in Equation (9.36) has complexityexponential in the packet size J .9 -22                                  Broadcasting and Optical Communication Technology    The GLRTtest in Equation (9.36) can be contrasted with the standard desired signal absent SMI detection scheme summarized below                                                                            bb^   ¼  sgn Re v H RR ^   1 ð K Þ x ; j ¼ 1 ; ... ; J     ð 9 : 38Þ                         j SMI  dsa      0  z     j               P       ^    D 1  K    H where RR z ð K Þ¼K k ¼ 1 z k z k based on pure disturbance observations, z k , k ¼ 1 ; ... ; K ,that are independent from x j , j ¼ 1 ; ...; J ,and the subscript part‘‘dsa’’ is used to emphasize that the desired-signal-absent SMI detector utilizes additional pure disturbance observations. When pure disturbance observations (secondary  data), z k , k ¼ 1 ; ... ; K are not available,apopular version of the test in Equation (9.38) utilizes directly the sample-average correlation matrix of the (desired-signal-present) received data, RR ^ ð J Þ ,evaluated using the  same received data x j , j ¼ 1 ; ... ; J ,that are processed by the detector.This results in the widely known SMI receiver,                          bb^ ¼  sgn½ Ref v H RR ^   1 ð J Þ x g ; j ¼ 1 ; ... ; J  ð 9 : 39Þ                          j SMI         0       j  Recent analytical results on shortdata recordadaptiveﬁltering [14,16,19] indicate that for ﬁnite sample supportofequal size ð K ¼ J Þ the test in Equation (9.38) signiﬁcantly outperforms the test in Equation (9.39) in terms of BER. Yet, as is shown [20], for sufﬁciently large transmitted energyper bit, the packet-data GLRTdetector in Equation (9.36) can achieve approximately the same average1 BER performanceasthe test in Equation (9.38) that utilizes extra K ¼ J   1independent pure disturbance observations (secondarydata), i.e.,                                        BER     ð J Þ                                 lim       GLRT      <  1                          ð 9 : 40Þ                                 g ! 1 BERSMI  dsað J   1 Þ         D  H   1 where g ¼ v 0 R z v 0 .   Using the above result, we can derive an approximation of the average BER of the coherent GLRTpacket- data detector that operates on adata packet of size J > P þ 2asfollows [20]                                            q ﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃ    q ﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃ                           2    p ﬃﬃﬃﬃ 1           p ﬃﬃ    1           p ﬃﬃ             BER     ð J Þ < Q   2 m þ  Q    2 m þ 2 3 s þ  Q    2 m   2 3 s       ð 9 : 41Þ                 GLRT      3           6                   6          J   P þ 1         ð J   P þ 1 ÞðP   1 Þ where m ¼ D     g ,and s 2 ¼ D           g 2             J                 J 2 ð J þ 1 Þ GLRTDetection:Unknown          Channel  When the channel is unknown, i.e., when E 0 and c 0 in Equation (9.34) are unknown, the GLRTreceiver is given by the following proposition [20]. Proposition 9.2 The GLRTtest for the detection of the data packet, b , of size Jtransmitted over an unknown  linear channel in the presence of complex Gaussian disturbance of unknowncovariance matrix R z is given by                                                                           ^                   bb GLRT ¼ arg max max f ð X j b ; S 0 ; c 0 ; R z Þ ¼ arg max l 2 ð b Þð9 : 42Þ                                b   c 0 ; R z                   b where                           J b T X H ½ XXH     1 S ð S H ½ XXH     1 S Þ   1 S H ½ XXH     1 Xb                   l ð b Þ¼              0  0         0    0                       ð 9 : 43Þ                   2                  J 2   J b T X H ½ XXH     1 Xb    1 The average BER of apacket-data detector is deﬁned as the expected number of bits in error divided by the packetsize.Low Sample Support Adaptive Parameter Estimation and Packet-Data Detection         9 -23     We note that the function l 2 ð b Þ in Equation (9.43) is ambiguous with respect to the phase of b .Inpractice, phase ambiguityisresolved either by using apilot sequence or by employing differential modulation at the transmitter;the rest of this section deals exactly with these twoapproaches. Pilot Assisted GLRTDetection Proposition 9.3 Let f b g J p and f b g J denote, respectively,J knownpilot bits and J   J unknown                      j j ¼ 1   i i ¼ J p þ 1           p                      p information bits within the data packet, b , of size Jthat is transmitted over an unknownlinear channel in the presence of complex Gaussian disturbance of unknowncovariance. Then, the pilot assisted GLRTdetector of f b g J is given by   i j ¼ J p þ 1                              f bb^ g J   ¼ arg max    l ð b Þð9                     : 44Þ                                i GLRT i ¼ J p þ 1     2                                                b i ; i > J p þ 1                                                            J p   It is interesting to note that in Equation (9.44) the pilot sequence f b i g i ¼ 1 is not used to directly estimate the phase in an explicit manner,but is incorporated implicitly in the GLRTrule. It is also interesting to observe that the GLRTtest expression in Equation (9.44) maintains the same structureasinEquation (9.43). We conclude the treatment of the pilot assisted GLRTdetection by providing aclosed form approximation of the BER performanceofthe abovepilot assisted GLRTdetector for adata packet of size J > P þ 3[20]          BERGLRT  pilotð J Þ < BERSMI  dsað J   2 Þð9                                : 45Þ                                            q ﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃ    q ﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃ                           2    p ﬃﬃﬃﬃ 1           p ﬃﬃ    1           p ﬃﬃ                        <   Q   2 m þ   Q   2 m þ 2 3 s þ   Q    2 m   2 3 s       ð 9 : 46Þ                           3           6                   6          J   P      ð J   P Þð J   1 Þ where m ¼ D  g , s 2 ¼ D       g 2 ,and BER   ð J   2 Þ is the BER of the coherent desired-signal-            J             J 3             SMI  dsa absent SMI detector in Equation (9.38) that would require perfect knowledge of c 0 and utilize K ¼ J   2 independent pure disturbanceobservations. DPSK GLRTDetection As an alternativetopilot signaling,phase ambiguityofthe GLRTdetector in Equation (9.43) can be resolved by employing differential encoding at the transmitter.Toavoid redundancy in our presentation,inthis section we keep the size of the transmitted packet equal to, J while the number of the information bits embedded in                                       J   1 the differentiallyencoded packet is J   1, f b j g j ¼ 1 .The differentially encoded bits themselvesare e 0 ¼þ1 and e j ¼ e j   1 b j , j ¼ 1 ; 2 ; ... ; J   1. The j th received vector, x j ,isstill of the form of Equation (9.34) with e j in placeof b j .Given the transmitted bits, e j , j ¼ 0 ; 1 ... J   1, the information bits can be uniquely determined by b j ¼ e j   1 e j , j ¼ 1 ; ... ; J   1.   Under unknown input statistics and channel coefﬁcients, the common approach has been to produce estimates of the unknown quantities and insertthe estimates in the J-symbol (or 2-symbol) block differential detector.Instead, what we propose in this section is aGLRT-type scheme that combines into asingle optimization effortestimation of interference-plus-noise covariance matrix and channel coefﬁcients and detection of packet data. The following proposition identiﬁes the DPSK GLRTpacket-data detector [20].                                                                       J   1 Proposition 9.4 The DPSK GLRTdetector of differentially encoded packet data, f b j g j ¼ 1 , transmitted over an unknownlinear channel in the presence of complex Gaussian disturbance of unknowncovariance is given by                                 f ee^ g J   1 ¼ arg max l ð e Þð9                   : 47Þ                                   j GLRT j ¼ 1      2                                                e j ; j > 1                          bb^  ¼ ee^    ee^ ;  j ¼ 1 ; 2 ; ... ; J   1             ð 9 : 48Þ                           j GLRT j   1 GLRT j GLRT         D          T where e ¼ ½ e 0 ; ... e J   1   .   Finally,weprovide the following closed form approximation of the BER performanceofthe above DPSK GLRTdetector for adata packet of size J > P þ 3derived in Ref. [20]9 -24                                  Broadcasting and Optical Communication Technology            BERGLRT  DPSKð J Þ < BERSMI  dsa; DPSKð J   2 Þð9                          : 49Þ                                             q ﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃ    q ﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃ                            4    p ﬃﬃﬃﬃ 1           p ﬃﬃ    1           p ﬃﬃ                          <   Q   2 m þ  Q    2 m þ 2 3 s þ   Q   2 m   2 3 s      ð 9 : 50Þ                            3           3                   3           J   P      ð J   P Þð J   1 Þ where m ¼ D  g , s 2 ¼ D       g 2 ,and BER       ð J   2 Þ is the BER of adetection scheme that            J             J 3             SMI  dsa; DPSK                                                     J utilizes the coherent desired-signal-absent SMI detector of f e j g j ¼ 1 based on K ¼ J   2independent pure                                                                                ^ disturbanceobservations under perfect knowledge of c 0 ,followed by the differential decoder bb j ¼ ee^ j   1 ee^ j , j ¼ 1 ; ... ; J   1.  ImplementationIssues Direct implementation of the GLRTdetectors has complexityexponential in the number of bits. Below,we consider aproceduretoobtain effective suboptimum implementations with linear complexity. The procedure is basically aparallel implementation of T sequential bit updates with D steps for each sequential bit update. The  procedureissummarized   below:Westartwith     an  initial estimate of the packet bits, ^      ^       ^                                                     ^ ð t Þ bb ð 0 Þ¼½ bb 1 ð 0 Þ ; ... ; bb J ð 0 Þ ,replicated T times for T distinct sequential bit updates, bb ð d Þ , t ¼ 1 ; 2 ; ... ; T ,                                                                              J d ¼ 0 ; 1 ; ... ; D .With each sequential bit update, t ,weassociate abit update index order, f p t ð j Þgj ¼ 1 ,that is a distinct permutation of f 1 ; 2 ; ... ; J g .Ateach step, d ,wecheck one bit in each of T sequential bit updates, namely bb^ ð t Þ .Upon completion, step, D ,wedeclareasour approximate GLRTdecision the most likely        p t ð d mod J Þ among the T bit vectors bb^ ð t Þ ð D Þ , t ¼ 1 ; ... ; T .  Suboptimum   GLRTAlgorithm   Initialization: Number of parallel sequential bit updates, T ;search depth D ;                                 ^ ð t Þ ^    ^       ^    T              Initial decision vector bb ð 0 Þ : ¼½bb 1 ð 0 Þ ; bb 2 ð 0 Þ ; ...; bb J ð 0 Þ  ; t ¼ 1 ; 2 ; ... ; T ;                                        J              Bit update index orders, f p t ð j Þgj ¼ 1 ; t ¼ 1 ; 2 ; ... ; T .   Forstep d ¼ 1 ; 2 ; ... D        Forpath t ¼ 1 ; 2 ; ... ; T             i : ¼ p t ð d mod J Þ n                    o            ^ ð t Þ                ^ ð t Þ     ð t Þ            bb ð d Þ : ¼ arg max max f ð X jfbb ð d   1 Þgj 6¼ i ; b ; v 0 ; R z Þ             i          ð t Þ       j          i                       b    R z                        i            ^ ð t Þ ^ ð t Þ            bb j ð d Þ : ¼ bb j ð d   1 Þ ; j 6¼ i        end   end   ^   bb GLRT : ¼ arg max l 1 ; 2 ð b Þ :           b 2 f bb^ ð 1 Þ ð D Þ ; ...; bb^ ð T Þ ð D Þg    The complexityofone bit update is of order, O ð P Þ .Hence, the overall complexityofthe above algorithm for the detection of adata packet of size J is of order O ð JP2 ÞþO ð JPÞþO ð P 3 ÞþO ð DTPÞ ,which includes the cost of the initial evaluation of RR ^ ð J Þ and RR ^   1 ð J Þ .Wenote that agood initial estimate mayallow relatively small values for T and D .Inthis context,wecan regularly reinitialize the parallel searchalgorithm using the best sequence estimate among the current T alternatives.  Simulation   Studies In this section, we examined the performanceofthe proposed GLRTschemes for apacket-data DS-CDMA communication system.1 At all times, the GLRTdetectors are implemented viathe linear complexity    1 The combined effect of DS-CDMA multiple access interference (MAI) and AWGN is Gaussian-mixturedistributed and not plain Gaussian. It is interesting to examine how the proposed GLRTdetectors perform in such an environment.Low Sample Support Adaptive Parameter Estimation and Packet-Data Detection         9 -25  algorithm with T ¼ 16 and D ¼ 6 J .Initial bit estimates are either taken by conventional matched-ﬁlter (MF) outputs (Case Study 9.1) or set arbitrarily (Case Studies 9.2 and 9.3).  DS-CDMA Case Study 9.1    Synchronous Multiuser System and Single-Path Channel We consider asystem with 10 synchronous users with Gold signatures of length, L ¼ 31. We select a‘‘user of interest’’and havethe SNR’s of the interfering users ﬁxed in the range, [6dB,11dB]. In this study,weassume exact knowledge of the channel of the user of interest. We comparethe BER of the GLRTdetector with the BER of the MF,SMI, LMS (step size 10  4 ), RLS (initialization parameter 100), and the desired-signal-absent SMI detector in Equation (9.38) that assumes availabilityof J   1additional pure disturbanceobservations. In Figure9.15 and Figure9.16, we plot the BER as afunction of the SNR of the user of interest and the packet size J ,respectively.Inview of the nearly overlapping analytical and simulated GLRTBER curves, we may claim that our linear cost GLRTimplementation performs veryclose to full GLRTand Equation (9.41) provides an accurate approximation of the BER of the GLRTpacket-data detector.Furthermore,the GLRTpacket-data                          100                        10− 1                        10− 2                        10− 3                       BER                                MF                       10− 4    SMI                                SMI− dsa                                LMS                       10− 5    RLS                                GLRT simulated                                GLRT by (41)                       10− 6                           3   4    5   6   7    8   9   10  11   12                                       SNR of user of interest            FIGURE 9.15 Case Study 9.1: BER as function of the SNR of the user of interest ð J ¼ 127Þ :                           100                                                     MF                                                     SMI                                                     SMI− dsa                                                     LMS                        10− 1                        RLS                                                     GLRT simulated                                                     GLRT by (41)                         10− 2                       BER                         10− 3                          10− 4                           0    50    100  150   200  250   300  350                                           Packet Size J   FIGURE 9.16 Case Study 9.1: BER as function of the packet size J .The SNR of the user of interest is ﬁxed at 9dB.9 -26                                  Broadcasting and Optical Communication Technology  detector outperforms signiﬁcantly the SMI, LMS, and RLS detectors and performs nearly the same as the desired-signal-absent SMI detector in Equation (9.38) that requires J   1 additional pure disturbance observations.  DS-CDMA    Case Study 9.2 Asynchronous Multipath Fading Channel: Pilot-Assisted Signaling We consider the same setup as in Case Study 9.1, except that nowthe users transmit asynchronously and each user channel has 3resolvable paths. The path coefﬁcients are modeled as independent complex  Gaussian random variables all of unit variance. The length of the pilot sequence is ﬁxed at J p ¼ 10. We compare the BER of the GLRTdetector with the BER of the RAKE-MF,the desired-signal-absent SMI and SMI detectors in Equation (9.38) and Equation (9.39), and the LMS and RLS detectors. We note that in this study the GLRTdetector assumes no knowledge of the channel while all other detectors assume exact knowledge of the channel.Inaddition, the desired-signal-absent SMI detector in Equation (9.38) uses J   2 extra pure disturbanceobservations that are assumed to be available. It is worth noting that the pilot sequence is incorporated and processed internally and elegantly by the GLRTalgorithm without the need for aseparate phase estimation stage.  DS-CDMA    Case Study 9.3 Asynchronous Multipath Fading Channel: DPSK Signaling We consider the same setup as in Case Study 9.2, except that the transmitter now uses DPSK encoding instead of pilot signaling; hence, at the receiver end adifferential decoderisneeded to recover the information bits. We compare the BER of our DPSK GLRTdetector with the BER of the DPSK version of the following detectors: RAKE-MF,SMI, LMS, RLS, and ideal MMSE. A2-symbol differential decoder is used in all cases. The coherent desired-signal-absent SMI detector is also included as areference. We note that the DPSK GLRTdetector assumes no knowledge of the channel, while the RAKE-MF,LMS, RLS, SMI, and ideal MMSE  detectors assume perfectly known path coefﬁcients up to an unknownphase (phase ambiguity is resolved by differential encoding/decoding). In addition, the ideal MMSE detector assumes perfectly known interference-plus-noise covariance matrix, and the coherent desired-signal-absent SMI detector requires perfect knowledge of the path coefﬁcients (including the phase) and J   2additional pure disturbance observations. In Figure 9.18, we repeat the studies of Figure 9.17. We note that for data packets of size J ¼ 250 and higher the DPSK GLRTdetector outperforms even the ideal MMSE (2-symbol decoder) detector.                         100                                                     RAKE− MF                                                     SMI                                                     SMI− dsa                                                     LMS                                                     RLS                        10− 1                        GLRT pilot simulated                                                     GLRT pilot by (45)                       BER                        10− 2                          10− 3                            50    100   150    200    250   300    350                                           Packet Size J   FIGURE 9.17 Case Study 9.2: BER as function of the packet size J .The SNR of the user of interest is ﬁxed at 9dB.Low Sample Support Adaptive Parameter Estimation and Packet-Data Detection         9 -27                          100                                                    RAKE− MF DPSK                                                    SMI DPSK                                                    Coherent SMI− dsa DPSK                                                    LMS DPSK                         1                          RLS DPSK                        10                          GLRT− DPSK simulated                                                    GLRT− DPSK by (49)                                                    Ideal MMSE DPSK                         102                       BER                          103                          104                          50    100    150   200    250    300   350                                         Packet Size J   FIGURE 9.18 Case Study 9.3: BER as function of the packet size J .The SNR of the user of interest is ﬁxed at 9dB.  9.7    Concluding Remarks  Wireless cellular and personal communications services (PCS) networks experiencedsigniﬁcant growthinthe past few years, driven by astrong market interest for highly mobile, widely accessible,two-way voiceand data communications. Current research efforts are focusing on system improvements to meet future demand and qualityofservice requirements. User capacity increase may be soughtinthe form of asynergy of effective multiple accessing schemes and advanced receiver technology(for example, code-division-multiple-access with adaptive antenna arrays). Improvedreceiver output SINR and BER performancemay be sought in the form of intelligent modulation techniques, as well as intelligent signal processing at the receiver end of the communications link. However,realistically,receiver output SINR and BER improvements in rapidly changing channel environments can be achieved only by means of adaptiveshort-data-record-optimized receiver designs (as opposed to designs based on ideal asymptotic optimization solutions).   In this chapter,weﬁrst focused on packet data receivers that are implemented by using sample average estimates in placeofideal statistics in the optimum receiver formula and perform disjoint parameter estimation and packet-data detection. In particular,weexamined linear MMSE/MVDR-typereceivers. We presented twoalternativemethods that approximate the optimum solution under perfectly known input statistics (input autocorrelation matrix and input/desired-output cross-correlation vector): The generalized sidelobe canceller,and the auxiliary-vector ﬁlters. When the input statistics are unknown and estimated, these approximate solutions provide estimates of the optimum solution withvarying performancelevels (output SINR and BER). When estimation is based on ashortdata record,that is, when system adaptation and redesign has to be performed with limited data support(which is the case for most systems of practical interest), then the performancedifferences become even more pronounced.   Aviable solution for adaptiveMMSE/MVDR system designs under limited data supportisprovided by the auxiliaryvector (AV) algorithm. AV estimators exhibit varying bias/covariance characteristics: The bias of the generated estimator sequence decreases rapidly to zerowhile the estimator covariance trace rises slowly from zero (for the initial, ﬁxed-valued, matched-ﬁlter estimator) to the asymptotic covariance trace of the SMI ﬁlter. Sequences of practical estimators that offer such control over favorable bias/covariance balancepoints are always aprime objective in the estimation theoryliterature. Indeed, under quasi-static fading over the duration of a packet and packet-rate adaptation, members of the generated sequenceofAVestimators outperform in MS estimation error LMS/RLS-type, and SMI ﬁlter estimators. In addition, the troublesome, data-dependent tuning of the real-valued LMS learning gain parameter,orthe RLS initialization parameter is replaced by an integer9 -28                                  Broadcasting and Optical Communication Technology  choice among the ﬁrst several members of the estimator sequence.Thus, we presented two data-driven criteria for the selection of the best AV ﬁlter estimator in the sequence.   Next, we considered the GLRT-type detection schemes that perform joint estimation of the unknown system parameters and detection of the packet-data. In particular,for the known channel case, we developed a coherent GLRTdetector,while for the unknown channel case we derived apilot assisted GLRTdetector (the pilot signal is implicitly used to resolve phase ambiguity)and aDPSK GLRTdetector.Wealso derived analytical expressions for the BER performanceofeach proposed GLRT-type scheme, and compared it to the corresponding conventional estimate-and-plug-in detector that replaces unknown parameters in its ideal formula by estimates obtained through aseparate estimation process. In all cases, the GLRTschemes maintained the same elegant corestructure,regardless of known or unknown channels and pilot or DPSK signaling. Finally,wedeveloped suboptimum implementations of the GLRTpacket-data detectors that exhibit linear complexityinthe packet size.  Acknowledgments This work was supported in partbythe National Science Foundation under Grant CCR-0219903 and by the Ofﬁce of Scientiﬁc Research under Grant FA9550–04–1-0256R.  Deﬁning   Terms Adaptive ﬁlter: Aself-designing ﬁlter that recalculates itself from data to followchanges in the statistics of      the environment. Antenna array:  An electromagnetic wavereceiving system composed of several antennae that is capable of      directional reception/transmission. Auxiliary-vector ﬁlters: Asequence of linear ﬁlters that approximate the optimum MMSE/MVDR linear      ﬁlter. Auxiliary-vector estimates: Asequenceofestimates of the optimum MMSE/MVDR linear ﬁlter that are      generated by aconditional statistical optimization procedureand are built as alinear combination of      properly selected nonorthogonal auxiliaryvectors. Early,nonasymptotic elements of this sequence      exhibit superior estimation performanceunder low sample supportconditions. Biased estimator: An estimator whose mean value is not equal to the true parameter value. Code-division-multiple-access: Aphysical layer multiple access technique that allows manyuser signals      to coexist in time and frequency and, yet, be recovered/separated reliably as necessary. This is achieved      by assigning aunique codesequence to each user signal, known as signature. Cross-validation: Astatistical technique that validates amodel on adata set different from the one used      for estimation of model parameters. Generalized likelihood ratio test: Alikelihood ratio test that utilizes maximum-likelihood estimates      (MLEs) of the unknown parameters. J -divergence: Adistancemeasure between twoprobabilitydistributions. Linear MMSE  ﬁlter: Alinear ﬁlter that minimizes the mean-square-error between the desired response      and the actual ﬁlter output. Linear MVDR  ﬁlter: Alinear ﬁlter that minimizes the mean ﬁlter output powersubject to the constraint      that the ﬁlter maintains acertain response with respect to agiven input. Small-sample-support estimate: Aparameter estimate that is calculated based on adata record of small      (nonasymptotic) size. Smartantenna:    An antenna array with an advanced post-reception signal processing circuitrythat      intelligently suppresses or cancels unwanted signals.  References  1.  S.N. Batalama, ‘‘Packet-rate adaptive receivers for mobile communications,’’ in WileyEncyclopedia of      Telecommunications,vol. 4, J. Proakis, Ed., NewYork: Wiley,2003, pp.1886–1905.Low Sample Support Adaptive Parameter Estimation and Packet-Data Detection         9 -29   2.  J. Proakis, Digital Communications,3rd ed., NewYork: McGraw-Hill,1995.  3.  E. Dahlman, B. Gudmundson, M. Nilsson, and J. Skold, ‘‘UMTS/IMT-2000 based on wideband CDMA,’’      IEEE Commun. Mag.,vol. 36, pp.70–80, 1998.  4.  D.A. Pados and S.N. Batalama, ‘‘Joint space-time auxiliary-vector ﬁltering for DS/CDMA systems with      antenna arrays,’’ IEEE Trans. Commun.,vol. 47, pp.1406–1415, 1999.  5.  S. Haykin, Adaptive Filter Theory ,2nd ed., Englewood Cliffs, N.J.: Prentice Hall, 1991.  6.  G.H. Golub and C.F.Van Loan, Matrix Computations,Baltimore, MD: The Johns Hopkins University      Press, 1990.  7.  K.A. Byerly and R.A. Roberts, ‘‘Output power based partially adaptive array design,’’ in Proc. Asilomar      Conf. Signal. Syst. Comput.,CA: Paciﬁc Grove, 1989, pp.576–580.  8.  S.N. Batalama, M.J.Medley,and D.A. Pados, ‘‘Robust adaptiverecoveryofspread-spectrum signals with      shortdata records,’’ IEEE Trans. Commun.,vol. 48, pp.1725–1731, 2000.  9.  D.A. Pados and G.N. Karystinos, ‘‘An iterative algorithm for the computation of the MVDR ﬁlter,’’ IEEE      Trans. Signal Process.,vol.49, pp.290–300, 2001. 10.  D.A. Pados and S.N. Batalama, ‘‘Low-complexityblind detection of DS/CDMA signals: auxiliary-vector      receivers,’’ IEEE Trans. Commun.,vol. 45, pp.1586–1594, 1997. 11.  D.A. Pados, T. Tsao,J.H. Michels, and M.C. Wicks, ‘‘Joint domain space-time adaptiveprocessing with      small training data sets,’’ in Proc. IEEE Radar Conf.,Dallas, TX, May1998, pp.99–104. 12.  H. Qian and S.N. Batalama, ‘‘Data-record-based criteria for the selection of an auxiliary-vector      estimator of the MMSE/MVDR ﬁlter,’’ IEEE Trans. Commun.,vol. 51, pp.1700–1708, 2003. 13.  H.V.Poor and S. Verdu´ ,‘‘Probabilityoferror in MMSE multiuser detection,’’ IEEE Trans. Info.Theory ,      vol. 43, pp.858–871, 1997. 14.  I.N. Psaromiligkos and S.N. Batalama, ‘‘Recursiveshort-data-recordestimation of AV and MMSE/      MVDR linear ﬁlters for DS-CDMA antenna array systems,’’ IEEE Trans. Commun.,vol.52, pp.136–148,      2004. 15.  I.N. Psaromiligkos, M.J.Medley,and S.N. Batalama, ‘‘Rapid synchronization and combined demodu-      lation for DS/CDMA communications. Part I: Algorithmic developments,’’ IEEE Trans. Commun.,      vol. 51, pp.983–994, 2003. 16.  I.N. Psaromiligkos and S.N. Batalama, ‘‘Data record size requirements for adaptivespace-time      DS/CDMA   signal detection and direction-of-arrival estimation,’’ IEEE Trans. Commun.,vol.52,      pp.1538–1546, 2004. 17.  P. Chaudhury, W. Mohr,and S. Onoe, ‘‘The 3GPP proposal for IMT-2000,’’ IEEE Commun. Mag.,vol. 37,      pp.72–81, 1999. 18.  I.N. Psaromiligkos and S.N. Batalama, ‘‘Blind self-synchronized demodulation of DS-CDMA      communications,’’ in Proc. IEEE ICC 2000 —Int.Conf. Commun.,New Orleans, LA, June 2000,      pp.2557–2560. 19.  J.M. Farrell,I.N. Psaromiligkos, and S.N. Batalama, ‘‘Design and analysis of supervised and decision-      directed estimators of the MMSE/LCMV ﬁlter in data-limited environments,’’ in Proc. SPIE, Dig.      Wireless Commun. Conf.,vol. 5100, Orlando,FL, 2003, pp.227–237. 20.  H. Qian, S.N. Batalama, and B. Suter,‘‘Novel GLRTpacket-data detectors,’’ in Proc. IEEE ICASSP 2004      —Int. Conf. Acoust. Speech Signal Process.,Montreal, Canada, May2004. 21.  I.S. Reed, J.D. Mallet, and L.E. Brennan,‘‘Rapid convergence rate in adaptive arrays,’’ IEEE Trans.      Aerospace Electron. Syst.,vol. 10, pp.853–863, 1974. 22.  J.S. Goldstein and I.S. Reed, ‘‘Reduced-rank adaptiveﬁltering,’’ IEEE Trans. Signal Process.,vol.45,      pp.492–496, 1997.  Further Information IEEE Transactions on Information Theory is abimonthly journal that publishes papers on theoretical aspects of      estimation theoryand in particular on transmission, processing,and utilization of information.9 -30                                  Broadcasting and Optical Communication Technology  IEEE Transactions on Signal Processing is amonthly journal which presents applications of estimation theoryto      speech recognition and processing,acoustical signal processing, and communication. IEEE Transactions on Communications is amonthly journal presentingapplications of estimation theoryto      data communication problems, synchronization of communication systems, and channel equalization. IEEE Transactions on Aerospace and Electronic Systems is aquarterly journal presenting developments in sensor      systems, communications systems, command and control centers, avionics, spacesystems, military      systems, and digital signal processing simulators.                                                                         10                                          Bandwidth Efﬁcient                                    ModulationinOptical                                                Communications                                 10.1  Introduction .................................................................... 10-1                               10.2  Bandwidth Efﬁcient Modulation (BEM) Concept .................. 10-2                                      Multiplexing Limitations * Nyquist Theoretical Minimum                                      Bandwidth Requirement * Sampling AmplitudeModulated                                      Signals * Shannon–Hartley Capacity Theorem * ForwardError                                      Correction (FEC) * Bandwidth Efﬁciency Consideration in                                      Optical Systems * Power Efﬁciency Consideration                                     in Optical Systems                               10.3  Transmission Impairments and Technology Limitations ......... 10-7 Moncef B.  Tayahi                   PMD Effects in BEM Transmission Systems * Optical                                     Components Limitations University of Nevada, Reno                               10.4  Practical Bandwidth Efﬁcient Modulations ..........................10-11  BanmaliS.Rawat                      PAMModulation * SCM-WDM Optical Systems University of Nevada, Reno    10.5  Conclusion .....................................................................10-19  10.1     Introduction  The explosivegrowthofdata, particularly Internet trafﬁc, has led to adramatic increase in demand for transmission bandwidth imposing an immediate requirement for expanding the capacity current networks. An additional driving forcefor higher capacity, enhanced functionality, and ﬂexibilityinoptical networks is the increased trend in interactive exchange of data and multimedia ﬁles. Due to the unpredictable and ever growing size of data ﬁles and messages exchanged over global distances, the future communication grid must be agile in time and able to react rapidly to supportend-to-end bandwidth requirements for transmission of ﬁles of any conceivable size.   Telecommunication networks currently widely deploy wavelength division multiplexing (WDM) in single mode optical ﬁbers to interconnect discrete network locations, and offer highcapacity and long range transmission capabilities. The presence of dark ﬁber in existing networks can accommodate alarge percentage of the capacityrequirements, howevernovel solutions are needed. Equipping the already installed equipment with conventional technologies is not the most cost-effectivesolution [1].   The recent advances in optical technologies havehad asigniﬁcant impact on optical networking solutions deployed world wide. These interconnections lead to aweb of optical ﬁbers that connect the globe, offering various servicestothe end users. However,further optimization of the existing solutions involves not only the physical implementation, but also software control and network management. Network providers haveto address the continuous evolution of servicesand applications that are becoming available to the users in a                                                                                     10-110-2                                   Broadcasting and Optical Communication Technology  resilient and secure manner.Technical breakthroughs are expected to further accelerate the realization of transparent optical networks to offer increased transmission bandwidth, switching capabilities, and optical signal processing functionalities. This progress is not only expected in the core networks, but also in the metropolitan area and the access networks to provide scaleable, transparent, and ﬂexible end-to-end solutions [2]. These featuresare offering full access to the global information network for all with reliable system performanceand at reduced cost.   Because of the rapid growthofcapacityrequirement on current transmission systems, ﬁber-optic technology is advancing into highdata rate per channel, and multiplexing various channels in one single ﬁber.The capacity of asingle optical ﬁber has increased from asingle OC-3 signal, transmitting at rate of 155 Mb/s, to terabits capacity. In order to maximize the system capacityand minimize performancedegradations caused by transmission impairments, careful engineering rules havetobecrafted before networks deployment. Signal modulation format is akey factor to be considered in the initial system design. Adopted modulation format determines the signal quality, signal tolerancetotransmission impairments, system capacity, total system spectral efﬁciency,and total cost. Until not long ago,non-return-to-zero (NRZ), also known as On and Off Keying (OOK), had been the dominant modulation format of choice in intensity-modulation and direct- detection (IM/DD) ﬁber-optic systems. Recentneeds for advanced modulation formats are motivated by the demand of systems with highcapacity,better overall reliability, optimum operation conditions, and low operating cost. From an information theorypoint of view,avarietyofsignal modulation formats had been studied extensively in communications. In contrast to microwave transmission and wireless communication systems known by low data rates, ﬁber-optic systems havetheir unique properties for supporting large capacity and long range transmission systems. Althoughthere is no magic modulation format immune to performance degradations, the choice of asuitable modulation format in ﬁber optic systems depends on manyfactors such as ﬁber types, per-channel data rate and aggregate link rate, wavelength spacing,system reach, and so on.  10.2     Bandwidth      Efﬁcient   Modulation     (BEM)    Concept  Multiplexing   Limitations In ordertotake advantage of the ﬁnite bandwidth of state of the artoptical ﬁber moreefﬁciently,new multiplexing techniques havebeen adopted from other applications; such techniques are Time Division Multiplexing (TDM), Wavelength Division Multiplexing (WDM), Frequency Division Multiplexing (FDM), and their combinations. Apart from noise accumulation, high-speed multiplexed signals suffer from chromatic dispersion, nonlinear crosstalk, and polarization mode dispersion (PMD). In multi-wavelength DWDM optical systems with relatively highdata rate per wavelength, inter-channel crosstalk originated by ﬁber nonlinearity such as cross-phase modulation (XPM) and four-wavemixing (FWM), can be limiting factors. Optical systems with data rates of 10 Gb/s and higher require precise dispersion compensation and careful engineering of asuitable dispersion map.However,due to the limitations in the wavelength stabilityofsemiconductor lasers and the limited selectivityofoptical ﬁlters, the minimum channel spacing is currently limited to , 25 GHz in commercial WDM  systems [3]. Although, optical ﬁlter designs haveimprovedoverthe years, we are reaching physical limits that can only be overcome by new physics and novel material breakthroughs. Further improvement in the optical spectrum densitycan be achieved with bandwidth efﬁciency modulation techniques [4].  Nyquist  Theoretical  Minimum     Bandwidth   Requirement The Nyquist minimum  bandwidth limit requires specifying the transmitted pulse shape so that no Intersymbol Interference(ISI) willtake placeatthe receiver.The requirement states that the theoretical minimum  system bandwidth needed to avoid ISI is half the signal frequency [5]. Therefore the Nyquist channel for zero ISI is rectangular in shape and its impulse response is asinusoidal function. Nyquist established that each received signal that has sinusoidal pulse shape is ISI freeand can be called an ideal Nyquist signal. Therefore, when the sampling time is perfect, therewill be no ISI penaltyinduced. For baseband transmission, the required system bandwidth is half the pulse period without ISI. From thisBandwidth Efﬁcient Modulation in Optical Communications                            10-3  assumption, the transmission rate per hertzistwo symbols per second per hertz (Sym/s/Hz) that can be obtained at best. It is clear from this requirementthat achieving amatching ﬁlter with aperfect rectangular shape is not realizable, and only aquasi shape can be targeted.   The beneﬁt of multilevelsignaling can be seen in the following example. Afundamental ﬁgure of merit for communication systems is the bandwidth efﬁciency whose unit is bits/s/Hz. The Nyquist limit is set at 2Sym/s/Hz, althoughasymbol mayhavemore than one bit per symbol. Forexample, consider M-ary QuadratureAmplitude Modulation (QAM-XX) signal, the value for XX is equal to 2 k where k is the number of bits symbol. A10Gb/s data stream occupies aminimum of 20 GHz of the optical band when NRZ modulation is used, however, when QAM-256 is used the optical signal will only occupy 2.5 GHz; the 17.5 GHz sparedfromthe spectrum can be populated by other channels. Theoretically, k can be as highas100, however practical value for k in optical transmission systems has not exceeded 10 yet. Higher-order modulation for bandwidth efﬁciency is becoming more attractive today because of available hardware for pulse shaping to achieveoptimum spectral efﬁciency,the availabilityoflow cost forward-error-correction (FEC) with low overheads, and the advances in RF signal multiplexing and demultiplexing techniques.  Sampling Amplitude Modulated Signals Sampling is aprocess that allows modern communication systems to carrythousands of simultaneous signals within alimited bandwidth. Sampling theoryisfundamental to the understanding of signals in analog and digital communications; it describes the conversion of signals from the continuous-time (analog) form to a discrete-time form (digital). In practice, discrete-time signals are coded into anumeric form and transmitted as adigital signal or stored digitally in acomputer memory. The sampling theorem dictates that the sampling rate must be at least twice the frequency of the highest frequency component of the signal being sampled.   Digital systems must break up the signal in the time domain, aprocess known as time sampling or time discretization. This is aseparate process from quantization, which is the breakingofsignals in various levels. The discrete information on acompactdisc, for example, is aseries of samples of continuous audio signals that mayhavebeen generated by conventional instrument or asinger’svoice. The sampling process is used to cut down excess information storage achieved by aprocess called time discretization.   There are several features to note: digital signals are only non-zero at certain times, they actually spend most of the time being zero.Analog signals would normally be denoted x ( t ), the ( t )denoting that the signal x varies  continuously with time t .The digital signal would normally be denoted x n ,the subscript n being the sample number.Asthe subscript n can only be an integer (0, 1, 2, 3 ... ), this means that x n only has values at discrete times. This nomenclature is important to note as it is used throughout the industry[6]. Time samples are evenly spaced for the digital signal (the time space between adjacent samples is constant). This time spacing is  called the sampling interval and will be denoted D t .The sampling frequency f s is related to the sampling interval D t as f s ¼ 1/D t .Althoughthe digital waveform should be drawn as ahistogram for most real signals, a continuous line would be drawn through the top of the histogram to produceacontinuous waveform. The digitization can be viewed as apulse train amplitude modulating an all positive version of the continuous signal, hencethe term Pulse Code Modulation (PCM).  Shannon–Hartley Capacity Theorem The Shannon–Hartley capacitytheorem describes the maximum possible efﬁciency of error correcting methods versus levels of noise interferenceand data corruption [7]. The theorydoes not describe how to construct the error-correcting method; it only tells us how good the best possible method can be. Shannon’s theorem has wide-ranging applications in both communications and data storage applications. This theorem is the foundation of the modern ﬁeld of information theory.   In the communication domain, Shannon’s theorem is also known as the Shannon limit or Shannon capacity.The maximum rate of clean data C that can be sent throughananalog communication channel subject to Gaussian-distribution noise interferenceisgiven by                                   C <  W log2 ð 1 þ S = N Þð10:                       1 Þ10-4                                   Broadcasting and Optical Communication Technology  where C is the post-correction effectivechannel capacity in bits per second, W is rawchannel capacity in hertz, and S / N is the signal-to-noise ratio of the communication signal to the Gaussian noise interference expressed as astraight power ratio (not as decibels).   Simple schemes such as ‘‘send the message 3times and use abest 2out of 3voting scheme if the copies differ’’are inefﬁcient users of bandwidth and thus are far from the Shannon limit. Advanced techniques such as Reed–Solomon codes and, morerecently,Turbo codes come much closer to reaching the theoretical Shannon limit, but at acost of highcomputational complexity.   The approach of using bandwidth efﬁcient modulation is not anew concept even in modern ﬁber optics transmission networks, however it has been reconsidered recently due to the availabilityoflow overhead forward error correction modules (FEC) that are capable of reducing the required signal to noise ratio for a given link. With an overhead of only 7%, FEC can provide morethen 9dBcoding gain [8]. This coding gain can be used to offset the OSNR penaltycaused by multilevelsignaling.  Shannon’sLimit The importance of Shannon’s limit can be explained using the sphere-packing bound technique [9]. The                                                      Eb sphere-packing bound technique can be used to calculate the needed to achieve agiven word error                                                     N o probability P w for acode having information length k bits, coded block length n symbols, and rate R ¼ k/n. The above quantities are expressed by the following formula                                      P w > Q n ð y s n ; A Þð10:                      2 Þ          p ﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃ where A ¼  2 REb = N o and y s n is the solution of the following (solid angle) equation:                                                                                           n                                 Z        G   þ  1                                  y s n n   1 2                    1                     O ð y s ; A Þ¼           sinð     f Þ n   2 d f ¼             ð 10: 3 Þ                      n   n            n     n þ 1                2 Rn                                  0       G                                               2                      Eb Given R , n ,and P w ,the is obtained by solving the following equation for A                     N o                          Z p              n   2 Z 1                             ð n   1 Þ sinð f Þ        2   2  p ﬃﬃ             Q ð y s ; A Þ¼            s         n   1 e  ðs þ nA   2 s n Acos ð f ÞÞ= 2 d s d f ð 10: 4 Þ               n  n              p ﬃﬃ n þ 1                           y s n 2 n = 2 p G   0                                        2  It is possible to directly obtain numerical solutions for the above equations provided that n is small, sayless than 100. Larger values of n producenumerical overﬂowand underﬂow. In order to minimize these problems, arecursive relationship has been derivedfor the inner integral in Equation (10.4). Let’s deﬁne                                Z 1                                       n   1             2         2                   J ð n ; x ; f Þ¼         s      n   1 e  ðs   2 x cos ð f Þ s þ x Þ = 2 ð 10: 5 Þ                                     p ﬃﬃ  n þ dx                                0 2 n = 2 p G                                             2  so that Equation (10.4) becomes                                    Z p                                             n   2 p ﬃﬃ                      Q n ð y s n ; A Þ¼ sin ð f Þ J ð n ; n A ; f Þ d f ¼ P w     ð 10: 6 Þ                                    y s nBandwidth Efﬁcient Modulation in Optical Communications                            10-5  Arecursive relationship can be obtained for J as shown below                        Z 1                                 n þ 1                         2         2       J ð n þ 2 ; x ; f Þ¼            s       n ð s   x cos ð f ÞÞe  ðs   2 x cosð f Þ s þ x Þ = 2                                 p ﬃﬃ n þ dx                        0 2 ð n þ 2 Þ = 2 p G                                         2                         Z 1                                    n þ 1                    2         2                       þ                 s       n x cos ð f Þ e  ðs   2 x cos ð f Þ s þ x Þ = 2 ð 10: 7 Þ                                   p ﬃﬃ n þ dx                          0 2 ð n þ 2 Þ = 2 p G                                           2  Upon integrating the ﬁrst term by parts it is readily shown that                         Z 1                                 n ð n þ 1 Þ         2          2         J ð n þ 2 ; x ; f Þ¼           s       n   1 e  ðs   2 x cos ð f Þ s þ x Þ = 2                                  p ﬃﬃ n þ dx                         0 2 ð n þ 2 Þ = 2 p G                                          2                                   Z 1                                            n þ 1             2         2                       þ  x cos ð f Þ             s       n e  ðs   2 x cos ð f Þ s þ x Þ = 2 ð 10: 8 Þ                                            p ﬃﬃ n þ dx                                   0 2 ð n þ 2 Þ = 2 p G                                                    2  Hence                                                                           n þ 2                       p ﬃﬃ       G                         2 x cos ð f Þ 2                         n        J ð n þ 2 ; x ; f Þ¼          J          ð n þ 1 ; x ; f Þþ J ð n ; x ; f Þð10: 9 Þ                            n        n þ 1                     n   1                                  G                                       2  This equation has been used to calculate the sphere packing bound exactly for an information block size k up  to 1000, for differentcoderates, and P w .For values of k higher than 1000 the bound has been evaluated by using Shannon’s approximation as presented in Ref. [10]. Acomparison between the exact solution and the                     Eb approximation indicates errors of less than 0.01 dB for block length codes greater than 50.                     N o Forward Error Correction (FEC) One of the main reasons that BEM in optical transmission systems is getting serious attention is the availabilityof Forward Error Correction (FEC). The limitation factors for closely spaced levels in multilevel coding can be overcome with FEC coding gain to enable a70% increase in the number of channels or a60% increase in the transmission distance[11]. Additionally,FEC allows an improvement in the QualityofService (QoS) by guaranteeing areceived bit-error rate (BER) better than 10  15.FEC coding relies on Reed-Solomon algorithms to add redundancy bits to the data stream,enabling the identiﬁcation and correction of corrupted bits [12]. These added redundant bits take the optical carrier (OC)–192 data rate from 9.953 Gb/s to 10.709 Gb/s that is an overhead of only 7% to yield aminimum of 9dBimprovement in optical signal to noise ratio (OSNR) margin. This FEC related OSNR improvement allows for an increase in channel capacityand/or transmission distance.   Super FEC is an enhancement of Reed-Solomon FEC and adds additional gain to an optical link, typically 8dBofgain rather than 5or6,extending the reach of 40 Gb/s systems.   FEC has some interesting associated beneﬁts as well, which may play out as the market evolves to accommodate non-standardtransmission formats such as 10 Gb/s Ethernet and video.SinceFEC is akind of ‘‘wrapper’’around anyoptical signal, it can be used as alink performancemonitor for anyoptically transmitted signal.   To further enhance performance, FEC supported transponders utilize optimized threshold crossing control in the receiver to set the decision circuit threshold to the optimum decision level in the received data; when10-6                                   Broadcasting and Optical Communication Technology  multiple traces of arandom data stream are superimposed on top of each other,the 0s and 1s form an eye diagram of the modulated waveform. The moreopen the eye, the morereliably the 0s and 1s will be detected and the better the BER. However,amplitude noise from optical ampliﬁers, phase noise, dispersion effects, and interference resulting from conversion of phase into amplitude modulation starttoclose the eye. As the eye closes, the decision circuit that determines if abit is a0or a1gives fewer errors if the decision threshold level can be adaptively adjusted to an optimum threshold decision level [13,14]. The optical receiver with FEC line extender modules and featuressuch as adaptive threshold crossing control result in an improvedreceived optical signal to noise ratio error-free transmission.  Bandwidth   Efﬁciency  Consideration   in Optical Systems Among  manyfactors to be considered when evaluating the overall bandwidth efﬁciency of amodulation format and the most obvious one is the channel spectrum efﬁciency or channel loading.Channel loading is usually expressed as function of the number of bits-per-symbol transmitted and the amount of frequency guard band required to keep one channel frominterfering with another.Inthe literaturethe notations used for  bit rate and symbol rate sometimes havedifferent meanings [15]. The information bit rate R b and the symbol rate R s refer to the channel bit and symbol rates beforeand after the modulator.The notation used to denote the coded symbol rate is measured at the input of the modulator.Ifnoerror correction coding or formatting is  used, then R s is equal to the information bit rate R b .Likewise, T b is the bit period and T s is the symbol period. In the absence of error correction coding or formatting, T s ¼ T b .   Forbandwidth efﬁciency illustration, M-aryPhase Shift Keying (MPSK) has abit loading of k bits-per- symbol; it requires afrequency guard band of 25% of its real band. As aresult, the bandwidth efﬁciency of a MPSK  modulated channel is Log k bits/sec/Hz [16]. This technique increases channel loading by increasing the number of bits per symbol transmitted in agiven symbol period. Other factors besides channel loading impact the bandwidth efﬁciency in the optical spectrum are immunitytonoise impairments, restricted guard bands requirements, and close levels in multilevel signaling.Bandwidth efﬁciency modulations open new regions in the frequency spectrum that were occupied by redundant information. In order to maintain quality signaling in anyassigned channel, the reserved channels are no longer required to be sitting idle waiting for trafﬁc fromother channels to be switched into them.  Power  Efﬁciency  Consideration   in Optical  Systems Sincepowerisscarceinall transmission systems, techniques that use less power for the same transmitted bandwidth are always sought. In powerlimited systems, the following tradeoffs are to be considered [17]:                                                               Eb     i) Improve the poweratthe expense the usable bandwidth for aﬁxed              Eb                                              N o    ii) Reduce   at the expense of the useable bandwidth for aﬁxedpower.              N o    iii) Usepractical modulations for powerefﬁcient systems are M-aryFrequency Shift Keying (MFSK). If the        IF minimum Nyquist bandwidth is given by: B N ¼ M . R s .  where R s is the symbol rate, the required transmission bandwidth is expanded M -fold, which is whyMFSK is                                                                Eb called bandwidth expansivetechnique and can be used to reducethe required at the expense of increased                                                                N o bandwidth.   Twomodulations techniques (MPSK and MFSK) havebeen used as examples to provide some insight into the design issues and trade-offs between bandwidth and power efﬁciencies. As M increase, MPSK signaling                                         Eb                                Eb provides morebandwidth efﬁciency at the cost of ,while MFSK signaling allows areduced at the cost of                                         N o                               N o increased bandwidth.   Four general rules can be followed in the design stage:    1)  The modulation must not expand the required transmission bandwidth beyond the available bandwidth.    2)  BEM based systems must be interoperable with other technologies.Bandwidth Efﬁcient Modulation in Optical Communications                            10-7     3) The required optical signal to noise ratio must be met even under worst-case scenario.    4) The hardwareand software needed for BEM implementation must be simple and inexpensive.  10.3     Transmission Impairments and             Technology Limitations  The ﬁnite bandwidth of state of the artoptical ﬁbers has pushed manyresearchers to look for alternativesto increase the spectral efﬁciency.TimeDivision Multiplexing (TDM), WDM, and their combinations were intended for this purpose, but these current techniques use binaryOn-and-OffKeying (OOK) with direct detection, which is neither power efﬁcient nor bandwidth efﬁcient. The following is alist of additional implementation imperfections whose effect on the performanceofvarious modulations should be considered; most of them haveagreater impact on phase modulations [18]:    1. Phase and amplitude imbalanceinMPSK, MFSK, QAM.    2. Imperfect or noisy reference at acoherent demodulator.    3. Powerloss due to ﬁltering of the modulated signals.    4. Degradation due to non-ideal detection ﬁlters.    5. Degradation due to pre-detection ﬁltering.    6. Degradation due to bit synchronization timing errors.    7. Local oscillator phase noise and spurious.    8. Envelope amplitude variations.    9. Stabilityand slope of Quadrature circuits in discriminator detectors.   Signal distortions arising from transmission impairments must be controlled so that the associated penalties are accounted for during the initial design stage when the powerand OSNR budgets are allocated. These penalties are typically taken into consideration once the systems speciﬁcations are known (capacity, reach, cost target, etc.). Acritical performanceedge is found in the areaofdispersion tolerance[19]. Chromatic dispersion impairs performanceofall high-rate signals, and thereforemust be compensated for everylink and channel. While chromatic dispersion is alinear process and it is straight forward to mitigate it, Polarization Mode Dispersion (PMD) becomes one of the most challenging factors in optical transmission systems.  PMD Effects in BEM     Transmission Systems It is well known that the fundamental mode of acircularly symmetric dielectric waveguide is degenerate in two dimensions. In real ﬁber the degeneracy is split by the birefringenceproperties of the ﬁber.The birefringence maybeintroduced deliberately,asinpolarization-maintaining ﬁber for example, or it may be aby-product of ﬁber geometry[20]. In this case the birefringenceisintroduced randomly by geometrical or stress-induced perturbations.    The propagation constants, b i ( o ), of the twoorthogonal modes can be expanded in aTaylor series around the center frequency, o                    o                                                                                         2                                   q b                1 q b                b ð o Þ¼b  ð o Þþ   i     ð o   o Þþ     i     ð o   o Þ 2 þ ...    ð 10: 10Þ               i  o     i  o    q o           o    2 q o 2         o                                   o ¼ o o              o ¼ o o                                    q b                             q 2 b wherethe b ( o )isthe phase velocity v , i is related to the group velocity v , i and is related to the           i  o                   p q o                           g q o 2 dispersion of the group velocity.   With the development of dispersion-shifted ﬁbers and the deployment of systems operating near the zero dispersion wavelength, the contribution to dispersion from the second order term that accounts for the chromatic dispersion component can be eliminated, and the ﬁrst order term nowbecomes signiﬁcant. For the case of birefringent ﬁbers, the ﬁrst order term leads to agroup delay called polarization mode dispersion (PMD). This polarization dispersion introduces adifferential group delaybetween orthogonal states of polarization. Althoughthe effect of PMD is to change randomly the polarization state of apulse propagating in aﬁber,itispossible to deﬁne apair of orthogonal states or ‘‘principal states’’ at the input whose output10-8                                   Broadcasting and Optical Communication Technology  states are orthogonal, and show no dependenceonwavelength to ﬁrst order.(In some situations, however,this approximation falls apartand the principal states can show wavelength dependency,leading to further system degradation throughcoupling to chromatic dispersion [22].)   The birefringenceinthe ﬁber can also be caused by local random and asymmetric mechanisms such as stress, bending, and twisting.These random birefringence mechanisms redeﬁne the local birefringence axes along the length of the ﬁber,thus causing random coupling between the polarization modes along the length of the ﬁber. The cabling process also introduces acertain amount of random birefringence and random mode coupling.The ﬁber length between such changes is usually referred to as the couplinglength, which for aﬁber is usually quoted as the ensemble average of all of the local couplinglengths. Furthermore, changes in local environmental conditions such as temperature, for example, cause ﬂuctuations in the local birefringence axes, thus causing random polarization coupling.Asaresult of the randomly changing polarization coupling,the magnitude of the Differential Group Delay(DGD) becomes astatistically varying function [22]. It can be shown that distribution of differential group delays is described by aMaxwellian distribution function, deﬁned by                                                 "#                                       32D t 2       4 Dt2                              P ð D t Þ¼     exp                                  ð 10: 11Þ                                      p 2 hiD t 3   pDhit 2  where D t is the differential group delaybetween the two principal states, and k D t l is the mean differential group delayalso known as PMD value. As aconsequence of the statistical nature of polarization mode dispersion, the magnitude of k D t l increases with the square rootofthe ﬁber or cable length, for lengths much                                                                                p longer than the couplinglength. Polarization mode dispersion is usually quoted in units of ps or ps/ km. The unit of ps is usually reserved for single optical elements that haveaﬁxed dispersion (e.g., acoupler or isolator) or shortﬁber sections that do not exhibit mode coupling [23]. PMD  Induced Limitations in BinaryModulate Signals In adigital transmission system the principal effect of polarization mode dispersion is to cause intersymbol interference.Asarule of thumb,a1-dB penaltyinthe optical signal to noise ratio (OSNR) occurs when the total instantaneous differential group delayequals 0.3T ,where T is the bit period. Both principal states of polarization are excited equally (gamma ¼ 1/2). This value is commonly used to account for the maximum tolerable system power penalty. The related Differential Group Delay(DGD) can also be approximated in the  same way, as the maximum tolerable system value: DGDmax # 0.3T .Arelationship between DGDmax and PMD  values can be set on the bases of the Maxwellian probabilitydistribution choosing aproper adjustment factor that is related to the desiredmaximum outage probability. Higher bit-rate systems haveshorter bit periods so they tolerate less differential group delay. Current studies indicate that optical ﬁbers will be speciﬁed  according either to the DGDmax,deﬁned above or to the mean levelofDifferential Group Delay(PMD value).   In optical transmission systems operating at 10 Gb/s, astatistical speciﬁcation of 0.5 ps/sqrt(km) has been proposed for concatenated links of optical ﬁber cable. From the Maxwellstatistics, the probabilitythat the 1dB in OSNR penaltyat10Gb/s is exceeded for a400 km span is less than 4 · 10  5 .Here, the contributions of other components to PMD are not taken into consideration. The PMD impairment can therefore be seen as a system powerpenaltyfor agiven bit rate and atargeted bit errorrate.   Forsystems operating at 40 Gb/s the mean differential group delayequal to one-tenth of abit period, 0.1T , corresponds to 2.5 ps. As ageneral assumption, partofthis tolerated value could be allocated to the cable and parttooptical repeaters, depending on the link characteristics. The total PMD of alink encompassing optical ﬁbers and optical sub-systems is the quadratic sum of the ﬁber and sub-systems PMD                                      "#                                                 X          1 = 2                                             2           2                           PMDTOT   ¼   PMDF  þ     PMDCi                         ð 10: 12Þ                                                  i  wherePMDTOT  is Total PMD link (ps), PMDF is PMD of concatenated optical ﬁber cables (ps), and PMDCi is PMD value of the i th sub-system (ps).Bandwidth Efﬁcient Modulation in Optical Communications                            10-9    Forexample, considering aPMD value of the concatenated optical ﬁber cables of the link of 0.1 ps/sqrt (km) (that can be considered advisable at 40 Gbit/s), 2.0 ps is the PMD cable contribution on 400 km long links. According to the previous formula, this still leaves a1.5 ps PMD margin for optical sub-systems. Assuming the use of 4optical sub-systems with aPMD value of 0.6 ps, the total PMD will be below2.5 ps limit stated beforefor 40 Gb/s systems.                               q ﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃ                                         2          2                    PMDTOT   ¼   ð 0 : 1 · 20Þ þ 4 · ð 0 : 6 Þ ¼ 2 : 33 ps5 2 : 5ps ð 10: 13Þ  Furthermore,inlong-haul ampliﬁed systems employing polarization scramblers (devices that deliberately modulate the polarization state of asignal laser so that it appears to be unpolarized), the polarization mode dispersion causes an increase in the degree of polarization of the signal. This degrades system performance throughinteractions with polarization-dependent loss and polarization hole burning.   In directly modulated bandwidth efﬁcient modulation systems and due to the presence of analog signals, the interaction of polarization mode dispersion with laser chirp leads to asecond orderdistortion, proportional to the modulation frequency.Afurther second order penalty, independent of modulation frequency,incurs when additional polarization-dependent loss is present in the system. The second ordereffect can cause acoupling between polarization mode dispersion and chromatic dispersion. This is caused by the wavelength dependence of the differential group delay, and more importantly,the wavelength dependenceofthe principal states of polarization. This leads to astatistical contribution to the chromatic dispersion. This is an area that is not well understood, and is under study.The use of chromatic dispersion compensating devicesalso has an unclear impact on the PMD penalty. Methods to Minimize PMD Effect in BEM   Systems Given that the problemarises from birefringence, much of the effortinreducing the effects of polarization mode dispersion havebeen concerned with minimizing the birefringence introduced by ﬁber or cable manufacturing. Care is taken to optimize ﬁber production to ensuregeometrical and optical circular symmetry, and to induce polarization mode coupling.Optical cables are manufactured using materials and processes that minimize the residual strain in the cable structure across the ﬁber.Elaborate cable structures can also be used, which introduce acircular component to the induced birefringence. By careful design, such an effect can counteract linear birefringencetoproduce acable with aresultant zero polarization mode dispersion. Typically,the mean polarization mode dispersion of ﬁbers and cables lie in the range:                                                  p ﬃﬃﬃﬃ                                  0 5Dhit 5 0 : 5ps = km                          ð 10: 14Þ  Furthermore,advanced ﬁber designs show less PMD value, e.g., 0.1 ps/sqrt(km), as mentioned earlier. Another method to reducethe effect of PMD is PMD compensation. APMD compensator can accept at its input asignal affected by PMD,and can mitigate to giveout asignal properly restored. It generally consists of a PMD  equalizer,aPMD  monitor,and  afeed-back controller.The equalizer and the monitor can be implemented in the optical or in the electrical domain. There is also the possibilityfor mixed or hybrid solutions. Afeed-back controller makes decisions on the basis of the monitoredinformation accordingtoa predeﬁned algorithm, and drives the equalizer according to the decisions.  Optical Components Limitations Optical Filters Optical ﬁlters that isolate anarrow range of wavelengths from abroader spectrum with highresolution are not yetwell developed, especially for wavelength separation below25GHz. Acceptable performanceoptical ﬁlters should have Flat In-Band Spectral Response (FBSR), low insertion loss, center wavelength stabilityunder various temperatures, and be tunable over awide band [24]. Optical channel spacing is ultimately limited by10-10                                  Broadcasting and Optical Communication Technology  the bandwidth of the information carried by the wavelengths. Optical channels in one ﬁber can not be placed inﬁnitely close together,evenifthe optical sources haveverypure spectrum; since the modulating signal caused the spectrum to broaden, this is an ultimate (theoretical) limit. One should also consider technical limits such as the characteristics of the optical ﬁlters used to separate the optical channels beforethe optical receiver.These ﬁlters havetobeable to sufﬁciently suppress the signal of adjacent channels, even in the event of very close channel spacing.Inpresent-daynetworks, the suppression is supported by the electronic bandwidth of the receiver,which is usually much smaller than the 100 or 50 GHz optical channels spacing.   In an Optical AddDrop Multiplexing (OADM), whereasignal in one particular wavelength is dropped and another signal having almost the same wavelength is added, the dropped signal has to be completely blocked; otherwise it will interfere with the newly added signal and can seriously degrade the BER performance. The optical receiver is anon-linear element, and as such, signals with closely spaced wavelengths will create beating products. In systems where the channels are very closely spaced the wavelength stabilityofthe deployed optical sources and the ﬁltering elements are also very important. Concatenation of ﬁlters leads to spectral narrowing [25]. This poses stringent requirements on the wavelength stabilityofthe laser sources and the absolute wavelength position of the wavelength selectiveelements, which may makenecessarythe use of an absolute wavelength reference and an active feedback control. Experiments on cascadabilityshowed a reduction of the bandwidth of morethan factor of twoafter ten elements, and afactor of threeafter 20 elements. By using BEM, which will compress the signals and allowthe transmission of the same capacityin smaller spectral bandwidth, the use of narrowband ﬁlters is avoided.   Afurther question is whether,byincreasing the number of channels, the wavelength spacing has to be reduced. The bandwidth of the single-windowErbium Doped Fiber Ampliﬁers (EDFAs) limits the maximum number of channels that asystem can supportwithagiven channel spacing.Althoughdouble-windowEDFAs havebeen proposed, widening the ampliﬁed spectral range from30to60nm, most of the suppliers are opting for the use of single-windowEDFAs, at least for the time being.This is because double-windowampliﬁers are still new and very expensive. Double-window EDFAs contain twoEDFAs withdifferent operating wavelength ranges connected in parallel. One of the EDFAs has an operating wavelength range shifted towards the higher wavelengths, in contrast to traditional EDFAs. The incoming optical signal is wavelength demultiplexed and each ampliﬁer is amplifying the channels falling into its operating wavelength range. The outputs of the EDFAs are recombined. There is also the question how the growthinnumber of channels will be implemented. Some manufacturers mightconsider upgrading by inserting new channels between the existing ones, effectively halving the channel spacing.Such asolution would not only requirethe replacement of the WDM elements, but would giverise to nonlinear effects, as discussed below.  Ampliﬁer Spontaneous  Emission ASE degrades the optical signal to noise ratio (OSNR). The required OSNR level(ROSNR) depends on the bit rate and transmitter-receiver technology(e.g., FEC). The OSNR margins allocated for the impairments needs to be maintained at the receiver.Inorder to satisfy these requirements, vendors often provide some general engineering rule in terms of maximum length of the transparent segment and number of spans. For example, current transmission systems are often limited to up to six spans each 80 km long before deploying adynamic-gain-ﬂattening device.For larger transparent domains, moredetailed OSNR computations will be needed to determine whether the OSNR levelthrough adomain of transparency is acceptable. This would provide ﬂexibilityinprovisioning or restoring alink throughatransparent link. Assume that the average optical powerlaunched at the transmitter is uniform and constant. The signal from the transmitter to the receiver goes through various optical ampliﬁers, with each introducing some noise power. Unitygain can be used at all ampliﬁer sites to maintain constant signal power at the input of each span to minimize noise power and nonlinearity.Aconstraint on the maximum number of spans can be obtained that is proportional to P and inversely proportional to ROSNR, optical bandwidth, ampliﬁer gain, and spontaneous emission factor of the optical ampliﬁer,assuming all spans haveidentical gain and noise ﬁgure.Bandwidth Efﬁcient Modulation in Optical Communications                           10-11  10.4     Practical Bandwidth Efﬁcient Modulations  Recently,there has been agreat deal of interest in the use of multilevel signaling in order to increase the transmission capacityinbandwidth-limited systems. Pulse amplitude modulation is used to increase the ﬁber capacity.This modulation format helps in increasing the bandwidth efﬁciency compared to the basic binary modulation. Multilevel signaling such as M -ary(where M is the number of signal levels) provides better  bandwidth utilization by allowing afactor of Log2 ( M )reduction in frequency over conventional binary formats. In a4-aryPAM system, for example, thereare two bits in everybaud (or symbol), thereforea 10 Gbaud/s can transmit 20 Gb/s signal (native data rate). It is important to note that the transmission  bandwidth of M -aryisscaled down by afactor of 1/Log2 ( M )compared to the usual on and offkeying (OOK) at the same rate. This is averystrong argument if one is looking to upgrade anetworkcapacity.Nomajor changes in hardwareare needed to double acapacityofnetwork operating at 10 Gb/s, for instance.Previous work in multilevelsignaling has been concentrated on wireless and point-to-point links [26–27]. Multilevel signaling has also been used for very shortreach(VSR) links wherehighdata rates are transmitted over a relatively shortreach, and wherehighspeed serial links using copper cables become moreattractive for areach less then 15 m, this limit in reach and data rates are dictated by the cable skin-effect loss.  PAMModulation Here, the focus is on 4-aryPAM signaling that can double the capacity of aDWDM metro networkoperating at 10 Gb/s or higher.This upgrade in capacitywill not require an upgrade of the system hardware. The proposed PAMmodulation uses asimple scheme to realize quadratic signal spacing to obtain optimum receiver sensitivityinasystem dominated by ampliﬁed spontaneous emission (ASE) noise. The quadratic spacing allows an improvement of the receiver sensitivitybyafactor of , 6dBcompared with equal signal spacing.The quadratic level spacing can be realized in three different ways: (a) by electronic means that consists of squaring the output voltage levelsbyanoff the shelf voltage squarer.Ifthe four levels of aPAM-4 are deﬁned as (0, 1, 2, 3), the output of the squarerwill be (0, 1, 4, 9), which is aquadratic spacing that closely matched the response of MZ-modulator,(b) by optical means, which consists of adding two streams of optical data interferomerecally with one signal attenuated by 6dBinpower with respect to the other;aphase shifter is used to ensure in-phase addition of the two modulated optical signals into the optical coupler,and (c) optoelectronic means, which uses aMZmodulator with suitable driving voltage and bias position.  PAMSystem    Design Implementing multilevel coding at highbit rates is rather straightforward. The 4-aryPAM is simply generated from twonon-correlated data sources whereone of data sources has half amplitude of the other.Both signals are powercombined to form a4-aryPAM compositesignal. If the input bit streams are correlated, adelayline maybeused to decorrelate the two input signals. A6-dB RF attenuator can be used in one of the data sources to generate four levels with equal spacing.Inthe non-return to zero(NRZ) setup,the 4-aryPAM signal drives  asingle arm MZ modulator biased at V p /2. This shows one of the advantages of multilevelcoding that does not require highRFvoltage drivers. Figure 10.1 shows the transmitter (encoder) used to generate the 4-ary PAMmodulation. At the receiver,direct detection is used to convert the optical 4-aryPAM signal into two separate electrical binarytributaries. Also shown in Figure 10.1 is the decoder,which uses threeindependent decision circuits; each circuit is preset to athreshold level corresponding to respective levels. The threshold levels can be dynamically adjusted for optimal bit error rate.   Equal level spacing (0, 1, 2, 3) can easily be obtained by applying an equally spaced electrical 4-aryPAM to a MZ modulator driven at its linear regime.Inorder to achieveoptimal receiver sensitivityinASE noise dominated systems [28], quadratic level spacing is preferred for both direct modulation and external modulation cases. This is because in an optically preampliﬁed receiver,the signal-ASE beating is stronger at highpowerlevels than at lowpowerlevels. Simple calculations show that quadratic level spacing,such as (0, 1, 4, 9), can improvethe receiver sensitivitybyover5dB compared to equal levelspacing.Quadratic  spacing can be realized by driving the MZ modulator in such away that the lowest level is at V p and the highest10-12                                  Broadcasting and Optical Communication Technology                                                                  DECODER                       V                                   DFF Q        ENCODER         3                                   3                                                               Q     Binary Source     V 2                                                       B          A                       V                                   V 3                        1                                         A          O                                                           DFF Q                                              Power                          R                    Power                                   2                                              Splitter            6     Combiner                                     Q                                                                     &                                                           V 2                            Multilevel Signal              DFF Q     Binary Source                                          1         B                                                    Q                                                              V 1                             FIGURE 10.1 PAM-4 encoder decoder setup.                                            M-Z response                     1                    0.9             1Linear drivevoltage:                                           Vpl+Vpi/G* [0, 1, 2, 3]                    0.8             ...Quadratic output optical power*:                                           [0, 0.67, 0.25, 0.5]                    0.7             *: offering ~5 dB better RX sensitivity                                    than equal signalling                    0.6                    0.5                    0.4                     Normalized output 0.3                    0.2                    0.1                     0                       00.5                    1          1.5          2                                          Drivevoltage ( V π )         FIGURE 10.2 Schematic of the driving conditions to achieve , quadratic signaling of 4-aryPAM.   levelisat , V p /2, as shown in Figure10.2. It is worthynoting that subquadratic signal leveling may be needed in practical systems whereother degradations havetobeconsidered. Forexample, thermal noise in the receiver,ﬁnite extinction ratio (ER) of the transmitter,and inter-symbol-interference (ISI) favor awider spacing between the lowest twolevels. We point out that the proposed scheme can ﬂexibly accomplish sub- quadratic leveling by simply increasing the driving voltage.  PAM-4  Experimental Results Back-to-back and transmission through a75kmstandardsingle mode ﬁber (SSMF) experiment on 20 Gb/s 4-aryPAM are shown in Figure10.3, which shows the measured back-to-back NRZ and RZ optical waveforms. Subquadratic signal leveling is obtained by suitably driving the M–Z modulator in the RZ modulation case. Figure10.4 shows the measured NRZ and RZ eyes after transmission. No obvious degradation was noticed. The required OSNR for back to back transmission was 26 dB for abit error rate of 10  10.Byusing efﬁcient forward error correction (EFEC), morethen 9dBcoding can be achieved.Bandwidth Efﬁcient Modulation in Optical Communications                           10-13                 FIGURE 10.3 Electrical back to back, NRZ back to back, and RZ back to back.                        FIGURE 10.4 RZ and NRZ output eyes after 75 km of SSMF.                   20-G PAM-4     10x 50-km dispersion-managed spans                 NRZ or RZ50%                                 50- Km SMF           DCF                                 (12 dB loss)    4-Km, 3-dB loss                                          EDFA                       MUX                                        DMUX              FIGURE 10.5 Schematic of the simulated transmission link using 4-aryPAM format.  PAM-4 Simulation and Modeling To assess the performanceof20Gb/s 4-aryPAM over longer transmission distance, anumerical simulation, using commercial software was performed. The schematic of the simulated RZ-DPSK transmission is shown in Figure10.5. Several WDM channels are then multiplexed beforebeing launched into adispersion-managed link, which consists of 10 spans of 50-km SSMF (D ¼ 17 ps/km/nm), following each of which is adispersion compensation ﬁber (DCF) to bring the accumulated dispersion to zero.The ﬁber non-linear coefﬁcient is 1.5/W/km. Fiber loss of each span (12 dB) is compensated by an EDFA. At the receiver site the WDM channels are demultiplexed and decoded using athree-level decision circuit as described previously.The 4-aryPAM transmissions in both single-channel and DWDM conﬁgurations were modeled. Forthe DWDM transmission we assume ﬁve 50-GHz spaced 20-Gb/s, 4-aryPAM channels, each of which is modulated by a27 -1 pseudo- random binarysequence (PRBS). Modulator bandwidth is assumed to be 20 GHz. The WDM channels are multiplexed and de-multiplexed with 50-GHz third-order super-Gaussian and 20-GHz fourth-order Gaussian ﬁlters, respectively.Aﬁfth-order Bessel electrical ﬁlter with 7-GHz bandwidth is used at the receiver.Weadjust the ASE noise levelatthe receiver to ﬁnd the required OSNR (deﬁned as the ratio between the per-channel signal powerand the ASE noise powerin0.1-nm bandwidth) for achieving BER¼ 10  3 ,the threshold for a corrected BER of 10  16 by ultra-FEC.10-14                                  Broadcasting and Optical Communication Technology  NRZ  4-aryPAM   Transmission Figure10.6 shows the optical spectrum of the simulated NRZ 4-aryPAM signals and back-to-back eye diagram. The required OSNR for BER ¼ 10  3 (uncorrected) is , 19.5 dB.The optimum decision levels are (0.23, 0.8, 1.7). Figure10.7 shows the eye diagrams after 200 km and 400 km single-channel transmissions with 0dBm per channel launch power. The nonlinear penalties (in terms of the increase of required OSNR) are , 1.2 dB and , 4dBafter 200 km and 400 km transmission, respectively.Figure10.8 shows the eyediagrams after 200 km and 400 km 50-GHz spaced DWDM transmissions with 0dBm per channel launch power.The nonlinear penalties are , 1.4 dB and . 5dBafter 200 km and 400 km transmission, respectively.Evidently,there exists alarge nonlinear penaltydue to inter-channel cross-phase modulation (XPM) induced timing jitter at 400 km.   RZ 4-aryPAM   Transmission Figure10.8 shows the optical spectrum of the simulated 50%-RZ 4-aryPAM signals, and back-to-back eye diagram. The required OSNR for BER ¼ 10  3 is , 17.5 dB, , 2dB which is better than that of NRZ.   FIGURE 10.6 Optical spectrum of the simulated NRZ 4-aryPAM signals (left), and back-to-back eye diagram (right).   FIGURE 10.7 Eyediagrams after 200 km (left) and 400 Km (right) single-channel NRZ 4-aryPAM transmissions with 0dBm per channel launch power.Bandwidth Efﬁcient Modulation in Optical Communications                           10-15   FIGURE 10.8 Optical spectrum of the simulated 50%-RZ 4-aryPAM signals (left), and back-to-back eye diagram (right).   FIGURE 10.9 Eyediagrams after 200 km (left) and 400 km (right) single-channel 50%-RZ 4-aryPAM transmissions with 0dBm per channel launch power.  The optimum  decision levels are (0.25, 1, 2.25). The nonlinear penalties (in terms of the increase of required OSNR) are , 0.2 dB and , 0.5 dB after 200 km and 400 km transmission, respectively.Figure10.9 shows the eye diagrams after 200 km and 400 km 50-GHz spaced DWDM RZ 4-aryPAM transmissions with 0 dBm per channel launch power.The nonlinear penalties are , 0.5 dB and , 1dBafter 200 km and 400 km transmission, respectively.Asexpected, RZ is much more robust against nonlinear distortions than NRZ.   Forshort-reach(, 200 km) Metrooptical networks, the required OSNR for both 20 Gb/s NRZ and RZ 4-aryPAM formats is less than 22 dB. Assuming 8dBNFofEDFA, the received OSNR with 0-dBm/ch launch power after a200-km transmission link (consisting of four 50-km SSMF spans of 12-dB loss each) is , 32 dB, indicating a , 10 dB OSNR margin. It is thus reasonable to expect 200-km reach with 0.4 spectral efﬁciency. With it superior robustness against nonlinear distortions, RZ 4-aryPAMF mayallow asystem reachofaround 400 km with reasonably large OSNR margin ( , 10 dB).   Figure10.10 shows the eye diagrams after 200 km (left) and 400 km (right) single-channel 50%-RZ 4-ary PAMtransmissions with 0dBm per channel launch power.The 4-aryPAM coding can be used for doubling the capacityofcurrent 10 Gb/s Metro optical networks. A20Gb/s 4-aryPAM transmitter is constructed by a novel scheme using asingle M–Z modulator.Multilevel PAMcoding mayﬁnd applications in future Metro optical communications systems in which both capacity and robustness against non-perfect dispersion compensation and PMD are important.10-16                                  Broadcasting and Optical Communication Technology   FIGURE 10.10 Eyediagrams after 200 km (left) and 400 km (right) DWDM (with 50-GHz spacing) 50%-RZ 4-aryPAM transmissions with 0dBm per channel launch power.  SCM-WDM      Optical  Systems Optical subcarrier multiplexing (SCM) is ascheme where multiple signals are multiplexed in the RF domain and transmitted by asingle wavelength. Asigniﬁcant advantage of SCM is that microwave devicesare more mature than optical devices; the stabilityofamicrowave oscillator and the frequency selectivityofa microwave ﬁlter are much better than their optical counterparts. In addition, the low phase noise of RF oscillators makes coherent detection in the RF domain easier than optical coherent detection, and advanced modulation formats can be applied easily.Apopular application of SCM technologyinﬁber optic systems is analog CATV distribution [29]. Because of its simple and low-cost implementation, SCM has also been proposed to transmit multichannel digital optical signals using direct detection [30] for local area optical networks. The performanceofhigh-speed digital ﬁber-optic transmission using SCM both analytically and numerically were investigated by various research groups. Fiber nonlinearities such as cross phase modulation (XPM)  and four-wavemixing (FWM) may generate signiﬁcant amounts of nonlinear crosstalk between adjacent SCM channels because they are very closely spaced. Chromatic dispersion when properly compensated for is not alimiting factor in modulated SCM systems because the data rate at each subcarrier is relatively low,but carrier fading due to PMD is signiﬁcant because of highsubcarrier frequencies. In orderto optimize the system performance, tradeoffs havetobemade between data-rate per subcarrier,levels of modulation, channel spacing between subcarriers, optical power,and modulation indices. An experiment of 10 Gb/s SCM ﬁber-optical system was performed in which 4 · 10 Gb/s data streamswerecombined into one wavelength, which occupied an approximately 80 GHz optical bandwidth. The combination of SCM and WDM   may provide amore ﬂexible platform for high-speed optical transport networks with highoptical bandwidth efﬁciency and highdispersion tolerance. For example, n independent high-speed digital signals are mixed by N different microwave carrier frequencies f i .They are combined and optically modulated on to an optical carrier. M wavelengths are then multiplexed together in an optical WDM conﬁguration. At the receiver an optical demultiplexer separates the wavelengths for individual optical detectors. Then RF coherent detection is used at the SCM leveltoseparate the digital signal channels. Channels add/drop are also possible at both the wavelength and SCM levels. While this SCM-WDM is in fact an ultra-dense WDM system, sophisticated microwave and RF technologyenables the channel spacing to be comparable to the spectral width of the baseband, which is otherwisenot feasible by using optical technology. Comparedtoconventional high-speed TDM systems, SCM is less sensitivetoﬁber dispersion because the dispersion penaltyis determined by the width of the baseband of each individual signal channel. Compared to conventional WDM systems, on the other hand, it has better optical spectral efﬁciency because much narrower channel spacing is allowed.Bandwidth Efﬁcient Modulation in Optical Communications                           10-17  SCM-BEM   Systems SCM  is acomplementarytechnology to traditional TDM and WDM systems. It provides an additional dimension of multiplexing to increase the efﬁciency and ﬂexibilityofanoptical transmission link. It has largely been used in wireless and CATV applications, but it can be adopted for increasing the capacityand bandwidth on agiven optical transmission link. SCM, when combined with abandwidth efﬁcient modulation (i.e., QAM, QPSK, DPSK... ), permits manyadditional capabilities wheredesigners can take advantages of two proven techniques (i.e., SCM and QAM). Optical SCM-QAM approach is avaluable technologydue to several keyimprovements in pulse shaping for spectral efﬁciency,the availabilityofforward-error-correction (FEC) for OSNR improvement and the advances in RF signal pre-and post processing [31].   SCM-QAM    Technique The most commonly used modulation with SCM for capacityimprovement is Quadrature Amplitude Modulation (QAM), wheremultiple bits are mapped in amplitude and phase plane. An example is 256 QAM, which transmits eight information bits per symbol by transmitting one of 256 symbols in the amplitude phase plane. The digital clock speed used in timing these symbols is much lowerthan the pure bit clock speed used in an on/off-keyed (OOK) modulation. This translates to lowercost electronics and to an abilitytotransmit moreinformation in agiven bandwidth, referred to as bandwidth-efﬁcient modulation (BEM).   Quadratureamplitude modulation (QAM) is amodulation scheme in which twosinusoidal carriers, one exactly 90– out of phase with respect to the other,are used to transmit data over agiven physical channel. Because the orthogonal carriers occupy the same frequency band and differ by a90 – phase shift, each can be modulated independently,transmitted over the same frequency band, and separated by demodulation at the receiver.For agiven available bandwidth, QAMenables data transmission at twice the rate of standard pulse amplitude modulation (PAM) without anydegradation in the bit error rate (BER) [32]. QAMand its derivatives are used in both mobile radio and satellite communication systems, and are starting to appear in ﬁber optics networks. Numerically controlled oscillator (NCO) Compiler can be used to design adual-output oscillator that accurately generates the in-phase and quadrature carriers used by aQAM modulator.The carrier frequency of each sinusoid can be set to anyprecision by deﬁning the phase increment input to the NCO.Araised cosine ﬁnite impulse response ﬁlter is used to ﬁlter the data streams beforemodulation onto the quadraturecarriers. When passed through aband-limited channel, rectangular pulses suffer from the effects of time dispersion and tend to smear into one another.This pulse shaping ﬁlter eliminates inter-symbol interference by ensuring that at agiven sampling instance, the contributiontothe response from all other symbols is zero.   In addition to transmitting several information bits in each symbol period, digital signal processing is used to shape the spectrum of the transmitted pulses so that most of the modulated signal energyisretained in a minimum frequency band, which signiﬁcantly reducesthe required spacing between carriers, enabling SCM systems to provide very highbandwidth efﬁciency sincehaving 256 states (symbols) to choose from each period would naturally increase the likelihood of error.Suitable techniques to enhance performanceare also used with these modulation approaches so that link budgets comparable to or better than alternative approaches can be achievedwithsimilar power. These include linear equalization of the received signal within the demodulator and use of multiple advanced FEC coding algorithms.   Keytothe performancebeneﬁts in SCM-QAM  modulation is the availabilityofRFsignal processing components, which can be adopted from other areas such as satellite communications. The individual  channels are frequency converted and ﬁltered to aseries of equally spaced subcarrier frequencies ( f 1 , f 2 ... f k ). The spaced subcarriers are combined together creating asingle compositeRFsignal consisting of k independent subcarriers. This RF signal is then used to directly modulate laser or an optical Mack Zehnder modulator in external modulation or DWDM networks. Systemshavebeen developed with these technologies that provide up to 40 Gb/s of data transmission on asingle wavelength in as little as 20 GHz of spectrum where the same data rate in NRZ format will occupy aminimum of 80 GHz. These individual wavelength signals can be fed into WDM systems to provide even greater capacity per ﬁber.SCM provides superior bandwidth10-18                                  Broadcasting and Optical Communication Technology  efﬁciency and overall capacitytothe much simpler NRZ modulation. Some additional inherent featuresthat provide anew level of ﬂexibilityare:      . Straightforward network design     . Easy and quick system upgrades     . Improvedsystem robustness with reduced adjacent channel interference and link performance       enhancements due to equalization and FEC     . Lowersusceptibilitytopolarization-mode dispersion and chromatic dispersion than 10-Gb/s     . Easy performancemonitoring using the SCM technique itself   In addition to supporting avarietyofapplications with these conﬁgurations, the digital and RF processing inherent in SCM provide alevel of visibilitytogether with data transparency that is unique. The digital processing performed yields agreat deal of insight into the performanceofthe link on agiven channel with metrics ranging from encoded channel BER to received signal-to-noise ratio.That enables SONET-level protection to be provided on anytrafﬁc type carried by an SCM system. Since all trafﬁc is digitally modulated, all trafﬁc can be protected with less than 50-msec switching to protect achannel, including native IP data trafﬁc without SONET framing or digital wrappers. The RF processing performed in the SCM system provides the desired transparency that network operators seek. Each channel can independently carrytrafﬁc operating at different rates. Anytrafﬁc type can be carried transparently,and systems havebeen designed that carry SONET,ATM,  and IP trafﬁc seamlessly on the different subcarriers. This transparency provides the network operator with the abilitytoreconﬁgure the system easily and quickly as the demand in new services changes without swapping out alot of equipment. The rise of Gigabit Ethernet as aservice provided by carriers is pushing demand for ﬂexible systems that enable better transitions. Also SCM eliminates the need for equipment to convert non-SONET signals into SONET signals.   Because SCM is effectively orthogonal to TDM and WDM, the technologies are inherently interoperable. SCM adds another dimension to the space networkoperators can choose from to implement capacity. On the tributaryside, SCM systems can and indeed must be designed to provide complete compatibilitytoSONET and TDM  signals. The use of standardlasers supporting the established ITU-grid wavelengths enables SCM systems to be combined with WDM for very-high-capacitysystems with superior ﬂexibility, providing sub- wavelength granularitywhile supporting multiple services in their nativeformat. With so manytechnologies being applied to optical networking,traditional notions of performancestandards are being challenged and adapted. Network design is morechallenging at higher rates, over morewavelengths, for longer distances. Any edge in simplifying network design throughenhancedormore robust performancecan makeareal difference to designers and to cost. SCM with digital signal processing for bandwidth-efﬁcient modulation provides some signiﬁcant performancebeneﬁts to optical networks. Optical-link budgets can be difﬁcult to maintain through anetwork that, despite standards used for initial design, has arange of channel conditions across its spans; each change can mean having to take anew look at the link budgets. However,the optical powerfor an SCM signal remains constant regardless of how manysubcarrier channels are in service.Bymodulating acommon light source with the aggregate signal, the optical power is afunction of the laser,not the number of channels in the signal. Anypower not used by achannel is returned to the unmodulated portion of the SCM waveform, which enables the add/ drop of subcarriers without reengineering the link.   The SCM and BEM  signal processing combination also provides ahighlevel of robustness in system link performanceoverawide  range of channel conditions. Because of the pulse shaping and ﬁltering of the subcarriers, SCM signals have very low adjacent-channel interference (see Figure10.4a). UnlikeanOOK signal, the spectral side lobes of an SCM signal are negligible; an ideal SCM signal will havenosidelobes at all, whereas nonlinearities in anypractical SCM modulator will producesmall side lobes well belowthose of an OOK  signal. The lack of out-of-band power results in very low adjacent-channel interferenceinDWDM systems. The digital signal processing in the SCM modems provides additional robustness with linear equalization to improve signals received throughvaried channels and with FEC morepowerful than that used in other optical systems. The relatively low subcarrier clock rate allows the implementation of multitap, fractionally spaced linear equalizers at each receiver to combat linear distortion in the channel (see Figure10.4(b)). The improved signal qualityresults in lower BER in highly distorted channels.Bandwidth Efﬁcient Modulation in Optical Communications                           10-19    Many optical systems are nowemploying linear block-code FEC, typically Reed–Solomon block codes. However,with binarysignaling the full powerofFEC cannot be tapped. The use of higher-order modulation with SCM allows the traditional Reed–Solomon FEC to be paired with trellis coded modulation (TCM), a form of FEC that takes advantage of the modulation format to maximize the coding gains. Together these codes provide coding gains of 7dBormore. Single-sideband (SSB) SCM eliminates the need for chromatic- dispersion compensation in all but the longest optical networks, even at very highaggregate bit rates. By eliminating the energyonone side of the optical carrier,the chromatic-dispersion toleranceislimited only by the symbol rate of the individual subcarriers, not by the aggregate signal bandwidth. Thus, a20-Gbit/sec SCM signal, operating with OC-48 rate subcarriers, will exhibit chromatic-dispersion tolerancefar exceeding that of a10-Gbit/sec OC-192 system, and even that of an OC-48 system (see Figure10.5).   Polarization-mode dispersion (PMD) is another impairment that gets worse with increased bandwidth. Unfortunately,SSB does not help in this case. However,the inherent bandwidth efﬁciency of an SCM signal limits the impact of PMD.A40-Gb/s SCM signal exhibits greater PMD tolerancethan an OC-192 signal. At 10 Gb/s, the PMD toleranceofSCM approaches that of an OC-48 signal. In networks wherePMD is the primary limitation, an SCM signal can be designed to maximize the PMD tolerance.   Extensiveperformancemetrics provided by the digital signal processing together with adecentralized architecture provide SCM systems with averyhighlevel of performancemonitoring. The underlying digital signal processing and RF processing at lowclock rate provide BER and SNR measurement in real time. The channel BER is much higher in the operating region, providing BER degradation information to the operator as gradual or slight degradations occur,well before they accumulate to result in an outage. That provides a valuable maintenancecorrective-action tool to avoidcostly downtime. While SCM clearly provides the capabilitytoincrease optical-networkcapacityand improveﬁber bandwidth efﬁciency,italso provides many other potential beneﬁts to future optical networks. Flexibilityinconﬁguration and trafﬁc mix combined with true transparency and carrier-class protection of all trafﬁc reduces operations cost and improves provisioning. Highly robust optical links with constant optical power together with highly informativeperformancemetrics and lower susceptibilitytodispersions make SCM systems easier and less costly to design and maintain. Sub carrier multiplexing is clearly partofthe future of optical networks, providing more than just capacity.  10.5     Conclusion  BEM importanceinoptical networks is evident with its efﬁcient bandwidth use, enabling moreusers within a limited channel bandwidth, and the ﬂexibilityinupgrading existent networks as needed. PAM, SCM, and QAMare making importantcontributions to optical communications from local area to long haul. The results above indicate that thereisnosingle, prominent modulation scheme. Bandwidth efﬁcient modulation and DWDM havestrong features that provide adesirable optical communication capacity and performance. When it comes to anyone particular application it is important to look at the tradeoffs involved. While DWDM effectivelyutilizes bandwidth, SCM when combined with QAMrequires less bandwidth. Furthermore, due to its frequency modulating characteristic, BEM shows agreater immunity to signal ﬂuctuations. DWDM and BEM  each provide beneﬁcial features althoughneither dominates the other; both contribute to the advancement of optical communication systems.  Nomenclature Symbol    Quantity                           Symbol    Quantity  ASE       Ampliﬁed Spontaneous Emission      CSRZ      Carrier Suppressed RZ ASON      Automatically Switched             DPSK      Differential Phase Shift Keying             Optical Networks                 FEC       Forward Error Correction ATMAsynchronous       Transfer Mode          GMPLS     Generalized Multi-Protocol BER       Bit-Error Rate                                 Label Switching10-20                                  Broadcasting and Optical Communication Technology  Symbol    Quantity                           Symbol    Quantity  IETF      Internet Engineering Task Force    PAMPulse       Amplitude Modulation IP        Internet Protocol                  QAMQuadrature       Amplitude Modulation ITU       International Telecommunications   QoS       QualityofService             Union                            RWARouting       and Wavelength MPLS      Multi-Protocol Label Switching                 Assignment OBS       Optical Burst Switching            RZ        Return to Zero OADM      Optical AddDropMultiplexer         SDH       Synchronous Digital Hierarchy O-PNNI    Optical Private Network to         SONNET    Synchronous Optical Network             Network Interface                TCP/IP    Transmission Control OPS       Optical Packet Switching                       Protocol/ Internet Protocol OSNR      Optical Signal-to-Noise Ratio      VPN       Virtual Private Network OTDM      Optical Time Division Multiplexing VSRZ      Vestigial Sideband RZ OTNOptical       TransportNetwork            WDM       Wavelength Division Multiplexing OXCOptical       Cross-Connect               WSSWavelength       SelectiveSwitch  References  1.  A.R. Pratt,B.Charbonnier,P.Harper,D.Nesset, B.K. Nayar,and N.J.Doran, ‘‘40 · 10.7 Gb/s DWDM      transmission over meshed ULH network with dynamically re-conﬁgurable optical cross connects,’’      PD09, Post deadline OFC,Atlanta, March 2003.  2.  D. Banerje and B. Mukerjee, ‘‘Wavelength-routed optical networks: linear Formulation, resource      budgeting tradeoffs, and reconﬁguration study,’’ Proc. IEEE INFOCOM,1997, pp.269–210.  3.  R. Ramaswami and K. Sivarajan, Optical Networks: APractical Perspective,Los Altos, CA: Morgan      Kaufmann Publishers, 1998.  4.  S. Waklin and J. Conradi, ‘‘Multilevel signaling for increasing the reachof10Gb/s lightwavesystems,’’      J. Lightwave Technol.,vol.17, pp.2235–2248, 1999.  5.  H. Nyquist, ‘‘Certain topics in telegraph transmission theory,’’ Trans. AIEE,vol. 47, pp.617–644, 1928.  6.  W.H. Joseph, Applications of Discrete and Continuous Fourier Analysis,New York: Wiley,1983.  7.  C.E. Shannon, ‘‘Communication in the presenceofnoise,’’ Proc. Inst. Radio Eng.,vol. 37, no.1, pp.10–21,      1949.  8.  C. Berrou,A.Glavieux, and P. Thitimajshima, ‘‘Near shannon limit error-correcting coding and      decoding: turbo-codes (1),’’ IEEE Int. Conf. Commun.,May,1064–1070, 1993.  9.  W. Weaver and C.E. Shannon, The Mathematical TheoryofCommunication,Illinois, IL: Urbana, 1963,      republished in paperback. 10.  J.R. MarksII, Introduction to Shannon Sampling and Interpolation Theory ,Berlin: Springer-Verlag,1991. 11.  W.W. Peterson, Error Correcting Codes,New York: Wiley,1961. 12.  S. Lin and D.J. Costello, Error Control Coding: Fundamentals and Applications,Englewood Cliffs,      NJ: Prentice Hall, 1983. 13.  O.E. Agazzi, T. Koh, S.S. Haider,R.W.Walden, D.R. Cassiday,G.A. Wilson, T.M. Lalumia, C.M.      Gerveshi, J. Kumar,R.E. Crochiere, R.F.Shaw, R.A. Wilson III, W.R. McDonald, N.L. Gottfried, N.S.      Ramesh, and R.B. Blake Jr., ‘‘Adigital signal processor for an ANSI standardISDN transceiver,’’ IEEE      J. Solid-State Circ.,vol. 24, no.6,pp. 1605–1613, 1989. 14.  A.M. Gottlieb,P.M. Crespo,J.L. Dixon, and T.R. Hsing,‘‘The DSP implementation of anew timing      recoverytechnique for highspeed digital data transmission,’’ Proc. IEEE Int.Conf. Acoust. Speech Signal      Process.,Albuquerque, NewMexico,1990, pp.1679–1682. 15.  J.B. Anderson and D.P. Taylor,‘‘A bandwidth-efﬁcient class of signal-spacecodes,’’ IEEE Trans. Inf.      Theory ,vol.IT-24, no.6,pp. 703–712, 1978. 16.  J.B. Anderson, C-E.W.Sundberg,T.Aulin, and N. Rydbeck, ‘‘Power-bandwidth performanceof      smoothed phase modulation codes,’’ IEEE Trans. Commun.,vol. COM-29, no.3,pp. 187–195, 1981.Bandwidth Efﬁcient Modulation in Optical Communications                           10-21  17.  M. Shtaif and A. Mecozzi ProceedingsofOptical Fiber Communication Conference, Paper MM1,      Washington DC: Optical SocietyofAmerica, 2001. 18.  D. Dahan and G. Eisenstein, ‘‘Numerical comparison between distributed and discrete ampliﬁcation in a      point-to-point 40 Gb/s 40 WDM-based transmission system with three differentmodulation formats,’’      J. Lightwave Technol.,vol. 20, no.3,pp. 371–378, March 2002. 19.  N. Antoniades, M. Lee, J.K. Rhee, M. Sharma, and A. Boskovic, ‘‘Extending the reach of WDM networks      by combating highchannel resolution powerdivergence using dynamic power equalization,’’ LEOS,      14th Annual Meeting of the IEEE, vol.1.1, pp.354–355, 2001. 20.  S. Lanne, D. Pennincks, J-P.Thie´ ry,and J-P.Hamaide, ‘‘Extension of polarization-mode dispersion limit      using optical mitigation and phase-shaped binarytransmission,’’ Proc. Opt. Fiber Commun. Conf.,OFC,      paper ThH3, 2000. 21.  L.T Lima, R. Khosravani, O.H. Adamczyd, P. Ibrahimi, E. Ibragimov,A.E. Willner,and C.R. Menyuk,      ‘‘Enhanced PMD mitigation using forward error correction coding and aﬁrst ordercompensator,’’      Conference on Optical Communications, ThB4, 2000. 22.  M. Tomizawa, Y. Kisaka, A. Hirano, and Y. Miyamoto,‘‘PMD mitigation by frequency diverse detection      receiver employing error correction function,’’ Proc. ECOC2002,Copenhagen, Denmark. 23.  F.Q. Zhou, M. Zhou, and J.J. Pan, ‘‘Optical coating computer simulation of narrowbandpass ﬁlters for      dense wavelength division multiplexing,’’ Optical Interference Coating,OSA Technical Digest Series 9, pp.      223–224, 1988. 24.  J.D. Downie, I. Tomkos, N. Antoniades, and A. Boskovic, ‘‘Effects of ﬁlter concatenation for directly      modulated transmission lasers at 2.5 and 10 Gb/s,’’ J. Lightwave Technol.,vol. 20, no.2,pp. 218–228,      2002. 25.  R. Farjad-Rad, C.K. Yang,M.Horowitz, and T. Lee, ‘‘A0.3-um CMOS 8Gb/s 4-PAM serial link      transceivers,’’ IEEE J. Solid State Circ. ,vol.35, pp.757–764, 2000. 26.  R.A. Grifﬁn, ‘‘Multi-level signaling withquadratic levelspacing,’’UKPatent Application GB 2366106A,      2001. 27.  J. Poultron and W.J. Dally,‘‘A tracking clock recoveryreceiver for 4Gs/s signaling,’’ HotInterconnects      Symposium,August 1997. 28.  T.E. Darcie and G.E. Bodeep,‘‘Lightwavesubcarrier CATV transmission systems,’’ IEEE Trans.      Microwave TheoryTech.,vol. 38, no.5,pp. 524–533, 1990. 29.  L. Pophillat, ‘‘Optical modulation depth improvement in SCM lightwavesystems using a      dissymmetrization scheme,’’ IEEE Photon. Technol. Lett.,vol. 6, no.6,pp. 750–753, 1994. 30.  J.H. Angenent, ‘‘Simple model for calculation of distortion in an optical analogue subcarrier      multiplexed CATV system,’’ Electron. Lett.,vol. 26, no.24, pp.2049–2050, 1990. 31.  A.A.M. Saleh, ‘‘Fundamental limit on number of channels in subcarrier-multiplexed lightwaveCATV      system,’’ Electron. Lett.,vol. 25, no.12, pp.710–777, 1989.This page intentionally left blank                                                                          11                                            Phase-Locked Loop                                 11.1  Introduction .................................................................... 11-1                               11.2  Loop Filter ...................................................................... 11-2                               11.3  Noise .............................................................................. 11-5                               11.4  PLL Design Procedures...................................................... 11-6 Steven L. Maddy               11.5  Components .................................................................... 11-6 SpectraLink Corporation       11.6  Applications..................................................................... 11-8  11.1     Introduction  A phase-locked loop (PLL) is asystem that uses feedback to maintai.n an output signal in aspeciﬁc phase relationship with areferencesignal. PLLs are used in manyareas of electronics to control the frequency and/or phase of asignal. These applications include frequency synthesizers, analog and digital modulators and demodulators, and clock recoverycircuits. Figure11.1 shows the block diagram of abasic PLL system. The phase detector consists of adevice that produces an output voltage proportional to the phase difference of the twoinput signals. The VCO(voltage-controlled oscillator) is acircuit that produces an ac output signal whose frequency is proportional to the input control voltage. The divide by N is adevice that produces an output signal whose frequency is an integer (denoted by N )division of the input signal frequency.The loop ﬁlter is a circuit that is used to control the PLL dynamics and therefore the performanceofthe system. The F ( s )term is used to denote the Laplace transfer function of this ﬁlter.   Servo theorycan now be used to derive the equations for the output signal phase relative to the reference input signal phase. Because the VCOcontrol voltage sets the frequency of the oscillation (rather than the phase), this will produceapure integration when writing this expression. Several of the components of the PLL haveaﬁxed  gain associated with them. These are the VCO control voltage to output frequency conversion gain ( K v ), the phase detector input signal phase difference to output voltage conversion gain ( K f ), and the feedback division ratio ( N ). These gains can be combined into asingle factor called the loop gain ( K ). This loop gain is calculated using Equation (11.1) and is then used in the following equations to calculate the loop transfer function.                                            K  · K                                      K  ¼   f    V                                ð 11: 1 Þ                                              N    The closed-loop transfer function [ H ( s )] can now be written and is shown in Equation (11.2). This function is typically used to examine the frequency or time-domain response of aPLL and deﬁnes the relationship of the  phase of the VCOoutput signal ( y o )tothe phase of the referenceinput ( y i ). It also describes the relationship of achange in the output frequency to achange in the input frequency.This function is low-pass in nature.                                         y ð s Þ  KFð s Þ                                 H ð s Þ¼ o  ¼                                     ð 11: 2 Þ                                        y i ð s Þ s þ KFð s Þ                                                                                     11-111-2                                   Broadcasting and Optical Communication Technology                                  FIGURE 11.1 PLL block diagram.    The loop error function, shown in Equation (11.3), describes the difference between the VCOphase and the referencephase and is typically used to examine the performanceofPLLs that are modulated. This function is high-pass in nature.                              y ð s Þ y ð s Þ y ð s Þ  s                               i     o   ¼   e  ¼                                  ð 11: 3 Þ                                 y i ð s Þ  y i ð s Þ s þ KFð s Þ    The open-loop transfer function [ G ( s )] is shown in Equation (11.4). This function describes the operation of the loop beforethe feedback path is completed. It is useful during the design of the system in determining the gain and phase margin of the PLL. These are indications of the stabilityofaPLL when the feedback loop is connected.                                              KFð s Þ                                       G ð s Þ¼                                    ð 11: 4 Þ                                                s    These functions describe the performanceofthe basic PLL and can nowbeused to derive synthesis equations. The synthesis equations will be used to calculate circuit components that will give adesired performancecharacteristic. These characteristics usually involve the low-pass corner frequency and shape of the closed-loop response characteristic (Equation (11.2)) and determine such things as the loop lock-up time, the abilitytotrack the input signal, and the output signal noise characteristics.  11.2     Loop   Filter  The loop ﬁlter is used to shape the overall response of the PLL to meet the design goals of the system. There are two implementations of the loop ﬁlter that are used in the vast majorityofPLLs: the passive lag circuit shown in Figure11.2 and the activecircuit shown in Figure11.3. These twocircuitsboth produce aPLL with a second-order response characteristic.   The transfer functions of these loop ﬁlter circuits maynow be derivedand are shown in Equation (11.5) for the passive circuit (Figure 11.2) and (11.6) for the activecircuit (Figure 11.3).                                             sC1 R 2 þ 1                                 F p ð s Þ¼                                        ð 11: 5 Þ                                        s ð R 1 þ R 2 Þ C 1 þ 1            FIGURE 11.2 Passive loop ﬁlter.               FIGURE 11.3 Active loop ﬁlter.Phase-Locked Loop                                                                  11-3                                              sR2 G 1 þ 1                                    F a ð s Þ¼                                     ð 11: 6 Þ                                              sR1 C 1    These loop ﬁlter equations may now be substituted into Equation (11.2) to form the closed-loop transfer functions of the PLL. These are shown as Equation (11.7) for the case of the passive ﬁlter and 11.8 for the active.                                         KR           K                                     s     2  þ                                      R  þ  R    ð R þ R Þ C                   H  ð s Þ¼      1          2     1    2  1                       ð 11: 7 Þ                     p                 1         KR             K                           s 2 þ s           þ      2   þ                                  ð R 1 þ R 2 Þ C 1 R 1 þ R 2 ð R 1 þ R 2 Þ C 1                                            KR      K                                          s   2 þ                                            R     R C                                H  ð s Þ¼     1    1  1                            ð 11: 8 Þ                                  a           KR      K                                        s 2 þ s  2 þ                                              R 1   R 1 C 1    These closed-loop equations can also be written in the forms shown below to placethe function in terms of  the damping factor ( z )and the loop natural frequency ( o n ). It will be shown later that these are very useful parameters in specifying PLL performance. Equation (11.9) is the form used for the PLL with apassiveloop ﬁlter,and Equation (11.10) is used for the activeloop ﬁlter case.                                                  2        2                                      s ½ 2 zon  ðo n = K Þ  þ o n                              H p ð s Þ¼  2            2                           ð 11: 9 Þ                                         s þ s 2 zon þ o n                                                      2                                           s 2 zon þ o n                                 H a ð s Þ¼ 2          2                          ð 11: 10Þ                                         s þ s 2 zon þ o n    Solving Equation (11.7) and Equation (11.9) for R 1 and R 2 in terms of the loop parameters z and o n ,we nowobtain the synthesis equations for aPLL with apassive loop ﬁlter.These are shown as Equation (11.11) and Equation (11.12).                                            2 z    1                                     R 2 ¼                                        ð 11: 11Þ                                          o n C  KC                                             K                                      R 1 ¼ 2    R 2                              ð 11: 12Þ                                           o n C    To maintain resistor values that are positivethe passiveloop ﬁlter PLL must meet the constraint shown in Equation (11.13).                                             o                                         z >   n                                  ð 11: 13Þ                                             2 K    Forthe activeloop ﬁlter case Equation (11.8) and Equation (11.10) are solved and yield the synthesis equations shown in Equation (11.14) and Equation (11.15). It can be seen that no constraints on the loop damping factor exist in this case.11-4                                         Broadcasting  and Optical Communication     Technology             FIGURE 11.4  Closed-loop second-order type-2 PLL error response for various damping factors.             FIGURE  11.5 Closed-loop second-order type-2 PLL step response for various damping factors.Phase-Locked Loop                                                                  11-5                     FIGURE 11.6 Closed-loop PLL response for various damping factors.                                               K                                        R 1 ¼  2                                  ð 11: 14Þ                                             o n C                                               2 z                                        R 2 ¼                                     ð 11: 15Þ                                             o n C    Atypical design procedurefor these loop ﬁlters would be, ﬁrst, to select the loop damping factor and natural frequency based on the system requirements. Next, all the loop gain parameters are determined. A convenient capacitor value may then be selected. The remaining resistors can now be computed from the synthesis equations presented above.   Figure11.4 shows the closed-loop frequency response of aPLL with an activeloop ﬁlter (Equation (11.10)) for various values of damping factor.The loop natural frequency has been normalized to 1Hzfor all cases.   Substituting Equation (11.6) into Equation (11.3) will givethe loop error response in terms of damping factor.This function is shown plotted in Figure 11.5. These plots may be used to select the PLL performance parameters that will give adesiredfrequency response shape.   The time response of aPLL with an active loop ﬁlter to astep in input phase was also computed and is shown plotted in Figure 11.6.  11.3     Noise  An important design aspect of aPLL is the noise content of the output. The dominant resultantnoise will appear as phase noise (jitter) on the output signal from the VCO. Due to the dynamics of the PLL some of these noise sources will be ﬁlteredbythe loop transfer function (Equation (11.2)) that is alow-pass characteristic. Others will be processed by the loop error function (Equation (11.3)) that is ahigh-pass characteristic. Table 11.1 shows the major sourcesofnoise in aPLL and the effect of the loop dynamics on this noise. All these factors must be combined to evaluate the completenoise performanceofaPLL. Often it will be11-6                                   Broadcasting and Optical Communication Technology                           TABLE 11.1 PLL Noise Sources                                Noise SourceFilter       Function                           Reference oscillator phase noise Low pass                          Phase detector noise         Low pass                          Activeloop ﬁlter input noise Low pass                          Digital divider noise        Low pass                          Activeloop ﬁlter output noise High pass                          VCOfree-running phase noise  High pass  found that one particular noise source will be dominant and the PLL performancecan then be adjusted to minimize the output noise.   APLL  is frequently used to enhance the noise performanceofanoscillator by taking advantage of these noise-ﬁltering properties. Forexample, acrystal oscillator typically has very good low-frequency noise characteristics, and afree-running LC oscillator can be designed with very good high-frequency noise performancebut will exhibit poor low-frequency noise characteristics. By phase-locking an LC oscillator to a crystal oscillator and setting the loop response corner frequency to the noise crossover point between the two oscillators, the desirable characteristics of both oscillators are realized.   When  designing frequency synthesizers using PLLs, care must be taken to prevent noise from the PLL components from introducing excessivenoise. The divider ratio ( N )used in the feedback of the loop has the effect of multiplying anynoise that appears at the input or output of the phase detector by this factor. Frequently,alarge value of N is required to achieve the desiredoutput frequencies. This can cause excessive output noise. All these effects must be taken into account to achieve aPLL design with optimum noise performance.  11.4     PLL   Design    Procedures  The speciﬁc steps used to design aPLL depend on the intended application. Typically the architectureofthe loop willbedetermined by the output frequency agilityrequired (frequency synthesizer) and the reference sources available. Other requirements such as size and cost play important factors, as well as available standard components. Oncethe topologyhas been determined, then the desiredloop transfer function must be synthe- sized. This maybedictated by noise requirements as discussed above or other factors such as loop lock-up time or input signal tracking ability. The design Equation (11.11) through Equation (11.15) may then be used to determine the component values required in the loop ﬁlter.   Frequently several of these factors must be balanced or traded offtoobtain an acceptable design. Adesign that requires highperformanceinseveral of these areas usually can be realized at the expense of design complexityorincreased component cost.  11.5     Components  The development of large-scale integrated circuits over the past several years has made the design and implementation of PLLs and frequency synthesizers much cheaper and easier.Several major manufacturers (Motorola, Signetics, National, Plessey,etc.) currently supply awiderange of componentsfor PLL implementation. The most complex of these are the synthesizer circuits that provide aprogrammable reference divider,programmable divide by N, and aphase detector.Several conﬁgurations of these circuits are available to suit most applications. Integrated circuitsare also available to implement most of the individual blocks shown in Figure11.1.   Awidevarietyofphase detector circuitsare available,and the optimum type will depend on the circuit requirements. An analog multiplier (or mixer) may be used and is most commoninapplications wherethePhase-Locked Loop                                                                  11-7  comparison frequency must be very high. This type of phase detector produces an output that is the multiplication of the twoinput signals. If the inputs are sine waves, the output will consist of a double-frequency component as well as adccomponent that is proportional to the cosine of the input phase difference.The double-frequency component can be removed withalow-pass ﬁlter,leaving only the dc component.The analog multiplier has asomewhat limited phase range of ^ 90 degrees. The remainder of the phase detector types discussed hereare digital in nature and operate using digital edges or transitions of the signals to be compared.   The sample-and-hold phase detector is widely used whereoptimum noise performanceisrequired. This circuit operates by using one of the phase detector inputs to sample the voltage on the other input. This latter input is usually converted to atriangle wave to give alinear phase detector characteristic. Once the input is sampled, its voltage is held using acapacitor.The good noise performanceisachieved since most of the time the phase detector output is simply astored charge on this capacitor.The phase range of the sample-and-hold phase detector depends on the type of waveform shaping used and can range from ^ 90 to ^ 180 degrees.   One of the simplest types of phase detectors to implement uses an exclusive OR gate to digitally multiply the twosignals together.The output must then be low-pass ﬁlteredtoextract only the dc component. The main drawback to this circuit is the large component that exists in the output at twice the input frequency. This requires alarge amount of low-pass ﬁltering and may restrict the PLL design. The phase range of this type of circuit is ^ 90 degrees.   One of the main drawbacks of all the abovetypes of phase detectors is that they only provide an output that is proportional to phase and not to afrequency difference in the input signals. For manyapplications the PLL input signals are initially not on the same frequency.Several techniques havebeen used in the past to resolve this such as sweeping the VCOorusing separate circuitrytoﬁrst acquirethe input frequency.The sequential (sometimes called phase/frequency) phase detector has become the most commonly used solution due to its wide availabilityinintegrated form. This type of phase detector produces pulses with the width of the pulses indicating the phase difference of the inputs. It also has the characteristic of providing the correct output to steer the VCOtothe correct frequency.The noise characteristic of this type of phase detector is also quite good sinceeither no or very narrow pulses are produced when the inputs are in phase with each other.The phase range of this type of circuit is ^ 360 degrees.   Digital dividers are widely available and mayeither haveprogrammable or ﬁxed division ratios depending on the application. For optimum noise performanceasynchronoustypeofdivider should be used. When aprogrammable divider is required to operate at ahighfrequency ( . 50 MHz), adual modulus circuit is normally used. This circuit uses atechnique called pulse swallowing to extend the range of normal programmable divider integrated circuitsbyusing adual modulus prescaler (usually ECL). The dual modulus prescaler is ahigh-frequency divider that can be programmed to divide by only two sequential values. Asecond programmable divider section is then used to control the prescaler.Further details of this type of divider are available from component manufacturers’data sheets as well as in the references.   The voltage-controlledoscillator is typically the most critical circuit in determining the overall noise performanceofaPLL. Forthis reason it is often implemented using discrete components, especially at the higher frequencies. Some digital integrated circuits exist for lower-frequency VCOs, and microwave integrated circuit VCOs are now available for use to several gigahertz. The major design parameters for aVCO include the operating frequency,tuning range, tuning linearity, and phase noise performance. Further information on the design of VCOs is contained in the references.   Loop ﬁlters used in PLLs may be either activeorpassive depending on the speciﬁc application. Active ﬁlters are normally used in morecritical applications when superior control of loop parameters and referencefrequency suppression is required. The loop ﬁlter is typically followed by alow-pass ﬁlter to removeany  residual referencefrequency component from the phase detector.This low-pass ﬁlter will affect the calculated loop response and willtypically appear to reduce the loop damping factor as its corner frequency is brought closer to the loop natural frequency.Toavoid this degradation the corner frequency of this ﬁlter should be approximately one order of magnitude greater than the loop natural11-8                                   Broadcasting and Optical Communication Technology  frequency.Insome cases anotch ﬁlter may be used to reduce the reference frequency when it is close to the reference frequency.  11.6     Applications  Phase-locked loops are used in manyapplications including frequency synthesis, modulation, demodulation, and clock recovery. Afrequency synthesizer is aPLL that uses aprogrammable divider in the feedback. By selecting various values of division ratio,several output frequencies may be obtained that are integer multiples of the referencefrequency (Fref). Frequency synthesizers are widely used in radio communications equipment to obtain astable frequency source that may be tuned to adesired radio channel. Since the output frequency is an integer multiple of the referencefrequency,this will determine the channel spacing obtained. The main design parameters for asynthesizer are typically determined by the required channel change time and output noise.   Transmitting equipment for radio communications frequently uses PLLs to obtain frequency modulation (FM) or phase modulation (PM). APLL is ﬁrst designed to generate aradio frequency signal. The modulation signal (i.e., voice) is then applied to the loop.For FM the modulating signal is added to the output of the loop ﬁlter.The PLL will maintain the center frequency of the VCO, while the modulation will varythe VCO frequency about this center.The frequency response of the FM input will exhibit ahigh-pass response and is described by the error function shown in Equation (11.3). Phase modulation is obtained by adding the modulation signal to the input of the loop ﬁlter.The modulation will then varythe phase of the VCOoutput signal. The frequency response of the PM input will be alow-pass characteristic described by the closed-loop transfer function shown in Equation (11.2).   Acommunications receiver must extract the modulation from aradio frequency carrier.APLL may be used by phase locking aVCO to the received input signal. The loop ﬁlter output will then contain the extracted FM signal, and the loop ﬁlter input will contain the PM signal. In this case the frequency response of the FM output will be alow-pass function described by the closed-loop transfer function and the PM output response will be ahigh-pass function described by the error function.   In digital communications (modems) it is frequently necessarytoextract acoherent clock signal froman input data stream. APLL is often used for this task by locking aVCO to the input data. Depending on the type of data encoding that is used, the data mayﬁrst need to be processed beforeconnecting the PLL. The VCO output is then used as the clock to extract the data bits from the input signal.  Deﬁning   Terms Capture range:  The range of input frequenciesoverwhich the PLL can acquirephase lock. Damping  factor: Ameasure of the abilityofthe PLL to track an input signal step.Usually used to indicate      the amount of overshoot present in the output to astep perturbation in the input. Free-run frequency: The frequency at which the VCOwill oscillate when no input signal is presented to      the PLL. Sometimes referred to as the rest frequency. Lock range:  The range of input frequencies over which the PLL will remaininphase lock once acquisition      has occurred. Loop ﬁlter: The ﬁlter function that follows the phase detector and determines the system dynamic perfor-      mance. Loop gain:  The combination of all dc gains in the PLL. Low-pass ﬁlter: Aﬁlter that usually follows the loop ﬁlter and is used to remove the reference frequency      componentsgenerated by the phase detector. Natural frequency: The characteristic frequency of the PLL dynamic performance. The frequency of the      closed-loop transfer function dominant pole. Phase detector gain: The ratio of the dc output voltage of the phase detector to the input phase difference.      This is usually expressed in units of volts/radian. VCO  gain:  The ratio of the VCOoutput frequency to the dc control input level.This is usually expressed in      units of radians/second/volt.Phase-Locked Loop                                                                  11-9  References AFDPLUS Reference Manual,Boulder,Colo.: RLM Research, 1991 (softwareused to generate the graphs in this      section). R.G. Best, Phase-Locked Loops—Theory, Design &Applications,New York: McGraw-Hill, 1984. A. Blanchard, Phase-Locked Loops, Application to Coherent ReceiverDesign,New York: Wiley Interscience, 1976. W.F. Egan, Frequency Synthesis by Phase Lock,New York: Wiley Interscience, 1981. F.M. Gardner, Phaselock Techniques,New York: Wiley,1979. J. Gorski-Popiel, Frequency Synthesis; Techniques &Applications,Piscataway, N.J.: IEEE Press, 1975. W.C. Lindsey and M.K. Simon, Phase-Locked Loops &Their Applications,Piscataway,N.J.: IEEE Press, 1978. V. Manassewtsch, Frequency Synthesizers: Theoryand Design,New York: Wiley Interscience, 1980. U.L. Rhode, Digital PLL Frequency Synthesizers Theoryand Design,Englewood Cliffs, N.J.: Prentice-Hall, 1983.  Further Information Recommended periodicals that coverthe subject of PLLs include IEEE Transactions on Communications, IEEE Transactions on Circuits and Systems,and IEEE Transactions on Signal Processing.Occasionally articles dealing with PLLs may also be found in EDN, Electronic Design, RF Design,and Microwaves and RF Magazine.Afour- partPLL tutorial article titled PLL Primer,byAndrzejB.Przedpelski, appearedin RF Design Magazine in the March/April 1983, May/June 1983, July/August 1983, and November 1987 issues.   Another good sourceofgeneral PLL design information can be obtained from application notes available from various PLL component manufacturers. Phase-Locked Loop Design Fundamentals,byGarth Nash, is available fromMotorola, Inc. as AN-535 and gives an excellent step-by-step synthesizer design procedure.This page intentionally left blank                                                                          12                                                                Telemetry                                 12.1 Introduction .................................................................... 12-1                               12.2 Basic Concepts................................................................. 12-1                               12.3 Data Measurements........................................................... 12-3                               12.4 Data Channels .................................................................. 12-5 Stephen Horan                 12.5 Data Transmission Formats ................................................ 12-7 New Mexico State University   12.6 Data Processing .............................................................. 12-10   12.1     Introduction  Telemetrysystems are found in avarietyofapplications—from automobiles, to hospitals, to interplanetary spacecraft. Although these examples represent abroad range of applications, they all havemanycharacteristics in common: anatural parameter is measured by asensor system, the measurement is converted to numbers or data, the data are transported to an analysis point, and an end user makes use of the data gathered.This series of actions fulﬁlls the deﬁnition of telemetry ,which is to ‘‘measure at adistance.’’ It is not unusual for the data collection function to be partofanoverall control system where information also ﬂows back from the monitoring point to modify the measurement discipline or to initiate some action. Forexample, in controlling amanufacturing process, the telemetrydata about the process will cause the process monitor to change a parameter such as atemperature or ﬂowrate to optimize the process. The ﬂow of information back is telecommand data from the monitoring point.   In this chapter,wewill look at an overview of the system-leveldesign issues involved in the synthesis of a telemetryand telecommand system. Further details can be found in Ref. [1].  12.2     Basic Concepts  The general purpose of atelemetrysystem is to gather information about asubject of interest and present that measurement to auser.The physical quantityorproperty that is being measured is known as the measurand. Each measurand is sampled by asensor at arate appropriate for the bandwidth of the signal. The sampled analog signals are then frequently digitized by using an analog-to-digital converter.This produces a pulse- coded modulation ( PCM)representation of the measurand. This process is illustrated in Figure 12.1 wherean analog signal is sampled as afunction of time. The sampled signal is represented by avoltage at the sensor output. This voltage is converted to the PCM representation in the analog-to-digital conversion process to produce one digital sample for each sample. The user of the measurements will reverse this process to estimate the measurand value as afunction of time. To produce an accurate estimate of the measurand, the sensor and conversion process will need to be calibrated and the calibration coefﬁcients incorporated into the inverse transformation. Measurement systems requiring the sampling of morethan asingle sensor require more complicated supportand coordination to allow sampling of several measurands.                                                                                     12-112-2                                   Broadcasting and Optical Communication Technology             FIGURE 12.1 Sampling an analog voltage signal and converting it to aPCM digital signal.    The measurement system is typically located at adistance fromthe eventual data user,which can be a person or amachine. The measurements are transmitted over some type of data channel, for example, aradio link, aﬁber optic cable, or atelecommunications network. This architecture produces atelemetrysystem having the characteristics of the standardtelecommunications deﬁnition of telemetry that is [2]:    1. The use of telecommunication for automatically indicating or recording measurements at adistance      fromthe measuring instrument.    2. The transmission of non-voice signals for the purpose of automatically indicating or recording      measurements at adistancefrom the measuring instrument.   The control functions in atelemetrysystem are accomplished through actuators,which are devices that respond to transmitted control signals to affect the measurement system characteristics or interact withthe measurement environment. The control is achieved throughatelecommand link, which is the ‘‘use of telecommunication for the transmission of signals to initiate, modify or terminate functions of equipment at a distance.’’ [2] The data ﬂowinatelemetry/telecommand system is illustrated in Figure12.2. The telemetry measurements from the sensors ﬂowtothe user over the data channel.   The sampling and multiplexing components format and sequence order the data for transmission over the channel. The data processing returns the measurement values to estimates of the actual measurand values for presentation to the user.Inthe user interface, the PCM signal will either be converted to an analog waveform or left as aseries of discrete measurements for use and analysis. The user interface maybeadisplay screen, aset of gages, or achart recorder.The user interface often contains data-logging capabilities to provide apermanent recordofthe measurements.   The user also enters commands at the user interfacetocontrol the actuators. The data processing prepares the user input into telecommand data for transmission over the data channel. The control functions then interpretthese commands and cause the actuators to respond. The design of the telemetryand telecommand data structures is generally individualized for each system by the system designers. The overall structureofthe data structures can be classiﬁed as either frames or packets as discussed later.   Most telecommand systems transmit the command data as asmall packet or series of data words. It is not unusual for the telemetrydata transmission rate to exceed the telecommand transmission rate by afactor of 100 to 1000 to account for the relative differencesinthe real-time data volumes between the two types of data.   The data channel in Figure 12.2 can take on differentconﬁgurations depending upon the system. Many telemetrysystems found in aerospace, remote sensing,and even hospitals use some form of radio link to transmit the data. These channels are usually susceptible to reasonably higherror rates and interference from other users. At measurement sites wherenetworking drops are available, the network backbone can be used as the data channel. In this case, the channel is highqualitycoaxial cable or ﬁber optic cable; both having much lowerchannel error rates than possible with radio links. This allows the system designer to haveaccess to the highqualityofthe   network  channel and leverage offthe networking protocols found in the telecommunications infrastructure.The use of error correcting encoding of the data will need to be determined by the system designer based on the reliabilityofthe channel and the criticalityofthe data.Telemetry                                                                          12-3                  S1        Sampling                            and      Telemetry                         Multiplexing                S2                                Data               User                                    Data Channel Processing         Interface                           Control  Telecommand                 A   FIGURE 12.2 Overall data ﬂowinatelemetryand telecommand system composed of sensors (S) and actuators (A).  Remote science telemetrysystems measuring natural phenomena are atypeofone-of-a-kind data systems that mayhaveerror-correcting coding applied sincethe data cannot be reacquired if the channel corrupts the transmission. However,process-control data may not haveerrorcorrections applied because the corrupted data can be replaced by anew sample almost immediately.  12.3     Data Measurements  The telemetrysystem will primarily produce PCM data as the result of sampling the analog sensor output. However,thereare other data types that may also be present in the system. The telemetrydata types include:      . PCM data—sampled analog values.     . Digital data—naturally occurring digital values such as event counts.     . Bi-level data—binaryvalues such as switch settings.     . Timing data—torecordwhen the measurements were taken.     . Command echoes—repeats of telecommand data for operator veriﬁcation.   The sampling rate for each sensor is determined by the signal’s Nyquist sampling rate.If W is the signal  bandwidth in hertz, then the Nyquist sampling rate, f N ,insamples per second, is given by:                                         f N ¼ 2 W                                  ð 12: 1 Þ  In practice, aminimal sampling rate of ﬁve times the signal bandwidth is typically used to accurately reconstruct the measurand.   The process of making aPCM measurement involves morethan sampling the output of asensor.Asshown in Figure 12.3, the actual measurement process involves the response functions of:      . the transducer converting the input signal to be measured into its electrical analog (voltage or current),     . the signal conditioning electronics that make the signal capable of being reliably measured by providing       anynecessarysignal gain, signal ﬁltering,orbuffering that is required by the measurement device,     . the measurement device, usually an analog-to-digital converter,totransform the electrical signal into       its digital representation, and     . the data formatter to efﬁciently package the data measurements for transmission.   Each of these processes mayimpart anon-linear transformation to the measurement that will need to be removed by inverse processing before presenting the data to the user.This will need to be calibrated as discussed below.   Digital, bilevel, and command echo data types are usually read directly fromadigital storage location and do not need to be further processed. Rather,they are takendirectly by the data formatter to placethose measurements in the transmission data stream.   As mentioned above, it is typical to time tag the data to record when it was measured. Telecommands may also be time tagged to specify the execution time. Time tags come in two typical formats: an integer count or a formatted set of values to represent time and date. The position tags can be encoded as acharacter string or ﬂoating point number.12-4                                   Broadcasting and Optical Communication Technology                           Applied                                        Transducer                         Stimulus                                           Signal                                        Conditioner                                         Measurement                                          Device                                             Data          Telemetry                                         Formatter         Data            FIGURE 12.3 Measurement electronics to convert asignal to its electronic representation.                                              High-Order Time                   BCD Weighting 1day  10 hr    1hour  10 min     1min                                                             655.36 sec                                              Low-Order Time                   BCD Weighting                               0  10 sec     1sec   0.100 sec 0.010 sec                                                              0.010 sec                FIGURE 12.4 Formatted time tag conﬁguration for representing time and date.    The integer time tag represents an elapsed countofseconds since aspeciﬁc reference time. Manycomputer languages havethis as abuilt-in system function and the time tag can be this system value. Forasingle day, the number of elapsed seconds sincemidnight can be represented as a17-bit integer.Generally,a32-bit integer can hold the total number of elapsed seconds for most applications and referencetimes. An alternativetothe integer method is to use aformatted sequence such as that shown in Figure 12.4. Here, the ﬁelds are used to encode date and time. While this format requires more bits than the integer method to hold the result, it is easier for humans to understand. In both cases, the time can be referenced to one of several standards. The traditional time standard was provided by timing frames broadcast by radio station WWVB or other standard time services. In the past several years, the use of Global Positioning Satellites (GPS) with receivers having standardcomputer interfaceshavebecome more popular as the means to provide alocal time reference. For networkedsystems, time servers using the NetworkTimeProtocol (NTP) are aconvenient waytokeep the system synchronized with the correct time.   In manysystems, the data also needs to be position tagged to specify wherethe data sample was made. The position tags can be easily determined from one of the standardGPS navigation messages having position information in the message. An example of aGPS GGA-typenavigation message, withboth time and position information, is given in Figure12.5 wherethe ﬁelds for the time, longitude, and latitude are indicated. These messages are in text format and the ﬁelds can be incorporated directly into the telemetrydata stream.   The measurement system will need to be calibrated to properly convert each measurement from the received rawPCM  value, sometimes called engineering units,tothe natural set of units for the measurement. The calibration process involves supplying aknown input, which should be referenced to acalibrated version of the standardvalue, to the measurementsystem and observing the resulting output. The application of the known standardinput values should cover the whole expected range to be experienced during the measurementTelemetry                                                                          12-5      $GPGGA,190719.72,3216.5110,N,10644.9837,W,1,03,3.7,00025,M,,,,*30       Message   Time (UTC)     Latitude      Longitude      ID  FIGURE 12.5 Example of the standard GGA type of GPS navigation message showing timing and position information.                                            S max                                               down-scale-going measurement                         I                                       I                        min                    scale factor      max                                            S min                       output response (dependent variables) up-scale-going measurement                                  applied stimulus (independent variable)      FIGURE 12.6 An example of atransducer response function showing hysteresis in the measurement proﬁle. process and include both increasing and decreasing ranges of the input. This will show anyhysteresis that may be in the transducer response function as illustrated in Figure12.6. The calibration process should also include anysignal conditioning and signal processing electronics to ensurethat the transfer functions from the devices in Figure12.3 are also included. From this set of measurements, amathematical model is generated to map the measured outputs onto the input values. This mathematical model is applied in the user interface to produce a processed measurement that can be related to astandard and with proper units.  12.4     Data Channels  The conﬁguration and qualityofthe data channel will inﬂuence the design of the data transmission structure. Telemetrysystems have two general channel conﬁgurations: either acommon channel mode or adistributed channel mode, as illustrated in Figure12.7. In common channel mode, all of the telemetryand telecommand data traverses asingle,logical link. This link maybeeither full-duplex or parallel simplex links, depending upon the nature of the physical channel and the access protocol. In this mode, all of the sensor data is combined at acommon data acquisition system prior to transmission over the channel. Typical environments for this mode include rocket payloads,laboratorydata acquisition systems, and remote vehicle telemetry systems. The common channel is frequently aradio link, awire, or alocal area network.   In the distributed mode channel, each sensor connects to the channel viaatransceiver.For example, this transceiver can be asimple IEEE 802.11 interfacethat permits the sensors to be connected as partofawireless network with acentral computer receiving the data viaawireless access point. Typical environments for this mode include building temperatureand ﬁresensing or factoryprocess measurement and control.   With the common channel transmission mode, the system will need to balance the data sampling requirements from the Nyquist rate withthe need to share the channel between the sensors. This is done via atime division multiplexing within the frame or packet format used for the data transmission. With the time12-6                                                 Broadcasting    and  Optical  Communication        Technology   multiplexing  of  the  sensor  data, therewill   need  to  be  an  accompanying    synchronization    algorithm   to  synchronize  the receiver to the incoming   data  stream  to fully recover the data.    The  distributed channel  can use either polling techniques  or codedivision  multiple  access techniques  to give  each  sensor  sharedaccess   to  the  channel.  Because   the individual  sensor  is only  sending   its own   data,  synchronization  is easier to achieve.    There  are speciﬁc frequencies  allocated for telemetryand   telecommand    transmission  in the radio spectrum.  This is in addition to other,generic  frequency  allocations for data transmission.  Examples   of telemetrybands  are given in Table 12.1. These bands  represent those used  for speciﬁc applications such  as biomedical  telemetry                                            Transmission                                           Channel                                                                  S1  TR                TR   S4                                       TR                                           User                                                             S2  TR                                                                                TR                                                                                            TR   S5                                                  TR                      Data                      Acquisition                      System                                                             S3  TR                    S1                                                      User          TR   S6                         S2                              S3                    S6                                      S4    S5                           Common    Channel Mode                     Distributed Channel Mode  FIGURE   12.7   Common    and distributed data channels for telemetryand telecommand systems. The data sources are the sensors (S) and the channel interface occurs at atransceiver (TR).  TABLE   12.1   Speciﬁc TelemetryBands   for Various Applications [3,5]    Frequency Range                                                   Usage  38–41  MHz                 Biomedicaltelemetryband 174–216  MHz               Biomedicaltelemetryband 400–406  MHz               Radiosonde; spaceoperation; Earth exploration satellite 449.75–450.25 MHz          Spaceoperation and spaceresearch services 460.650–460.875 MHz        Biomedicaltelemetryband 465.650–465.875 MHz 902–928  MHz               Industrial, scientiﬁc, and medical (ISM) band 1400–1427  MHz             Earth exploration satellite 1427–1435  MHz             Land mobile (telemetering and telecommand); ﬁxed (telemetering) 1435–1535  MHz             Aeronautical telemetry 1525–1530  MHz             Mobile service,including aeronautical telemetry, as asecondaryallocation 1530–1535  MHz             Mobile service,including aeronautical telemetry, as asecondaryallocation 1670–1700  MHz             Radiosonde 2200–2290  MHz             Government  ﬁxed, mobile, spaceresearch, spaceoperation, and Earth-exploration services 2290–2300  MHz             Spaceresearch for deep spaceshared with ﬁxed and mobile (except aeronautical mobile) services 2310–2390  MHz             Flight testing of manned or unmannedaircraft, missiles, space vehicles, or their major components 2310–2360  MHz             Aeronautical telemetryasasecondaryservice 2360–2390  MHz             Mobile service (will become secondaryafter 2005 or 2007, depending upon location) 2400–2500  MHz             ISM band 5725–5875  MHz             ISM bandTelemetry                                                                          12-7  or spaceoperations as well as those commonly used in scientiﬁc and commercial applications such as the Industrial, Scientiﬁc, and Medical (ISM) bands. Some of these bands are allocated for governmental use as partofthe national test ranges while others are open to nongovernment users as well. System designers need to consult the radio frequency allocation tables [5] to determine proper allocations for planned frequency use. If the allocations in Table 12.1 are being used outside of the United States, they will need to be checked in the countryofuse to verify that the planned usage is consistent with national allocations.  12.5     Data   Transmission Formats  As mentioned above,the data channel will need to be shared among the various sensors in the telemetry system when using the common transmission channel mode. The channel bandwidth and qualityare limiting factors in the system because they restrict the volumeofdata that can be reliably sent and they determine the data synchronization methodology. The channel bandwidth will affect the speed at which each sensor can be sampled to ensure that the Nyquist sampling rate is satisﬁed. Because different sensors will havedifferent sampling rates, atime-division multiplexing technique is frequently chosen as the means to share the data channel. Forchannels where data loss caused by data drop outs is asigniﬁcant problem or wheretight synchronization is required between the sensor system and the user interface, acontinuous frame transmission data structureisused. For systems where the channel provides highreliabilityorthereare manydifferent sampling rates possible, apacket data structuremay be preferred.Inthis section, we will look at both methods.   Frame telemetryisthe traditional method for time-multiplexing telemetrydata from the source to the destination. The multiplexing structurerepeats in ﬁxed-time intervals to allowthe receiving side to synchronize to the transmitter based on the contents of the data stream alone. Most frame systems use a format standard called IRIG 106 [3], developed by the militarytest ranges’Inter-Range Instrumentation Group (IRIG). This frame structure is used industry-wide and not just for militarysystems. The frame structureisarranged like amatrix, as shown in Figure12.8, with the major frame being one complete cycle throughthe time-multiplexing sequence,during which time each sensor is sampled at least once. Each major                      Major Frame               Synch Word 1 23...          SprCom 1SubFrame    SprCom 2 ... n-1             Synch Word 1   23...         SprCom 1SubFrame    SprCom 2 ... n-1               Synch Word 12    3   ...    SprCom 1SubFrame    SprCom 2 ... n-1                                     .                                     .                                     .                                     .               Synch Word 1  2   3   ...     SprCom 1 SubFrame  SprCom 2 ... n-1                  Minor Frame                                                        Subframe      FIGURE 12.8 The standard IRIG telemetryframe structure for major frames, minor frames, and subframes.12-8                                   Broadcasting and Optical Communication Technology  frame is divided along rows of the matrix into minor frames that begin with asynchronization word. The columns of the matrix in the major frame can represent either the output from sensors or management data repeating in each minor frame. Asensor’soutput reading stays in its assigned column from frame to frame. If the same sensor is sampled once per minor frame—for example, columns 1, 2, and 3inFigure12.8—then it is sampled at the commutation rate for the system and is called commutated data.Ifasmall number of sensors need to be sampled at arate higher than the commutation rate, they are supercommutated sensors (‘‘SprCom’’ locations in Figure12.8) and take up morethan one column in the matrix. Some columns in the matrix may represent agroup of sensors rather than one sensor.Asshown in the column labeled ‘‘subframe’’ in Figure 12.8, the sensors in that group form a subframe within the major frame. The sensors are sampled at arate belowthe commutation rate, so the data are subcommutated.   Standardsynchronization codes are usually 16 or 32 bits long.For example, the standard 16-bit codeis EB90 in hexadecimal, whereas the standard32-bit code is FE6BA840 in hexadecimal. These codes are optimal in the sense of having the lowest autocorrelation values when shifted by one bit. This minimizes the probabilityofafalse lock in the synchronization-detection circuitry. All synchronization codes suffer the possibilityofbeing mimickedbyrandom data. Therefore,several occurrencesofthe codeare frequently required upon initial synchronization. The condition in which the synchronizing circuitry locks onto random data and not the synchronization word is called afalse lock. The probabilityofthis false lock condition occurring, P ,isgiven by:           FL                                                                                P k N                                                 i                                      P  ¼  i ¼ 0                                  ð 12: 2 Þ                                       FL     2 N  Here N is the length of the synchronization code in bits and k is the number of differencesallowedbetween the received codeand the exact codevalue.   The synchronization codecan also be missed if the data become corrupted in the channel. The probability  of amissed synchronization code, P M ,caused by channel errorsisgiven by:                                                                                  X N   N                               P   ¼           p i ð 1   p Þ N   i                 ð 12: 3 Þ                                 M          i                                     i ¼ k þ 1  Here p is the channel bit-error rate while N and k are as before.   If the minor frame has alength of L bits, then withachannel BER of p ,the frame data has aprobabilityof  correct reception, P 0 ,given by:                                                  L                                      P 0 ¼ð1   p Þ                                ð 12: 4 Þ  This can be used by the designer to estimate necessity for anyerror detection and correction coding to be applied to the transmission. Equation (12.4) can also be used to estimate the probabilityofcorrect reception for command data words.   Packet telemetrysystems are becoming more common, especially in systems wherethe data acquisition and data reception subsystems have highdegrees of computational capabilityand the link between them can be viewed as areliable link. Packet systems haveseveral advantages over frame systems, with the main advantage being ﬂexibility. With apacket system, instead of having amaster commutation rate, the sampling rate for each sensor or sensor system can be individualized to the natural signal bandwidth. For example, battery voltages being measured maynot change signiﬁcantly over ﬁve minutes. With frame telemetrythey maybe sampled more frequently than once aminute because of the commutation rate speciﬁcation. With packets, the voltages maybesampled only as needed and then transmitted in adata packet. Packetsalso havethe advantage of allowing the data to be more easily routed over acomputer networkfor analysis and distribution to end users. Examples of packet telemetryformats are given in Ref. [4].   The general packet format is composed of aheader,containing accounting and addressing information, followed by the actual data similar to the format in Figure12.9. The packet mayend with atrailer composedofTelemetry                                                                          12-9                     Packet         Time                   Header         Stamp     Sensor 1 Sensor 2Sensor 3Sensor 4                 protocol specific  4bytes    2bytes  2bytes 2bytes  2bytes                           FIGURE 12.9 An example of atelemetrypacket.  error-checking codes or other administrativeinformation. The addressing information in the packet identiﬁes the sensor system originating the packet and the destination process for analysis. Other information included in the header might be counters to identify the sequence number or atime stamp to show when the packet was created. The header will often contain asize parameter to specify the length of the data ﬁeld.   Foractual transmission across the data channel, alink-layer packet may be used. This link packet may multiplex the data packets from several subsystems together for efﬁcient transport. If the data channel is synchronous, as with aradio channel, then it is common for the channel packets to be sent at regular intervals to maintain transmission synchronization. When this is done, ﬁll packets are used to keep the channel activeif thereare no actual data to be sent. The packet header will then haveaspecial code to indicate that the packet is aﬁll packet and should not be processed.   The packet usually begins with asynchronization marker,just as frame telemetrydoes. The same synchronization codes can be used in packet systems as in frame systems. After synchronization the header is analyzed to identify the type of processing to be performed based on the sourceofthe data. The probabilityof missing the synchronization marker because of channel errorsiscomputed as in Equation (12.3) and the probabilityofreceiving the packet correctly is computed as in Equation (12.4). Manycommercial protocols haveerror detection as partofthe protocol speciﬁcation so this can be used to assess data qualityinthe packet.   Both frame and packet telemetrysystems requireasynchronization phase at the receiver.Because frame telemetrysystems are designed for channels with higherror and drop-out rates, they use amore complicated synchronization algorithm to ensure that the receiver is properly synchronized to the transmitter.Packet systems use channels with higher inherent reliabilitysothe probabilityoflosing adata frame, or having adata frame arrivewith errors, is smaller.Inthis case, the networking protocol interfacedoes the necessary synchronization and delivers the frames to the user.Ifthe frames are corruptedorlost, then the transmission protocol will negotiate aretransmission.   The data synchronization process begins after the receiver has successfully locked onto the data signal. This initial lock-up involves locking onto the transmission carrier and then ﬁnding the individual bits in the data stream. At this point, the data is astream of bits without anyspeciﬁc context. The synchronization process places the bits into acontext so that the individual measurements can be located in the data stream. The synchronization process for atelemetryframe follows these steps, as illustrated in Figure 12.10:    1. In the SearchState, ﬁnd the occurrences of the frame synchronization word by using acorrelator      comparing the data with the desiredstored pattern.                                   Check Failure        Drop Lock                  Start     Search             Check               Lock                          State              State              State                     Search             Check           While All                   Duration           Duration        Checks Pass                     FIGURE 12.10 State diagram for the data synchronization process.12-10                                  Broadcasting and Optical Communication Technology     2. Oncethe synchronization marker is reliably found, ensurethat it repeats at the minor frame rate in the      Check State and that anymanagement information in the frame is intact by using aframe synchronizer.    3. Oncethe frame structureisfully identiﬁed, begin processing the data in the Lock State.   At the SearchState and the Check State, the hardware may stayinthese states for morethan one occurrence of the pattern to ensure that the correct pattern has really arrived and that it is not accidentally found in the random data.  12.6     Data   Processing  The telemetryand telecommand system will requireadata processing system to provide the necessary conversion function between the needs of the user and the rawdata format found at the hardware level. The major real-time processingfunctions are those associated with maintaining the telemetrydatabase and the user interface. Both of these are time critical functions that must keep pacewiththe incoming data. The processing must also validate and transmit the commands given by the user as they are entered.   The database maybeconﬁgured to hold the rawdata values (engineering units) or fully converted and calibrated values that are ready for display or analysis by the eventual user,orboth. Additional values can be created by performing combinations of basic measurements to form anew value or a derived parameter.The derivedparameter can then be uniquely stored in the telemetrydatabase. This database information can be displayed in real time for the user as well as stored to adata ﬁle for more extensive later analysis. The received data is normally stored in atelemetrydatabase as the values are received. The database is designed to allowfor parameter searches of individual values to supportdata analysis and displaying the values for the user.The database will contain morethan the received values. It will also havecaution and alarm limits to warn the operator when the measurements exceed desired values as well as identiﬁers for the mnemonic for the value, markers for wheretolocate the data in the incoming data stream, and time tags for the last received value. An example of atelemetrydatabase structure is illustrated in Figure 12.11. The ﬁrst partofthe ﬁgure shows how the data may be organized. The second partofthe ﬁgure shows how caution and alarm limits maybe conﬁgured.                                Caution Ca ution Alarm Alarm Enable Last  Time     No.  Type Location Mnemonic LowHigh      Low  High Warnings Value       1AnMF4           XY Sp 15.0      8.0    3.0  10.0    Y6.25       1058712      2DiMF7,20        Rx Cnt 11632864                     Y71058715      3BiMF9(3)        RNG TONE                     1Y01058717      4AnSF2;4,8       Com 1Sig 100.0  250.0  50.0 500.0   Y550.5      1058716                             (a)Telemetrydatabase structure                      Alarm Band            Data                                CautionBand         Sensor         Output                        Normal Range                                                Caution Band                       AlarmBand                                    time                            (b) Normal, Caution, and Alarmbands                 FIGURE 12.11 Telemetrydatabase structure and associated user alarm levels.Telemetry                                                                         12-11                                    Payload Subsystem-3                                                                        Sensor 2           Sun Sensor A2.2 V      Gyro A1200 rpm        Gyro C1553 rpm           Sun Sensor B5.0 V      Gyro B1300 rpm        Gyro D1662 rpm           Earth Sensor North 3.0 V Earth Sensor South 2.9 V ACS CHK +1       30                                 Real-Time Graphics Plot #1                    25                                                                  6                                                                  4                                                                  2              − 60 − 55 − 45 − 40 − 35 − 30 − 25 − 20 − 15 − 10 − 5            10                                   Command History                                                                              5           >TFON,2,3                                  Current Time: 333:01:02:03           >TFOF,6,7           >_                                                                      Caution List                                                                    [new data]            Power      Track       Weather    Payload   Payload                                                                    Alarm List           [new data]                          SS-1      SS-2      FIGURE 12.12 An example user data display for graphs, thermometer indicators, and text display windows.    The information needs to be accessible to the user who will be acting on it. The user interface mayhave regions for real-time plots (strip chart-typeplots), numerical displays of data, ‘‘dip stick’’or‘‘thermometer’’ indicators for individual values, and regions for user input into the command process. This type of displayis illustrated in Figure12.12. This type of displayassumes that the graphical capabilities of the display system are being used to organize the data into individual windows for easier recall by the user.The individual parameters in the displaywindows need to be linked to the telemetrydatabase. This can be done viaexplicit coding or through structured database calls that link the displayareas to the database structure.   The user will require adata input areafor telecommand operations. This user input will be linked to a telecommand processor to accept the input, validate that it forms an acceptable command, and then translate the command into the basic structureunderstood by the actuators. The validation step will include not only syntax validation but also user authentication. Telecommand data sent over an open channel (radio or Internet) may be encrypted to prevent unauthorized listening or spooﬁng of the commands by an unauthorized user.  References  1.  S. Horan, Introduction to PCM Telemetering Systems,2nd ed., Boca Raton: CRCPress, 2002.  2.  ‘‘American National StandardT1.523-2001TelecomGlossary2000,’’ http://www.atis.org/tg2k, February      2001.  3.  TelemetryStandards, IRIG Standard106–01, Part 1, Secretariat, Range Commanders Council, U.S.      Army White Sands Missile Range, NM, February2001.  4.  TelemetryStandards, IRIG Standard106–01, Part 2, Secretariat, Range Commanders Council, U.S.      Army White Sands Missile Range, NM, February2001.  5.  Manual of Regulations and Procedures for Federal Radio Frequency Management,Washington, DC:      National Telecommunications and Information Administration, September 2003.This page intentionally left blank                                                                                     13                                    Computer-Aided Design                                                            and Analysis of                                                         Communication                                                                               Systems                                     13.1   Introduction .................................................................... 13-1                                   13.2   The Role of Simulation ..................................................... 13-2                                   13.3   Motivation for the UseofSimulation .................................. 13-3                                   13.4   Limitations of Simulation .................................................. 13-3                                   13.5   Simulation Structure......................................................... 13-4                                   13.6   The InterdisciplinaryNature of Simulation .......................... 13-5                                   13.7   Model Design................................................................... 13-5                                   13.8   Low-Pass Models .............................................................. 13-6                                   13.9   Pseudorandom Signal and Noise Generators......................... 13-7                                   13.10  Transmitter,Channel, and Receiver Modeling ....................... 13-9 William   H. Tranter              13.11  Symbol ErrorRate Estimation ...........................................13-10 Virginia Polytechnic Institute    13.12  Validation of Simulation Results ........................................13-12 KurtL.Kosbar                      13.13  ASimple Example Illustrating Simulation Products..............13-12 University of Missouri–Rolla      13.14  Conclusions ....................................................................13-15   13.1      Introduction  It should be clear fromthe precedingchapters that communication systems exist to perform awide variety of tasks. The demands placed on today’scommunication systems necessitate higher data rates, greater ﬂexibility, and increased reliability.Communication systems are thereforebecoming increasingly complex,and the resulting systems cannot usually be analyzed using traditional (pencil and paper) analysis techniques. In addition, communication systems often operate in complicated environments that are not analytically tractable. Examples include channels that exhibit severebandlimiting, multipath, fading,interference,non- Gaussian noise, and perhaps even burst noise. The combination of   acomplex system   and  acomplex environment makes the design and analysis of these communication systems aformidable task. Some level of computer assistance must usually be invoked in both the design and analysis process. The appropriate levelof computer assistance can range from  simply using numerical techniques to solve adifferential equation                                                                                                  13-113-2                                   Broadcasting and Optical Communication Technology  deﬁning an element or subsystem to developing a computer simulation of the end-to-end communication system.   There is another important reason for the current popularityofcomputer-aided analysis and simulation techniques. It is nowpractical to makeextensiveuse of these techniques. The computing powerofmany personal computers and workstations available today exceeds the capabilities of manylarge mainframe computers of only adecade ago.The low cost of these computing resourcesmake them widely available. As a result, signiﬁcant computing resources are available to the communications engineer within the ofﬁce or even the home environment. Personal computers and workstations tend to be resources dedicated to aspeciﬁc individual or project. Since the communications engineer working at his or her desk has control over the computing resource,lengthysimulations can be performed without interfering with the work of others. Over the past few years anumber of software packages havebeen developed that allowcomplex communication systems to be simulated with relative ease [Shanmugan, 1988]. The best of these packages contains awide varietyofsubsystem models as well as integrated graphics packages that allowwaveforms, spectra, histograms, and performancecharacteristics to be displayed without leaving the simulation environment. Forthose motivated to generate their own simulation code, the widespread availabilityofhigh-quality C, Pascal, and FORTRAN  compilers makes it possible for large application-speciﬁc simulation programs to be developed for personal computers and workstations. When computing tools are both available and convenient to use, they will be employed in the day-to-day efforts of system analysts and designers.   The purpose of this chapter is to provide abrief introduction to the subject of computer-aided designand analysis of communication systems. Since computer-aided design and analysis almost always involves some levelofsimulation, we focus our discussion on the important subject of the simulation of communication systems.   Computer simulations can, of course, neverreplaceaskilled engineer,althoughthey can be atremendous help in both the design and analysis process. The most powerful simulation program cannot solveall the problems that arise, and the process of making trade-offdecisions will always be based on experience. In addition, evaluating and interpreting the results of acomplex simulation require considerable skill and insight. While these remarks seem obvious, as computer-aided techniques become morepowerful, one is tempted to replace experience and insight with computing power.  13.2     The   Role   of Simulation  The main purposes of simulation are to help us understand the operation of acomplex communication system, to determine acceptable or optimum parameters for implementation of asystem, and to determine the performanceofacommunication system. There are basically twotypes of systems in which communication engineers haveinterest: communication links and communication networks.   Acommunication link is usually asingle source, asingle user,and the componentsand channel between source and user.Atypical link architecture is shown in Figure13.1. The importantperformanceparameter in a digital communication link is typically the reliability of the communication link as measured by the symbol or bit error rate (BER). In an analog communication link the performanceparameter of interest is typically the signal-to-noise ratio (SNR) at the receiver input or the mean-square error of the receiver output. The simulation is usually performed to determine the effect of system parameters, such as ﬁlter bandwidths or code rate, or to determine the effect of environmental parameters, such as noise levels, noise statistics, or power spectral densities.                               FIGURE 13.1 Basic communication link.Computer-Aided Design and Analysis of Communication Systems                        13-3    Acommunication network is acollection of communication links with manysignal sources and many users. Computer simulation programs for networks often deal with problems of routing,ﬂow and congestion control, and the network delay. While this chapter deals with the communication link, the reader is reminded that network simulation is also an important area of study.The simulation methodologies used for communication networks are different from those used on links because, in acommunication link simulation, each waveform present in the system is sampled using aconstant sampling frequency.Incontrast, network simulations are event-driven, with the important events being such quantities as the time of arrival of a message.   Simulations can be developed to investigate either transient phenomena or steady-state properties of a system. The study of the acquisition time of aphase-lock loop receiver is an example of atransient phenomenon. Simulations that are performed to study transient behavior often focus on asingle subsystem such as areceiver synchronization system. Simulations that are developed to study steady-state behavior often model the entire system. An example is asimulation to determine the BER of asystem.  13.3     Motivation for the Use of Simulation  As mentioned previously,simulation is areasonable approach to manydesign and analysis problems because complex problems demand that computer-based techniques be used to supporttraditional analytical approaches. There are manyother motivations for making use of simulation.   Acarefully developed simulation is much like having abreadboardimplementation of the communication system available for study.Experiments can be performed using the simulation much like experiments can be performed using hardware.System parameters can be easily changed, and the impact of these changes can be evaluated. By continuing this process, parameteric studies can easily be conducted and acceptable, or perhaps even optimum, parameter values can be determined. By changing parameters, or even the system topology, one can play‘‘what if’’ games much more quickly and economically using asimulation than with asystem realized in hardware.   It is often overlookedthat simulation can be used to supportanalysis. Manypeople incorrectly view simulation as atool to be used only when asystem becomes too complex to be analyzed using traditional analysis techniques. Used properly,simulation goes hand in hand withtraditional techniques in that simulation can often be used to guide analysis. Aproperly developed simulation provides insight into system operation. As an example, if asystem has manyparameters, these can be varied in away that allows the most importantparameters, in terms of system performance, to be identiﬁed. The least importantparameters can then often be discarded, with the result being asimpler system that is moretractable analytically.Analysis also aids simulation. The development of an accurate and efﬁcient simulation is often dependent upon acareful analysis of various portions of the system.  13.4     Limitations of Simulation  Simulation, useful as it is, does have limitations. It must be remembered that asystem simulation is an approximation to the actual system under study.The nature of the approximations must be understood if one is to have conﬁdence in the simulation results.The accuracy of the simulation is limited by the accuracy to which the various components and subsystems within the system are modeled. It is often necessarytocollect extensive experimental data on system componentstoensurethat simulation models accurately reﬂect the behavior of the components. Even if this step is done with care,one can only trust the simulation model over the range of values consistent with the previously collected experimental data. Amain source of error in a simulation results because models are used at operating points beyond which the models are valid.   In addition to modeling difﬁculties, it should be realized that the digital simulation of asystem can seldom be made perfectly consistent with the actual system under study.The simulation is affected by phenomena not present in the actual system. Examples are the aliasing errors resulting from the sampling operation and the ﬁnite word length (quantization) effects present in the simulation. Practical communication systems use13-4                                   Broadcasting and Optical Communication Technology  anumber of ﬁlters, and modeling the analog ﬁlters present in the actual system by the digital ﬁlters required by the simulation involves anumber of approximations. The assumptions and approximations used in modeling an analog ﬁlter using impulse-invariant digital ﬁlter synthesis techniques are quite different from the assumptions and approximations used in bilinear z -transform techniques. Determining the appropriate modeling technique requires careful thought.   Another limitation of simulation lies in the excessivecomputer run time that is often necessaryfor estimating performanceparameters. An example is the estimation of the system BER for systems having very low nominal bit errorrates. We will expand on this topic later in this chapter.  13.5     Simulation     Structure  As illustrated in Figure13.1, acommunication system is acollection of subsystems such that the overall system provides areliable path for information ﬂowfrom source to user.Inacomputer simulation of the system, the individual subsystems must ﬁrst be accurately modeled by signal processing operations. The overall simulation program is acollection of these signal processing operations and must accurately model the overall commu- nication system. The importantsubject of subsystem modeling will be treated in afollowing section.   The ﬁrst step in the development of asimulation program is to deﬁne the topologyofthe system, which speciﬁes the manner in which the individual subsystems are connected. The subsystem models must then be deﬁned by specifying the signal processing operation to be performed by each of the various subsystems. Asimulation structuremay be either ﬁxed topologyorfreetopology. In aﬁxedtopologysimulation, the basic structureshown in Figure13.1 is modeled. Various subsystems can be bypassed if desiredbysetting switches, but the basic topologycannot be modiﬁed. In afreetopologystructure,subsystems can be interconnected in anyway desired and new additional subsystems can be added at will.   Asimulation program for acommunication system is acollection of at least threeoperations, shown in Figure13.2, althoughinawell-integrated simulation these operations tend to merge together.The ﬁrst operation, sometimes referred to as the preprocessor, deﬁnes the parameters of each subsystem and the intrinsic parameters that control the operation of the simulation. The second operation is the simulation exercisor, which is the simulation program actually executed on the computer.The third operation performed in a simulation program is that of postprocessing. This is acollection of routines that format the simulation output in away which provides insight into system operations and allows the performanceofthe communication system under study to be evaluated. Apostprocessor usually consists of anumber of graphics-based routines, allowing the user to view waveforms and other displaysgenerated by the simulation. The postprocessor also consists of anumber of routines that allowestimation of the bit error rate, signal-to-noise ratios, histograms, and powerspectral densities.   When faced with the problem of developing asimulation of acommunication system, the ﬁrst fundamental choice is whether to develop acustom simulation using ageneral-purpose high-level language or to use one of the manyspecial-purpose communication system simulation languages available. If the decision is made to develop adedicated simulation using ageneral-purpose language, anumber of resources are needed beyond a qualitycompiler and amathematics library. Also needed are libraries for ﬁltering routines, software models for each of the subsystems contained in the overall system, channel models, and the waveform displayand data analysis routines needed for the analysis of the simulation results (postprocessing). While at least some of the required softwarewill havetobedeveloped at the time the simulation is being written, manyofthe required                         FIGURE 13.2 Typical structureofasimulation program.Computer-Aided Design and Analysis of Communication Systems                        13-5  routines can probably be obtained from digital signal processing (DSP) programs and other available sources. As moresimulation projects are completed, the database of available routines becomes larger.   The other alternative is to use a dedicated simulation language,which makes it possible for one who does not havethe necessaryskills to create acustom simulation using ahigh-level language to developa communication system simulation. Many simulation languages are available for both personal computers and work-stations [Shanmugan, 1988]. While the use of these resources can speed simulation development, the user must ensure that the assumptions used in developing the models are well understood and applicable to the problemofinterest. In choosing adedicated language from among those that are available, one should select alanguage that has an extensive model library, an integrated postprocessor with awide varietyofdata analysis routines, on-line help and documentation capabilities, and extensiveerror-checking routines.  13.6     The InterdisciplinaryNature of Simulation  The subject of computer-aided design and analysis of communication systems is very much interdisciplinary in nature.The major disciplines that bear on the subject are communication theory, DSP,numerical analysis, and stochastic process theory. The roles played by these subjects is clear.The simulation user must have knowledge of the behavior of communication theoryifthe simulation results are to be understood. The analysis techniques of communication theoryallow simulation results to be veriﬁed. Sinceeach subsystem in the overall communication system is asignal processing operation, the tools of DSP provide the algorithms to realize ﬁlters and other subsystems. Numerical analysis techniques are used extensivelyinthe development of signal processing algorithms. Since communication systems involve random data signals, as well as noise and other disturbances, the concepts of stochastic process theoryare importantindeveloping models of these quantities and also for determining performanceestimates.  13.7     Model Design  Practicing engineers frequently use models to investigate the behavior of complex systems. Traditionally, models havebeen physical devicesoraset of mathematical expressions. The widespread use of powerful digital computers now allows one to generate computer programs that model physical systems. Althoughthe detailed development and use of computer models differs signiﬁcantly from their physical and mathematical counterparts, the computer models sharemanyofthe same design constraints and trade-offs. Forany model to be useful one must guarantee that the response of the model to stimuli will closely match the response of the target system, the model must be designed and fabricated in much less time and at signiﬁcantly less                             FIGURE 13.3 Design constraints and trade-offs.13-6                                   Broadcasting and Optical Communication Technology  expense than the target system, and the model must be reasonably easy to validate and modify.Inaddition to these constraints, designers of computer models must assurethat the amount of processor time required to execute the model is not excessive. The optimal model is the one that appropriately balances these conﬂicting requirements. Figure13.3 describes the typical design trade-offfaced when developing computer models. Asomewhat  surprising observation is that the optimal model is often not the one that most closely approximates the target system. Ahighly detailed model will typically require atremendous amount of time to develop,will be difﬁcult to validate and modify,and mayrequire prohibitive processor time to execute. Selecting amodel that achieves agood balancebetween these constraints is as much an artasascience. Being ware of the trade-offs which exist, and must be addressed, is the ﬁrst step toward mastering the artofmodeling.  13.8     Low-Pass     Models  In most cases of practical interest the physical layer of the communication system will use continuous time (CT) signals, while the simulation will operate in discrete time (DT). For the simulation to be useful, one must develop DT signals and systems that closely match their CT counterparts. This topic is discussed at length in introductoryDSP texts. Aprominent result in this ﬁeld is the Nyquist sampling theorem,which states that if a  CT signal has no energyabovefrequency f h Hz, one can create aDTsignal that contains exactly the same information by sampling the CT signal at anyrate in excess of 2 f h samples per second. Sincethe execution time of the simulation is proportional to the number of samples it must process, one naturally uses the lowest sampling rate possible. While the Nyquist theorem should not be violated for arbitrarysignals, when the CT signal is bandpass one can use low-pass equivalent (LPE) waveforms that contain all the information of the CT  signal but can be sampled slower than 2 f h .   Assume the energyinabandpass signal is centered about acarrier frequency of f c Hz and ranges from f l to f h Hz, resulting in abandwidth of f h – f l ¼ W Hz, as in Figure13.4. It is not unusual for W to be manyorders of magnitude less than f c .The bandpass waveform x ( t )can be expressed as afunction of twolow-pass signals. Twoessentially equivalent LPE expansions are known as the envelope/phase representation [Davenportand Root, 1958],                                 x ð t Þ¼A ð t Þ cos½ 2 p f c t þ y ð t Þ           ð 13: 1 Þ  and the quadrature representation,                            x ð t Þ¼x c ð t Þ cosð 2 p f c t Þ x s ð t Þ sinð 2 p f c t Þð13: 2 Þ  All four real signals A ( t ), y ( t ), x c ( t ), and x s ( t )are low pass and havezeroenergyabove W /2 Hz. Acomputer simulation that replaces x ( t )withapair of LPE signals will require far less processor time sincethe LPE  waveforms can be sampled at W as opposed to 2 f h samples per second. It is cumbersome to work with two signals rather than one signal. Amoremathematically elegant LPE expansion is                                            no                                   x ð t Þ¼Re v ð t Þ e j 2 p fct                  ð 13: 3 Þ                          FIGURE 13.4 Amplitude spectrum of abandpass signal.Computer-Aided Design and Analysis of Communication Systems                        13-7  where v ( t )isalow-pass, complex-time domain signal that has no energyabove W /2 Hz. Signal v ( t )isknown as the complex envelope of x ( t )[Haykin, 1983]. It contains all the information of x ( t )and can be sampled at W samples per second without aliasing.This notation is disturbing to engineers accustomed to viewing all time domain signals as real. However,acomplete theoryexists for complex time domain signals, and with surprisingly little effortone can deﬁne convolution, Fourier transforms, analog-to-digital and digital-to-  analog conversions, and manyother signal processing algorithms for complex signals. If f c and W are known, the LPE mapping is one-to-one so that x ( t )can be completely recovered from v ( t ). While it is conceptually  simpler to sample the CT signals at arate in excess of 2 f h and avoid the mathematical difﬁculties of the LPE representation,the tremendous difference between f c and W makes the LPE far moreefﬁcient for computer simulation. This type of trade-offfrequently occurs in computer simulation. Acareful mathematical analysis of the modeling problem conducted before anycomputer code is generated can yield substantial performance improvements over aconceptually simpler,but numerically inefﬁcient approach.   The fundamental reason the LPE representation outlined above is popular in simulation is that one can easily generate LPE models of linear time-invariant bandpass ﬁlters. The LPE of the output of abandpass ﬁlter is merely the convolution of the LPE of the input signal and the LPE of the impulse response of the ﬁlter.Itis far more difﬁcult to determine aLPE model for nonlinear and time-varying systems. There are numerous approaches that trade offﬂexibilityand simplicity. If the system is nonlinear and time invariant, aVolterra series can be used. While this series willexactly represent the nonlinear device, it is often analytically intractable and numerically inefﬁcient. Fornonlinear devices withalimited amount of memorythe AM/AM, AM/PM   [Shimbo,1971] LPE model is useful. This model accurately describes the response of many microwave ampliﬁers including traveling-wavetubes, solid-state limiting ampliﬁers, and, under certain conditions, deviceswhich exhibit hysteresis. The Chebyshev transform [Blachman, 1964] is useful for memoryless nonlinearities such as hardand soft limiters. If the nonlinear device is so complex that none of the conventional LPE models can be used, one mayneed to convert the LPE signal back to its bandpass representation,route the bandpass signal through amodel of the nonlinear device, and then reconvert the output to aLPE signal for further processing.Ifthis must be done, one has the choice of increasing the sampling rate for the entire simulation or using different sampling rates for various sections of the simulation. The second of these approaches is known as a multirate simulation [Cochiereand Rabiner,1983]. The interpolation and decimation operations required to convertbetween sampling rates can consume signiﬁ- cant amounts of processor time. One must carefully examine this trade-offtodetermine if amultirate simula- tion will substantially reduce the execution time over asingle, highsampling rate simulation. Efﬁcient and ﬂexible modeling of nonlinear devices is in general adifﬁcult task and continues to be an area of activeresearch.  13.9     Pseudorandom        Signal and Noise Generators  The preceding discussion was motivated by the desire to efﬁciently model ﬁlters and nonlinear ampliﬁers. Sincethese devices often consume the majorityofthe processor time, they are given highpriority. However, thereare anumber of other subsystems that do not resemble ﬁlters. One example is the data sourcethat generates the message or waveform which must be transmitted. While signal sources may be analog or digital in nature,wewill focus exclusively on binarydigital sources. The two basic categories of signals produced by these devicesare known as deterministic and random. When performing worst-case analysis, one will typically produce known, repetitive signal patterns designed to stress aparticular subsystem within the overall communication system. For example, asignal with few transitions may stress the symbol synchronization loops, while asignal with manyregularly spaced transitions maygenerate unusually wide bandwidth signals. The generation of this type of signal is straightforward and highly application dependent. To test the nominal system performanceone typically uses arandom data sequence. While generation of atruly random signal is arguably impossible [Knuth, 1981], one can easily generate pseudorandom (PN) sequences. PN sequence generators havebeen extensivelystudied sincethey are used in Monte Carlo integration and simulation [Rubinstein, 1981] programs and in avarietyofwideband and secure communication systems. The twobasic13-8                                   Broadcasting and Optical Communication Technology                         FIGURE 13.5 Six-stage binaryshift register PN generator.                         FIGURE 13.6 Output of asix-stage maximal length BSR.   structures for generating PN sequences are binaryshift registers (BSRs) and linear congruential algorithms (LCAs).   Digital data sources typically use BSRs, while noise generators often use LCAs. Alogic diagram for asimple BSR is shown in Figure 13.5. This BSR consists of aclock, six D-type ﬂip–ﬂops (F/F), and an exclusive OR gate denoted by amodulo-twoadder.Ifall the F/F are initialized to 1, the output of the device is the waveform shown in Figure13.6. Notice that the waveform is periodic with period 63 ¼ 2 6 –1,but within one cycle the output has manyofthe properties of arandom sequence. This demonstrates all the properties of the BSR, LCA, and more advanced PN sequence generators. All PN generators havememoryand must thereforebe initialized by the user beforethe ﬁrst sample is generated. The initialization data is typically called the seed. One must choose this seed carefully to ensurethe output will have the desiredproperties (in this example, one must avoid setting all F/F to zero). All PN sequence generators willproduceperiodic sequences. This mayor maynot be aproblem. If it is aconcern, one should ensurethat one period of the PN sequencegenerator is longer than the total execution time of the simulation. This is usually not asigniﬁcant problem, since one can easily construct BSRs that haveperiods greater than 1027 clock cycles. The ﬁnal concern is how closely the behavior of the PN sequence generator matches atruly random sequence.Standardstatistical analysis algorithms havebeen applied to manyofthese generators to validate their performance.   Manydigital communication systems use m bit ( M-ary) sources where m . 1. Figure 13.7 depicts asimple algorithm for generating a M-aryrandom sequence from abinarysequence. The clock must nowcycle through m cycles for everygenerated symbol, and the period of the generator has been reduced by afactor of m .This mayforcethe use of alonger-period BSR. Another common application of PN sequencegenerators is to producesamples of acontinuous stochastic process, such as Gaussian noise. Astructurefor producing these samples is shown in Figure 13.8. In this case the BSR has been replaced by an LCA[Knuth, 1981]. The LCAis very similar to BSR in that it requires aseed value, is clocked once for each symbol generated, and will generate aperiodic sequence. One can generate awhite noise process with an arbitraryﬁrst-order probabilitydensity function (pdf)bypassing the output of the LCAthrough an appropriately designed nonlinear,memoryless mapping.Simple and well-documented algorithms exist for the uniform to Gaussian mapping.Ifone wishes to generate anonwhite process, the output can be passed throughthe appropriate ﬁlter.Generation of a wide-sense stationaryGaussian stochastic process with aspeciﬁed powerspectral densityisawell-understood and -documented problem. It is also straightforward to generate awhite sequencewith an arbitraryﬁrst-order pdf or to generate aspeciﬁed power spectral densityifone does not attempt to control the pdf.Computer-Aided Design and Analysis of Communication Systems                        13-9                              FIGURE 13.7 M -aryPNsequence generator.                              FIGURE 13.8 Generation of Gaussian noise.  However,the problem of generating anoise source with an arbitrarypdf and an arbitrarypowerspectral density is asigniﬁcant challenge [Sondhi, 1983].  13.10      Transmitter,Channel, and Receiver Modeling  Most elements of transmitters, channels, and receivers are implemented using standard DSP techniques. Effects that are difﬁcult to characterize using mathematical analysis can often be included in the simulation with little additional effort. Common examples include gain and phase imbalance in quadraturecircuits, nonlinear ampliﬁers, oscillator instabilities, and antenna platform motion. One can typically use LPE waveforms and devicestoavoid translating the modulator output to the carrier frequency. Signal levels in physical systems often varybymanyorders of magnitude, with the output of the transmitters being extremely highenergysignals and the input to receivers at very low energies. To reduce execution time and avoid working with extremely large and small signal level simulations, one often omits the effects of linear ampliﬁers and attenuators and uses normalized signals. Sincethe performanceofmost systems is afunction of the signal-to-noise ratio,and not of absolute signal level,normalization will havenoeffect on the measured performance. One must be careful to document the normalizing constants so that the original signal levels can be reconstructed if needed. Even some rather complex functions, such as error detecting and correcting codes, can be handled in this manner.Ifone knows the uncoded error rate for asystem, the coded error rate can often be closely approximated by applying amathematical mapping.Aswill be pointed out below,the amount of processor time required to produceameaningful error rate estimate is often inversely proportional to the error rate. While an uncoded error rate may be easy to measure, the coded error rate is usually so small that it would be impractical to execute asimulation to measurethis quantitydirectly.The performanceofacoded communication system is most often determined by ﬁrst executing asimulation to establish the channel symbol error rate.Ananalytical mapping can then be used to determine the decodedBER fromthe channel symbol error rate.   Oncethe signal has passed thoughthe channel, the original message is recovered by areceiver.This can typically be realized by asequence of digital ﬁlters, feedback loops, and appropriately selected nonlinear devices. Areceiver encounters anumber of clearly identiﬁable problems that one may wish to address independently.For example, receivers must initially synchronize themselvestothe incoming signal. This may involve detecting that an input signal is present, acquiring an estimate of the carrier amplitude, frequency,13-10                                  Broadcasting and Optical Communication Technology  phase, symbol synchronization, frame synchronization, and, in the case of spread spectrum systems, code synchronization. Onceacquisition is complete, the receiver enters asteady-state mode of operation, where concerns such as symbol error rate, mean time to loss of lock, and reaction to fading and interferenceare of primaryimportance. To characterize the system, the user may wish to decouple the analysis of these parameters to investigate relationships that mayexist.   Forexample, one may run anumber of acquisition scenarios and gather statistics concerning the probability of acquisition within aspeciﬁed time interval or the mean time to acquisition. To isolate the problems faced in synchronization from the inherent limitation of the channel, one may wishtouse perfect synchronization information to determine the minimum possible BER. Then the symbol or carrier synchronization can be held at ﬁxed errors to determine sensitivitytothese parameters and to investigate worst-case performance. Noise processes can be used to varythese parameters to investigate more typical performance. The designer may also wish to investigate the performanceofthe synchronization system to various data patterns or the robustness of the synchronization system in the face of interference. The abilitytomeasure the system response to one parameter while awiderange of other parameters are held ﬁxed and the abilitytoquickly generate awide varietyofenvironments are some of the moresigniﬁcant advantages that simulation enjoys over more conventional hardwareand analytical models.  13.11      Symbol    Error  Rate   Estimation  One of the most fundamental parameters to measureinadigital communication system is the steady-state BER. The simplest method for estimating the BER is to perform a Monte Carlo (MC) simulation.The simulation conducts the same test one would perform on the physical system. All data sources and noise sources producetypical waveforms.The output of the demodulator is compared to the output of the message source, and the BER is estimated by dividing the number of observederrors by the number of bits transmitted. This is asimple technique that willwork with anysystem that has ergodic [Papoulis, 1965] noise processes. The downside of this approach is that one must often pass averylarge number of samples through the system to produce areliable estimate of the BER. The question of how manysamples must be collected can be answered using conﬁdence intervals. The conﬁdenceinterval gives ameasure of how close the true BER will be to the estimate produced by the MC simulation. Atypical conﬁdence interval curve is shown in Figure13.9. The ratio of the size of the conﬁdenceinterval to the size of the estimate is afunction of the number of errors observed. Convenient rules of thumb for this work are that after one error is observedthe point estimate is                    FIGURE 13.9 Typical conﬁdence interval (BER) point estimate ¼ 10  6 .Computer-Aided Design and Analysis of Communication Systems                       13-11                          FIGURE 13.10 Typical digital communication system.   accurate to within 3orders of magnitude, after 10 errorsthe estimate is accurate to within afactor of 2, and after 100 errors the point estimate will be accurate to afactor of 1.3. This requirement for tens or hundreds of errors to occur frequently limits the usefulness of MC simulations for systems that have low error rates and has motivated research into more efﬁcient methods of estimating BER.   Perhaps the fastest method of BER estimation is the semi-analytic (SA) or quasi-analytic technique [Jeruchim, 1984]. This technique is useful for systems that resemble Figure13.10. In this case the mean of the decision metric is afunction of the transmitted data pattern and is independent of the noise. All other parameters of the pdf of the decision metric are afunction of the noise and are independent of the data. This means that one can analytically determine the conditional pdf of the decision metric given the transmitted data pattern. By using total probabilityone can then determine the unconditional error rate. The problem with conventional mathematical analysis is that when the channel has asigniﬁcant amount of memoryor the nonlinearity is rather complex, one must compute alarge number of conditional densityfunctions. Simulation can easily solvethis problem for most practical systems. Anoise-freesimulation is executed, and the value of the decision metric is recordedinadata ﬁle. Oncethe simulation is complete, this information can be used to reconstruct the conditional and ultimately the unconditional error rate. This method generates highly accurate estimates of the BER and makes very efﬁcient use of computer resources, but can only be used in the special cases whereone can analytically determine the conditional pdf.   The MC and SA techniques fall at the twoextremes of BER estimation. MC simulations requireno apriori information concerning the system performanceorarchitecture but may requiretremendous amounts of computer time to execute. SA techniques require an almost trivial amount of computer time for manycases but requirethe analyst to have aconsiderable amount of information concerning the system. There is a continuing search for algorithms that fall in between these extremes. These variance reduction algorithms all share the propertyofmaking alimited number of assumptions concerning system performanceand architecture,then using this information to reducethe variance of the MC estimate. Popular techniques are summarized in [Jeruchim, 1984] and include importancesampling,large deviation theory, extremal statistics, and tail extrapolation. To successfully use one of these techniques one must ﬁrst understand the basic concept behind the technique. Then one should carefully determine what assumptions weremade concerning the system architecture to determine if the system under study satisﬁes the requirements. This can be adifﬁcult task sinceitisnot always clear what assumptions are required for aspeciﬁed technique to be applicable. Finally,one should always determine the accuracy of the measurement through some technique similar to conﬁdence interval estimation.13-12                                  Broadcasting and Optical Communication Technology  13.12      Validation    of Simulation     Results  One often constructs asimulation to determine the value of asingle parameter,such as the system BER. However the estimate of this parameter has little or no value unless one can ensurethat the simulation model closely resembles the physical system. Anumber of methods can be used to validate asimulation. Individually,none of them will guarantee that the simulation results are accurate,but taken as agroup,they form aconvincing argument that the results are realistic. Seven methods of validation are mathematical analysis, comparison with hardware, bounding techniques, degenerate case studies, reasonable relationship tests, subsystem tests, and redundant simulation efforts.   If one has access to mathematical analysis or hardwarethat predicts or approximates the performanceofthe system, one should obviously comparethe simulation and mathematical results. Unfortunately,inmost cases these results willnot be available. Even thoughexact mathematical analysis of the system is not possible, it may be possible to develop bounds on the system performance. If these bounds are tight, they mayaccurately characterize the system performance, but even loose bounds will be useful since they help verify the simulation results. Most systems haveparameters that can be varied. While it may be mathematically difﬁcult to determine the performanceofthe system for arbitraryvalues, it is often possible to mathematically determine the results when parameters assume extreme or degenerate values.   Other methods of validation are decidedly less mathematical. One may wish to varyparameters and ascertain whether the performanceparameter changes in areasonable manner.For example, small changes in SNR rarely cause dramatic changes in system performance. When constructing asimulation, each subsystem, such as ﬁlters, nonlinear ampliﬁers, and noise and data sources, should be thoroughly tested beforebeing included in alarger simulation. Be aware,however,that correct operation of all the various subsystems that makeupacommunication  system does not imply that the overall system performs correctly.Ifone is writing his or her own code, one must verify that there are no software bugs or fundamental design errors. Even if one purchases acommercial software package, thereisnoguarantee that the designer of the software models made the same assumptions the user willmake when using the model. In most cases it will be far easier to test a module beforeitisinserted into asimulation than it will be to isolate aproblem in acomplex pieceofcode. The ﬁnal check one maywish to perform is aredundantsimulation. There are manymethods of simulating a system. One may wish to havetwo teams investigate aproblem or haveasingle team implement asimulation using two different techniques to verify that the results are reasonable.  13.13      ASimple     Example     Illustrating   Simulation     Products  To illustrate the output that is typically generated by acommunication system simulation, asimple example is considered. The system is that considered in Figure13.10. An OQPSK (offset quadrature phase-shift keyed) modulation format is assumed so that one of four waveforms is transmitted during each symbol period. The data sourcemay be viewed as asingle binarysource, in which the source symbols are taken twoatatime when mapped onto atransmitted waveform, or as twoparallel data sources, with one source providing the direct channel modulation and the second source providing the quadrature channel modulation. The signal constellation at the modulator output appears as shown in Figure 13.11(a), with the corresponding eye diagram appearing as shown in Figure 13.11(b). The eyediagram is formed by overlaying successive time intervals of atime domain waveform onto asingle graph, much as would be done withacommon oscilloscope. Sincethe simulation sampling frequency used in generating Figure13.11(b) was 10 samples per data symbol, it is easily seen that the eye diagram was generated by retracing every2data symbols or 20 simulation samples. SinceFigure13.11(a) and (b) correspond to the modulator output, which has not yet been ﬁltered, the transitions between binarystates occur in one simulation step.After ﬁltering,the eye diagram appears as shown in Figure13.11(c). Aseventh-order Butterworthbilinear z -transform digital ﬁlter was assumed with a3-dB bandwidth equal to the bit rate. It should be noted that the bit transitions shown in Figure13.11(c) do not occur at the same times as the bit transitions shown in Figure13.11(b). The differenceComputer-Aided Design and Analysis of Communication Systems                       13-13   FIGURE 13.11 Transmitter signal constellation and eyediagrams: (a) OQPSK signal constellation; (b) eye diagram of modulator output; (c) eye diagram of ﬁltered modulator output.  is due to the group delayofthe ﬁlter.Note in Figure13.10 that the transmitter also involves anonlinear ampliﬁer.Wewillsee the effects of this component later in this section.   Another interesting point in the system is within the receiver.Since the communication system is being modeled as abaseband system due to the use of the complex-envelope representation of the bandpass waveforms generated in the simulation, the data detector is represented as an integrate-and-dump detector. The detector is then modeled as asliding-average integrator,inwhich the width of the integration windowis one bit time. The integration is thereforeoverasingle bit period when the sliding windowissynchronized with abit period. The direct-channel and quadrature-channel waveforms at the output of the sliding-average integrator are shown in Figure13.12(a). The corresponding eye diagrams are shown in Figure13.12(b). In order to minimize the error probabilityofthe system, the bit decision must be based on the integrator output at the time for which the eye opening is greatest. Thus the eye diagram provides importantinformation concerning the sensitivityofthe system to timing errors.   The signal constellation at the sliding integrator output is shown in Figure 13.12(c) and should be carefully compared to the signal constellation shown in Figure13.11(a) for the modulator output. Three effects are apparent. First, the signal points exhibit some scatter,which, in this case, is due to intersymbol interference resulting fromthe transmitter ﬁlter and additive noise. It is also clear that the signal is both compressed and rotated. These effects are due to the nonlinear ampliﬁer that was mentioned previously.For this example13-14                                  Broadcasting and Optical Communication Technology   FIGURE 13.12 Integrator output signals and system error probability: (a) sliding integrator output signals; (b) sliding integrator output eyediagram; (c) sliding integrator output signal constellation; (d) error probability.   simulation the nonlinear ampliﬁer is operating near the saturation point, and the compression of the signal constellation is due to the AM/AM characteristic of the nonlinearityand the rotation is due to the AM/PM characteristic of the nonlinearity.   The performanceofthe  overall communication system is illustrated in Figure13.12(d). The error probabilitycurve is perhaps the most important simulation product. Note that both uncoded and coded results are shown. The coded results were calculated analytically from the uncoded results assuming a(63, 55) Reed–Solomon code. It should be mentioned that semi-analytic simulation was used in this example since, as can be seen in Figure13.10, the noise is injected into the system on the receiver side of the nonlinearity so that linear analysis maybeused to determine the effects of the noise on the system performance.   This simple example servestoillustrate only afew of the possible simulation products. There are many other possibilities including histograms, correlation functions, estimates of statistical moments, estimates of the power spectral density, and estimates of the signal-to-noise ratio at various points in the system.   Aword is in order regarding spectral estimation techniques. Twobasic techniques can be used for spectral estimation: Fourier techniques and model-based techniques. In most simulation problems one is blessed with atremendousamount  of data concerning sampled waveforms but does not haveasimple model describingComputer-Aided Design and Analysis of Communication Systems                       13-15  how these waveforms are produced. Forthis reason model-based spectral estimation is typically not used. The most common form of spectral estimation used in simulation is the Welch periodogram. While this approach is straightforward, the effects of windowing the data sequence must be carefully considered, and tens or even hundreds of data windows must be averaged to achieve an accurate estimate of the power spectral density.  13.14      Conclusions  We haveseen that the analysis and design of today’scomplex communication systems often requires the use of computer-aided techniques. These techniques allow the solution of problems that are otherwise not tractable and provide considerable insight into the operating characteristics of the communication system.  Deﬁning   Terms Communication link:   Apoint-to-point communication system that typically involves asingle informa-      tion source and asingle user.This is in contrast to acommunications network, which usually involves      manysources and manyusers. Computer-aided design and analysis:  The process of using computer assistance in the design and      analysis of complex systems. In our context, the design and analysis of communication systems,      computer-aided design and analysis often involves the extensive use of simulation techniques.      Computer-aided techniques often allow one to address design and analysis problems that are not      tractable analytically. Computer simulation:  Aset of computer programs which allowsone to imitate the important aspects of      the behavior of the speciﬁc system under study.Simulation can aid the design process by,for example,      allowing one to determine appropriate system design parameters or aid the analysis process by,for      example, allowing one to estimate the end-to-end performanceofthe system under study. Dedicated simulation language: Acomputer language, either text based or graphics based, speciﬁcally      developed to facilitate the simulation of the various systems under study,such as communication      systems. Low-pass equivalent (LPE) model: Amethod of representingbandpass signals and systems by low-pass      signals and systems. This technique is extremelyuseful when developing discrete time models of      bandpass continuous-time systems. It can substantially reduce the sampling rate required to prevent      aliasing and does not result in anyloss of information. This in turn reduces the execution time required      for the simulation. This modeling technique is closely related to the quadrature representation of      bandpass signals. Monte Carlo simulation:   Atechnique for simulating systems that contain signal sources producing      stochastic or random signals. The signal sources are modeled by pseudorandom generators.      Performancemeasures, such as the symbol error rate, are then estimated by time averages. This is a      general-purpose technique that can be applied to an extremely wide range of systems. It can, however,      requirelarge amounts of computer time to generate accurate estimates. Pseudorandom  generator: An algorithm or device that generates deterministic waveforms which in many      ways resemble stochastic or random waveforms. The power spectral density, autocorrelation, and other      time averages of pseudorandom signals can closely match the time and ensemble averages of stochastic      processes. These generators are useful in computer simulation whereone maybeunable to generate a      truly random process, and they havethe added beneﬁt of providing reproducible signals. Semi-analytic simulation: Anumerical analysis technique that can be used to efﬁciently determine the      symbol error rate of digital communication systems. It can be applied wheneverone can analytically      determine the probabilityofdemodulation error given aparticular transmitted data pattern. Although      this technique can only be applied to arestricted class of systems, in these cases it is far moreefﬁcient, in      terms of computer execution time, than Monte Carlo simulations.13-16                                  Broadcasting and Optical Communication Technology  Simulation validation: The process of certifying that simulation results are reasonable and can be used      with conﬁdenceinthe design or analysis process. Symbol error rate: Afundamental performancemeasure for digital communication systems. The symbol      error rate is estimated as the number of errors divided by the total number of demodulated symbols.      When  the communication system is ergodic, this is equivalent to the probabilityofmaking a      demodulation error on anysymbol.  References P. Balaban, K.S. Shanmugan, and B.W.Stuck (Eds.), ‘‘Special issue on computer-aided modeling,analysis and      design of communication systems,’’ IEEE J. Selected Areas Commun., no.1,1984. P. Balaban, E. Biglieri, M.C. Jeruchim, H.T.Mouftah, C.H. Sauer,and K.S. Shanmugan (Eds.), ‘‘Computer-      aided modeling,analysis and design of communication systems II,’’ IEEE J. Selected Areas Commun.,      no.1,1988. N. Blachman, ‘‘Bandpass nonlinearities,’’ IEEE Trans. Inf. Theory ,no. 2, 1964. P. Bratley,B.L. Fox, and L.E. Schrage, AGuide to Simulation, NewYork: Springer-Verlag,1987. R. Cochiere and L. Rabiner, Multirate Digital Signal Processing, EnglewoodCliffs, N.J.: Prentice-Hall,1983. W. Davenportand W. Root, An Introduction to the TheoryofRandom Signals and Noise,New York: McGraw-      Hill, 1958. R.L. Freeman, Telecommunications Systems Engineering, NewYork: Wiley,1996. J. Gagliardi, Optical Communication, NewYork: Wiley,1995. J. Gibson, The Mobile Communications Handbook, Boca Raton, Fla.: CRCPress, 1996. S. Haykin, Communication Systems,New York: Wiley,1983. S. Haykin, Communication Systems, NewYork: Wiley,1994. M. Jeruchim, P. Balaban, and K. Shanmugan, Simulation of Communication Systems,New York: Plenum, 1992. M. Jeruchim, ‘‘Techniques for estimating the bit error rate in the simulation of digital communication      systems,’’ IEEE J. Selected Areas Commun., no.1,January1984. D. Knuth, The ArtofComputer Programming, vol. 2, Seminumerical Algorithms, 2nd ed., Reading,Mass.:      Addison-Wesley,1981. H.T.Mouftah, J.F. Kurose, and M.A. Marsan (Eds.), ‘‘Computer-aided modeling,analysis and design of      communication networks I,’’ IEEE J. Selected Areas Commun., no.9,1990. H.T.Mouftah, J.F. Kurose, and M.A. Marsan (Eds.), ‘‘Computer-aided modeling,analysis and design of      communication networks II,’’ IEEE J. Selected Areas Commun., no.1,1991. A. Papoulis, Probability,Random Variables, and Stochastic Processes,New York: McGraw-Hill, 1965. R. Rubinstein, Simulation and the Monte Carlo Method, NewYork: Wiley,1981. K. Shanmugan, ‘‘An update on software packages for simulation of communication systems (links),’’ IEEE      J. Selected Areas Commun., no.1,1988. N.D.Sherali, ‘‘Optimal Location of Transmitters,’’ IEEE J. on Selected Areas in Communications, pp.662–673,      May1996. O. Shimbo,‘‘Effects of intermodulation, AM-PM conversion, and additivenoise in multicarrier TWT systems,’’      Proc. IEEE,no. 2, 1971. M. Sondhi, ‘‘Random processes with speciﬁed spectral density and ﬁrst-order probabilitydensity,’’ Bell Syst.      Tech. J. vol. 62, 1983.  Further  Information Until recently the subject of computer-aided analysis and simulation of communication systems was avery difﬁcult researcharea. There werenotextbooks devoted to the subject, and the fundamental papers were scattered over alarge number of technical journals. While anumber of excellent books treated the subject of simulation of systems in which random signals and noise are present [Rubinstein, 1981; Bratley et al., 1987], none of these books speciﬁcally focused on communication systems.Computer-Aided Design and Analysis of Communication Systems                       13-17    Starting in 1984, the IEEE Journal on Selected Areas in Communications (JSAC) initiated the publication of a sequence of issues devoted speciﬁcally to the subject of computer-aided design and analysis of communication systems. Abrief study of the contents of these issues tells much about the rapid development of the discipline. The ﬁrst issue, published in January1984 [Balaban et al., 1984], emphasizes communication links, although thereare anumber of papers devoted to networks. The portion devotedtolinks contained alarge collection of papers devoted to simulation packages.   The second issue of the series was published in 1988 and is roughly evenly split between links and networks [Balaban et al., 1988]. In this issue the emphasis is much moreontechniques than on simulation packages. The third partofthe series is atwo-volume issue devotedexclusively to networks [Mouftah et al., 1990, 1991].   As of this writing,the book by Jeruchim et al. is the only comprehensivetreatment of the simulation of communication links [Jeruchim, 1984]. It treats the component and channel modeling problem and the problems associated with using simulation techniques for estimating the performanceofcommunication systems in considerable detail. This textbook, together with the previously cited JSACissues, gives agood overview of the area.This page intentionally left blank                                                                           II Mathematics, Symbols, and Physical Constants              Greek Alphabet ........................................................................................................................... II-3             International System of Units (SI) ........................................................................................... II-3              Deﬁnitions of SI Base Units * Names and Symbols for the SI Base Units *              SI Derived Units with Special Names and Symbols *             Units in UseTogether withthe SI             Conversion Constants and Multipliers ................................................................................... II-6              Recommended Decimal Multiples and Submultiples * Conversion Factors—Metric              to English * Conversion Factors—English to Metric * Conversion Factors—General *              Temperature Factors * Conversion of Temperatures             Physical Constants....................................................................................................................... II-8              General * p Constants * Constants Involving e * Numerical Constants             Symbols and Terminologyfor Physical and Chemical Quantities...................................... II-9              Classical Mechanics * Electricityand Magnetism * Electromagnetic Radiation *             Solid State             Credits ........................................................................................................................................ II-13             Probability for Electrical and Computer Engineers Charles W. Therrien ..................... II-14              The Algebra of Events * Probability * An Example * Conditional Probability              and Bayes’Rule * Communication Example  Ronald J.   Tallarida Temple University  THE GREATACHIEVEMENTS in engineering deeply affect the lives of all of us and also serve to remind us of the importance of mathematics. Interest in mathematics has grown steadily with these engineering achievements and with concomitant advances in pure physical science.Whereas scholars in nonscientiﬁc ﬁelds, and even in such ﬁelds as botany, medicine, geology, etc., can communicate most of the problems and results in nonmathematical language, this is virtually impossible in present-day engineering and physics. Yetit is interesting to note that until the beginning of the twentieth century, engineers regarded calculus as something of amystery. Modern students of engineering now study calculus, as well as differential equations, complex variables, vector analysis, orthogonal functions, and avarietyofother topics in applied analysis. The study of systems has ushered in matrix algebra and, indeed, most engineering students nowtake linear algebra as acoretopic early in their mathematical education.                                                                                       II-1II-2                                         Mathematics, Symbols, and Physical Constants    This section contains concise summaries of relevant topics in applied engineering mathematics and certain keyformulas, that is, those formulas that are most often needed in the formulation and solution of engineering problems. Whereas even inexpensiveelectroniccalculators contain tabular material (e.g., tables of trigonometric and logarithmic functions) that used to be needed in this kind of handbook, most calculators do not givesymbolic results. Hence, we haveincluded formulas along with brief summaries that guide their use. In manycases we have added numerical examples, as in the discussions of matrices, their inverses, and their use in the solutions of linear systems. Atable of derivativesisincluded, as well as keyapplications of the derivative in the solution of problems in maxima and minima, related rates, analysis of curvature, and ﬁnding approximate roots by numerical methods. Alist of inﬁnite series, along with the interval of convergence of each, is also included.   Of the twobranches of calculus, integral calculus is richer in its applications, as well as in its theoretical content. Thoughthe theoryisnot emphasized here, important applications such as ﬁnding areas, lengths, volumes, centroids, and the work done by anonconstant forceare included. Both cylindrical and spherical polar coordinates are discussed, and atable of integrals is included. Vector analysis is summarized in aseparate section and includes asummaryofthe algebraic formulas involving dot and cross multiplication, frequently needed in the study of ﬁelds, as well as the importanttheoremsofStokesand Gauss. The partonspecial functions includes the gamma function, hyperbolic functions, Fourier series, orthogonal functions, and both Laplace and z -transforms. The Laplace transform provides abasis for the solution of differential equations and is fundamental to all concepts and deﬁnitions underlying analytical tools for describing feedback control systems. The z -transform, not discussed in most applied mathematics books, is most useful in the analysis of discrete signals as, for example, when acomputer receives data sampled at some prespeciﬁed time interval. The Bessel functions, also called cylindrical functions, arise in manyphysical applications, such as the heat transfer in a‘‘long’’ cylinder,whereas the other orthogonal functions discussed—Legendre, Hermite, and Laguerre polynomials—are needed in quantum mechanics and manyother subjects (e.g., solid-state electronics) that use concepts of modern physics.   The world of mathematics, even applied mathematics, is vast. Even the best mathematicians cannot keep up with morethan asmall pieceofthis world. The topics included in this section, however,havewithstood the test of time and, thus, are truly core for the modern engineer.   This section also incorporates tables of physical constants and symbols widely used by engineers. While not exhaustive,the constants, conversion factors, and symbols provided willenable the reader to accommodatea majorityofthe needs that arise in design, test, and manufacturing functions.Mathematics, Symbols, and Physical Constants                                        II-3                                                           Mathematics,                                                          Symbols, and                                             Physical Constants  Greek Alphabet             Greek         Greek      English     Greek       Greek      English            Letter        Name      Equivalent   Letter      Name      Equivalent            A   a          Alpha        aNn                  Nu            n           B   b          Beta         b        X   j       Xi            x           Gg             Gamma        gOoOmicron                         o˘           Dd             Delta        d        Q   p       Pi            P           E   e          Epsilon      e˘       P   r       Rho           r           Z   z          Zeta         z        S   s       Sigma         s           H   Z          Eta          e¯       T   t       Taut           Y   y q        Theta        th       Y   y       Upsilon       u           I   i          Iota          i       F   f j     Phi           ph           K   k          Kappa        kXw                  Chi           ch           Ll             Lambda        l       Cc          Psips           M   m          Mu           m        Oo          Omega         o¯  International System       of Units (SI)  The International System of units (SI) was adopted by the 11th General Conference on Weights and Measures (CGPM) in 1960. It is acoherent system of units built form seven SI base units, one for each of the seven dimensionally independent base quantities: they are the meter,kilogram, second, ampere,kelvin, mole, and candela, for the dimensions length, mass, time, electric current, thermodynamic temperature,amount of substance, and luminous intensity, respectively.The deﬁnitions of the SI base units are given below.The SI derived units are expressed as products of powersofthe base units, analogous to the corresponding relations between physical quantities but with numerical factors equal to unity.   In the International System thereisonly one SI unit for each physical quantity. This is either the appropriate SI base unit itself or the appropriate SI derivedunit. However,any of the approved decimal preﬁxes, called SI preﬁxes, may be used to construct decimal multiples or submultiples of SI units.   It is recommended that only SI units be used in scienceand technology(with SI preﬁxes where appropriate). Wherethere are special reasons for making an exceptiontothis rule, it is recommended always to deﬁne the units used in terms of SI units. This section is based on information supplied by IUPAC.  Deﬁnitions of SI Base Units   Meter: The meter is the length of path traveled by light in vacuum during atime interval of 1/299,792,458 of asecond (17th CGPM, 1983).   Kilogram: The kilogram is the unit of mass; it is equal to the mass of the international prototype of the kilogram (3rd CGPM, 1901).   Second: The second is the duration of 9,192,631,770 periods of the radiation corresponding to the transition between the twohyperﬁne levels of the groundstate of the cesium-133 atom (13th CGPM, 1967).II-4                                         Mathematics, Symbols, and Physical Constants    Ampere: The ampere is that constant current which, if maintained in twostraight parallel conductors of inﬁnite length, of negligible circular cross-section, and placed 1mapartinvacuum, would produce between these conductors aforce equal to 2 · 10  7 newton per meter of length (9th CGPM, 1948).   Kelvin: The kelvin, unit of thermodynamic temperature, is the fraction 1/273.16 of the thermodynamic temperatureofthe triple point of water (13th CGPM, 1967).   Mole: The mole is the amount of substanceofasystem which contains as manyelementaryentities as there are atoms in 0.012 kg of carbon-12. When the mole is used, the elementaryentities must be speciﬁed and may be atoms, molecules, ions, electrons, or other particles or speciﬁed groups of such particles (14th CGPM, 1971).   Examples of the use of the mole:                                       23                        23      1mol  of H 2 contains about 6.022 · 10 H 2 molecules, or 12.044 · 10 Hatoms.      1mol  of HgCl has amass of 236.04 g.       1mol  of Hg2 Cl2 has amass of 472.08 g.                 2 þ      1mol  of Hg2  has amass of 401.18 gand acharge of 192.97 kC.      1mol  of Fe0.91Shas amass of 82.88 g.      1mol  of e   has amass of 548.60 m gand acharge of   96.49 kC.      1mol  of photons whose frequencyis10 14 Hz has energyofabout 39.90 kJ.   Candela: The candela is the luminous intensityinagiven direction of asource that emits monochromatic radiation of frequency 540 · 1012 hertz and that has aradiant intensityinthat direction of (1/683) watt per steradian (16th CGPM, 1979).  Names   and  Symbols  for the SI Base  Units                       Physical Quantity  Name of SI Unit Symbol for SI Unit                   Length                   meter            m                  Masskilogram                              kg                  Time                     second           s                  Electric current         ampereA                  Thermodynamic temperature kelvin          K                  Amount of substancemole                   mol                  Luminous intensity       candela          cd  SI Derived  Units  with Special  Names   and Symbols                                  Name of   Symbol for        Expression in            Physical Quantity    SI Unit    SI Unit       Terms of SI Base Units         Frequency1             hertz          Hz       s   1        Force                  newton         Nmkg         s   2        Pressure, stresspascal                Pa       Nm  2   ¼ m   1 kg s   2        Energy,work, heat      joule          JNm              ¼ m 2 kg s   2        Power,radiant ﬂux      watt           WJs          1   ¼ m 2 kg s   3        Electric charge        coulomb        CAs        Electric potential,    volt           VJC           1  ¼ m 2 kg s   3 A   1          electromotiveforce        Electric resistanceohm                O        VA  1   ¼ m 2 kg s   3 A   2        Electric conductance   siemens        S        O   1   ¼ m   2 kg  1 s 3 A 2        Electric capacitancefarad             FCV           1  ¼ m   2 kg  1 s 4 A 2        Magnetic ﬂux density   tesla          TVsm          2  ¼ kg s   2 A   1        Magnetic ﬂux           weber         Wb        Vs      ¼ m 2 kg s   2 A   1        Inductance             henryHVA                    1 s ¼ m 2 kg s   2 A   2        Celsius temperature2   degree Celsius – CK                                                                      (continued)Mathematics, Symbols, and Physical Constants                                                                     II-5            SI Derived    Units  with Special  Names and Symbols (continued)                                            Name of      Symbol for              Expression in                 Physical Quantity          SI Unit        SI Unit           Terms of SI Base Units            Luminous ﬂux                  lumen               lm           cd sr           Illuminancelux                                     lx          cd sr m   2           Activity(radioactive)         becquerelBqs                       1           Absorbed dose (of radiation)  grayGyJkg                             1     ¼ m 2 s   2           Dose equivalent               sievertSvJkg                          1     ¼ m 2 s   2              (dose equivalent index)           Plane angle                   radian              rad          1         ¼  mm    1           Solid angle                   steradian            sr          1         ¼  m 2 m   2               1 For radial (circular) frequency and for angular velocitythe unit rad s   1 ,orsimply s   1 ,should be used,           and this maynot be simpliﬁed to Hz. The unit Hz should be used only for frequency in the sense of cycles           per second.              2 The Celsius temperature y is deﬁned by the equation:                                          y = – C ¼ T = K   273: 15               The SI unit of Celsius temperature interval is the degreeCelsius, – C, which is equal to the kelvin, K. – C           should be treated as asingle symbol, with no space between the – sign and the letter C. (The symbol – Kand           the symbol – should no longer be used.)    Units in Use       Together with the SI  These units are not partofthe SI, but it is recognized that they will continue to be used in appropriate  contexts. SI preﬁxes may be attached to some of these units, such as milliliter,ml; millibar,mbar; megaelectronvolt,   MeV;   kilotonne, ktonne.                  Physical                                       Symbol                Quantity            Name of Unit               for Unit           Value in SI Units                Time          minute                        min                 60 s               Time          hour                          h3600                    s               Time          day                           d86,400                    s               Plane angle   degree                        –                   ( p /180) rad               Plane angle   minute                        0                   ( p /10,800) rad               Plane angle   second                        00                  ( p /648,000) rad               Length        a˚ ngstrom1                   A ˚                 10  10 m               Area          barn                          b10                      28 m 2               Volume        liter                         l, Ldm                 3 ¼  10  3 m 3               Mass          tonne                         tMg                      ¼ 103 kg               Pressure      bar1                          bar                 105 Pa ¼  105 Nm   2               Energy        electronvolt2                 eV ( ¼ e · V)       < 1.60218 · 10  19 J                                                    2,3            12                           27               Mass          uniﬁed atomic mass unit       u(¼  m a ( C)/12)   < 1.66054 · 10    kg                  1 The a˚ ngstrom and the bar are approved by CIPM for ‘‘temporaryuse with SI units,’’ until CIPM               makes afurther recommendation. However,they should not be introducedwherethey are not used               at present.                 2 The values of these units in terms of the corresponding SI units are not exact, sincethey depend               on the values of the physical constants e (for the electronvolt) and N a (for the uniﬁed atomic mass               unit), which are determined by experiment.                 3 The uniﬁed atomic mass unit is also sometimes called the dalton, with symbol Da, although the               name and symbol havenot been approved by CGPM.II-6                                                         Mathematics,    Symbols,    and  Physical   Constants   Conversion          Constants         and    Multipliers   RecommendedDecimal                Multiples      and   Submultiples                Multiples and                             Multiples and               Submultiples     PreﬁxesSymbols           Submultiples     PreﬁxesSymbols                    1018          exa           E10              1         deci        d                   1015          peta          P10              2         centi       c                   1012          tera          T10              3         milli       m                   109           giga          G10              6         micro       m (Greek mu)                   106           mega          M10              9         nano        n                   103           kilo          k10               12       picop                   102           hecto         h10               15       femto       f                   10            deca          da           10  18        atto        a   Conversion       Factors—Metric          to  English                           To Obtain                 Multiply                   By                      Inches                    centimeters             0.3937007874                     Feet                      meters                  3.280839895                     Yards                     meters                  1.093613298                     Miles                     kilometers              0.6213711922                     Ounces                    grams                   3.527396195 ·  10  2                     Pounds                    kilogram                2.204622622                     Gallons (U.S. liquid)     liters                  0.2641720524                     Fluid ounces              milliliters (cc)        3.381402270 ·  10  2                     Squareinches              squarecentimeters       0.155003100                     Squarefeet                squaremeters           10.76391042                     Squareyards               squaremeters            1.195990046                     Cubic inches              milliliters (cc)        6.102374409 ·  10  2                     Cubic feet                cubic meters           35.31466672                     Cubic yards               cubic meters            1.307950619   Conversion       Factors—English          to  Metric*                          To Obtain                 Multiply                    By                      Microns                  mils                    25.4                     Centimeters              inches                   2.54                     Meters                   feet                     0.3048                     Meters                   yards                    0.9144                     Kilometers               miles                    1.609344                     Grams                    ounces                  28.34952313                     Kilograms                pounds                   0.45359237                     Liters                   gallons (U.S. liquid)    3.785411784                     Millimeters (cc)         ﬂuid ounces             29.57352956                     Squarecentimeters        squareinches             6.4516                     Squaremeters             squarefeet               0.09290304                     Squaremeters             squareyards              0.83612736                     Milliliters (cc)         cubic inches            16.387064                     Cubic meters             cubic feet               2.831684659 ·  10  2                     Cubic meters             cubic yards              0.764554858  *Boldfacenumbers  are exact; others are given to ten signiﬁcant ﬁgures where so indicated by the multiplier factor.Mathematics, Symbols, and Physical Constants                                                                     II-7   Conversion Factors—General*                              To Obtain                   Multiply                   By                      Atmospheres                  feet of water @4– C2.950          ·  10  2                     Atmospheres                  inches of mercury@0 – C3.342      ·  10  2                     Atmospheres                  pounds per squareinch        6.804 · 10  2                     BTU                          foot-pounds                  1.285 · 10  3                     BTU                          joules                       9.480 · 10  4                     Cubic feet                   cords                     128                     Degree(angle)                radians                    57.2958                     Ergs                         foot-pounds                  1.356 · 107                     Feet                         miles                    5280                     Feet of water @4– Catmospheres                          33.90                     Foot-pounds                  horsepower-hours             1.98 · 106                     Foot-pounds                  kilowatt-hours               2.655 · 106                     Foot-pounds per min          horsepower                   3.3 · 104                     Horsepower                   foot-pounds per sec          1.818 · 10  3                     Inches of mercury@0  – Cpounds per squareinch             2.036                     Joules                       BTU1054.8                     Joules                       foot-pounds                  1.35582                     Kilowatts                    BTUper min                   1.758 · 10  2                     Kilowatts                    foot-pounds per min          2.26 · 10  5                     Kilowatts                    horsepower                   0.745712                     Knots                        miles per hour               0.86897624                     Miles                        feet                         1.894 · 10  4                     Nautical miles               miles                        0.86897624                     Radians                      degrees                      1.745 · 10  2                     Squarefeet                   acres                   43,560                     Watts                        BTUper min                 17.5796   *Boldfacenumbers are exact; others are given to ten signiﬁcantﬁgures where so indicated by the multiplier factor.   Temperature Factors                                                   – F ¼ 9/5 ( – C) þ 32                               Fahrenheit temperature ¼ 1.8 (temperature in kelvins)   459.67                                                  – C ¼  5/9 [(– F)   32)]                                   Celsius temperature ¼ temperature in kelvins   273.15                                  Fahrenheit temperature ¼ 1.8 (Celsius temperature) þ 32   Conversion of       Temperatures                                From             To                             –               –                             Celsius         Fahrenheit     t F ¼ ( t C · 1.8) þ 32                                            Kelvin          T K ¼  t C þ 273.15                                            –                                             Rankine        T R ¼  ( t C þ 273.15) · 18                                                                 t   32                            – Fahrenheit    – Celsius       t  ¼ F                                                             C     1 : 8                                                                 t   32                                            Kelvin          T  ¼  F     þ 273: 15                                                             k     1 : 8                                            –                                             Rankine        T R ¼  t F þ 459.67                                            –                            Kelvin           Celsius        t C ¼ T K   273.15                                            –                                             Rankine        T R ¼ T K · 1.8                                                                  T                            – Rankine       Kelvin          T  ¼   R                                                             K   1 : 8                                            –                                             Fahrenheit     t F ¼ T R   459.67II-8                                         Mathematics, Symbols, and Physical Constants  Physical   Constants  General   Equatorial radius of the Earth ¼ 6378.388 km ¼ 3963.34 miles (statute)   Polar radius of the Earth,6356.912 km ¼ 3949.99 miles (statute)   1degreeoflatitude at 40– ¼ 69 miles   1international nautical mile ¼ 1.15078 miles (statute) ¼ 1852 m ¼ 6076.115 ft   Mean density of the earth ¼ 5.522 g/cm3 ¼ 344.7 lb/ft3   Constant of gravitation (6.673 ^ 0.003) · 10  8 cm3 gm  1 s   2   Acceleration due to gravity at sea level, latitude 45– ¼ 980.6194 cm/s2 ¼ 32.1726 ft/s2   Length of seconds pendulum at sea level, latitude 45– ¼ 99.3575 cm ¼ 39.1171 in.   1knot (international) ¼ 101.269 ft/min ¼ 1.6878 ft/s ¼ 1.1508 miles (statute)/h   1micron ¼  10  4 cm   1a˚ ngstrom ¼ 10  8 cm   Mass of hydrogen atom ¼ (1.67339^ 0.0031) · 10  24 g   Densityofmercuryat0– C ¼ 13.5955 g/ml   Densityofwater at 3.98– C ¼ 1.000000 g/ml   Density, maximum, of water,at3.98– C ¼ 0.999973 g/cm3   Densityofdry air at 0 – C, 760 mm ¼ 1.2929 g/l   Velocity of sound in dryair at 0 – C ¼ 331.36 m/s   1087.1 ft/s   Velocity of light in vacuum ¼ (2.997925^ 0.000002) · 1010 cm/s   Heat of fusion of water 0 – C ¼ 79.71 cal/g   Heat of vaporization of water 100– C ¼ 539.55 cal/g   Electrochemical equivalent of silver 0.001118 g/s international amp   Absolute wavelength of red cadmium light in air at 15– C, 760 mm pressure ¼ 6438.4696 A ˚   Wavelength of orange-red line of krypton 86 ¼ 6057.802 A ˚  p Constants              p ¼  3.14159 26535 89793 23846 26433 83279 50288 41971 69399 37511            1/p ¼ 0.31830 98861 83790 67153 77675 26745 02872 40689 19291 48091             p 2 ¼ 9.8690 44010 89358 61883 44909 99876 15113 53136 99407 24079            loge p ¼ 1.14472 98858 49400 17414 34273 51353 05871 16472 94812 91531          log p ¼ 0.49714 98726 94133 85435 12682 88290 89887 36516 78324 38044           p 10ﬃﬃﬃﬃ        log10 2 p ¼ 0.39908 99341 79057 52478 25035 91507 69595 02099 34102 92128  Constants   Involving e               e ¼ 2.71828 18284 59045 23536 02874 71352 66249 77572 47093 69996            1/e ¼ 0.36787 94411 71442 32159 55237 70161 46086 74458 11131 03177             e 2 ¼ 7.38905 60989 30650 22723 04274 60575 00781 31803 15570 55185       M ¼  log10e ¼ 0.43429 44819 03251 82765 11289 18916 60508 22943 97005 80367      1/M · ¼ loge 10 ¼ 2.30258 50929 94045 68401 79914 54684 36420 67011 01488 62877           log10M ¼ 9.63778 43113 00536 78912 29674 98645 –10  Numerical   Constants            p ﬃﬃ              2 ¼ 1.41421 35623 73095 04880 16887 24209 69807 85696 71875 37695            p ﬃﬃ            3 2 ¼ 1.25992 10498 94873 16476 72106 07278 22835 05702 51464 70151            loge 2 ¼ 0.69314 71805 59945 30941 72321 21458 17656 80755 00134 36026           log102 ¼ 0.30102 99956 63981 19521 37388 94724 49302 67881 89881 46211Mathematics, Symbols, and Physical Constants                                                                     II-9                  p ﬃﬃ                  3  ¼ 1.73205 08075 68877 29352 74463 41505 87236 69428 05253 81039                 p ﬃﬃ                 3 3 ¼ 1.44224 95703 07408 38232 16383 10780 10958 83918 69253 49935                loge 3 ¼ 1.09861 22886 68109 69139 52452 36922 52570 46474 90557 82275               log103 ¼ 0.47712 12547 19662 43729 50279 03255 11530 92001 28864 19070   Symbols and           Terminology for Physical and Chemical Quantities                    Name                     Symbol                   Deﬁnition                 SI Unit                                                  Classical Mechanics        Mass                          m                                                   kg         Reduced mass                  mm¼                        m 1 m 2 /(m 1 þ m 2 )kg        Density,mass density          rr¼                        M/V                      kg m   3        Relative density              dd¼                        r / r y                  l                                                                                                2        Surface density               r A , r S             r A ¼ m/A                     kg m        Momentum                      pp¼                        mv                       kg ms  1        Angular momentum,action       Ll¼                       r¥p                       Js                                                                    2                         2        Moment of inertia             I, JI¼                    S m i r i                 kg m        Force                         FF¼                        d p /dt ¼ ma             N        Torque, moment of aforce      T ,(M )               T ¼  r · F                    Nm        Energy                        E                                                   J         Potential energy              E p , V , F           E p ¼ F d s                   J                                                                         2        Kinetic energy                E k , T, K            e k ¼ (1/2)mv                 J        Work                          W, ww¼                    F d s                     J        Hamilton function             HH(                     q , p ) ¼ T ( q , p ) þ V ( q )J        Lagrange function             LLð                     q ; qq_ Þ T ð q ; qq_ Þ V ð q Þ J        Pressure                      p , Pp¼                  F/A                        Pa,Nm    2        Surface tension               g , s                 g ¼  d W /dA                  Nm   1 ,Jm   2        Weight                        G ,(W, P )            G ¼  mg                       N                                                                        2                     2     2        Gravitational constant        GF¼                       Gm1 m 2 / r               Nm   kg        Normal stress                 s                     s ¼  F/A                      Pa        Shear stress                  tt¼                        F/A                      Pa        Linear strain,                e , e                 e ¼  D l/l                    l          relative elongation        Modulus of elasticity,        EE¼                        s / e                    Pa          Young’smodulus        Shear strain                  gg¼                        D x/d                    l        Shear modulus                 GG¼                        t / g                    Pa         Volume strain, bulk strain    yy¼                        D V/V0                   l        Bulk modulus,                 KK¼ V                         0 (dp /dV )Pa          compression modulus         Z , mtx,z                 ¼  Z (dv x /dz )Pas        Viscosity,dynamic viscosity        Fluidity                      ff¼                        1/Z                      mkg   1 s        Kinematic viscosity           nn¼                        Z / r                    m 2 s   1         Friction coefﬁcient           m ,(f )               F frict ¼ m F norm            l        Power                         PP¼                        d W /dt                  W         Sound energy ﬂux              P, P a                P ¼  d E /dt                  W        Acoustic factors           Reﬂection factor            rr¼                        P t / P 0                l          Acoustic absorption factor  a a ,(a )             a a ¼ 1   r                   l          Transmission factor         tt¼                        P tr/ P 0                l        Dissipation factor            dd¼                        a a   t                  l                                                                                                  (continued)II-10                                                       Mathematics,    Symbols,    and  Physical   Constants        Symbols    andTerminologyfor       Physical   and  Chemical   Quantities   (continued)                  Name                     Symbol                   Deﬁnition                 SI Unit                                              Electricity and Magnetism       Quantity of electricity,      Q                                                   C         electric charge       Charge density                rr¼                        Q/V                      Cm   3       Surface charge density        s                     s ¼  Q/A                      Cm   2       Electric potential            V , f                 V ¼  d W /dQ                  V, JC  1        Electric potentialdifference  U , D V , D f         U  ¼  V 2   V 1               V       Electromotiveforce            EE¼ðF                       = Q Þ d s               V       Electric ﬁeld strength        EE¼                        F / Q ¼ grad  V          Vm   1       Electric ﬂux                  CC¼                        D d A                    C       Electric displacement         DD¼                         e E                     Cm   2       Capacitance                   CC¼                        Q/U                      F, CV  1       Permittivity                  e                     D  ¼ e E                      Fm   1                                                                     1   2                    1       Permittivityofvacuum          e 0                   e 0 ¼ m 0 c 0                 Fm       Relative permittivity         e r                   e r ¼ e / e 0                 l                                                                                               2       Dielectric polarization       PP¼                        D    e 0 E               Cm       (dipole moment  per volume)        Electric susceptibility       w e                   w e ¼ e r   1l       Electric dipole moment        p , m                 p ¼  Q r                      Cm       Electric current              II¼                        d Q /dt                  A       Electric current density      j, J                  I ¼ j d x A                   Am   2       Magnetic ﬂux density,         BF¼                        Qv  · B                  T         magnetic induction       Magnetic ﬂux                  F                     F ¼ B d A                     Wb       Magnetic ﬁeld strength        HB¼                        m H                      AM   1       Permeability                  m                     B ¼  m H                      NA   2 ,Hm   1                                                                                               1       Permeability of vacuum        m 0                                                 Hm       Relative permeability         m r                   m r ¼ m / m 0                 l                                                                                               1       Magnetization(magnetic        MM¼                         B / m 0   H             Am         dipole moment         per volume)        Magnetic susceptibility       w , k ,(w m )         w ¼  m r   1l                                                                                           3      1       Molar magnetic susceptibility w m                   w m ¼ V m w                   m  mol                                                                                             2      1       Magnetic dipole moment        m , m                 E p ¼ m·B                     Am,JT       Electricalresistance          R                     P = Y / I                     O       Conductance                   GG¼                        1/R                      S        Loss angle                    dd¼                        ( p /2) þ f I   f U      1, rad       Reactance                     XX¼                        ( U/I)sin d              O       Impedance  (complex           ZZ¼                        R þ  i X                 O         impedance)       Admittance(complex            YY¼                        1/Z                      S         admittance)       Susceptance                   BY¼                        G þ  iB                  S       Resistivity                   rr¼                        E/j                      O m       Conductivity                  k , g , s             k ¼  1/r                      Sm   1       Self-inductance               LE¼ L                         (dI /dt )H        Mutual inductance             M, L 12               E 1 ¼ L 12(Di 2 /dt )H       Magnetic vector potential     AB¼                        HHHHH · A                Wb  m   1       Poynting vector               SS¼                        E ·  H                   Wm   2                                              Electromagnetic Radiation       Wavelength                    l                                                   m       Speed of light                                                                    ms  1          in vacuum                   c 0         in amedium                  cc¼                        c 0 / n                                                                                                 (continued)Mathematics, Symbols, and Physical Constants                                                                   II-11         Symbols and     Terminologyfor Physical and Chemical Quantities (continued)                   Name                     Symbol                   Deﬁnition                 SI Unit                                               Electromagnetic Radiation                                                                                              1        Wavenumber in vacuum          VV¼                       V = c 0 ¼ 1 = n l         m        Wavenumber (in  amedium)      ss¼                        1/l                      m   1        Frequency                     nn¼                        c / l                    Hz        Circular frequency,           oo¼                        2 p n                    s   1 ,rad s   1          pulsatance         Refractiveindex               nn¼                        c 0 / c                  l        Planck constant               h                                                   Js        Planck constant/2p            ""¼                        h /2p                    Js        Radiant energy                Q, W                                                J        Radiant energy density        r , w                 r ¼  Q/V                      Jm  3        Spectral radiant energy          density                                                                                               3    1          in terms of frequency       r n , w n             r n ¼ d r /dn                 Jm    Hz                                                                                               2          in terms of wavenumber      r vv  , w vv          r vv  ¼ d r = d vv            Jm                                                                                               4          in terms of wavelength      r l , w l             r l ¼ dr/dl                   Jm        Einstein transition          probabilities                                                                                             1          Spontaneous emission        A nm                  d N n /dt ¼ A nmN n           s                                                                                               1          Stimulated emission         B nm                  d n n = d t ¼ r vv  ð VV nmÞ · B nmN n skg        Radiant power,                F , P                 F  ¼ d Q /dt                  W          radiant energyper time        Radiant intensity             II¼                        d F /dO                  Wsr   1                                                                                                2        Radiant exitance              MM¼                         d F /dA source          Wm          (emitted radiant ﬂux)        Irradiance(radiant ﬂux        E ,(I )               E ¼  d F / d A                Wm   2          received)         Emittance                     ee¼                        M/Mbb                    l                                                                       4                        2   4        Stefan–Boltzmann constant     s                     M bb ¼  s T                   Wm     K                                                                      2                       2        First radiation constant      c 1                   c 1 ¼ 2 p hc0                 Wm        Second radiation constant     c 2                   c 2 ¼ hc0 / k                 Km        Transmittance, transmission   t , T                 t ¼  F tr/ F 0                l          factor         Absorptance, absorption       aa¼                        f abs/ f 0               l          factor         Reﬂectance, reﬂection factor  rr¼                        f reﬂ/ F 0               l        (Decadic) absorbance          AA¼                        lg(1   a i )l        Napierian absorbance          BB¼                        ln(1   a i )l        Absorption coefﬁcient          (Linear) decadic            a, Ka¼                     A/l                      m   1          (Linear) napierian          aa¼                        B/l                      m   1          Molar (decadic)             ee¼                        a/c ¼ A/cl               m 2 mol  1          Molar napierian             kk¼                        a / c ¼ B/cl             m 2 mol  1        Absorption index              kk¼                       a = 4 p vv                l        Complex refractiveindex       nn^                   nn^ ¼ n þ i k                 l        Molar refraction              R, R                  R ¼ ð n 2   1 Þ V             m 3 mol  1                                          m                     ð n 2 þ 2 Þ m        Angle of optical rotation     a                                                   l, rad                                                      Solid State         Latticevector                 R , R 0                                             m        Fundamentaltranslation        a 1 ; a 2 ; a 3 , a; b; c R ¼ n 1 a 1 þ n 2 a 2 þ n 3 a 3 m          vectors for the crystal          lattice        (Circular) reciprocal lattice G                     G·R   ¼  2 p m                m   1          vector                                                                                                  (continued)II-12                                                       Mathematics,    Symbols,    and  Physical   Constants        Symbols    and  Terminologyfor     Physical   and  Chemical   Quantities   (continued)                  Name                     Symbol                   Deﬁnition                 SI Unit                                                     Solid State                                                                                             1       (Circular) fundamental        b 1 ; b 2 ; b 3 , a*; b*; c* a i · b k ¼ 2 p d ik   m         translation vectors for         the reciprocallattice       Latticeplane spacing          dm       Bragg angle                   y                     n l ¼ 2d sin y                l, rad       Order of reﬂection            n                                                   l       Order parameters         Short range                 s                                                   l         Long  range                 s                                                   1       Burgers vector                b                                                   m        Particle position vector      r, R j                                              m       Equilibriumposition vector    R o                                                 m         of an ion        Displacement vector of an ion uu¼                        R    R 0                 m       Debye–Waller factor           B, D                                                l                                                                                             1       Debye circular wavenumber     q D                                                 m                                                                                            1       Debye circular frequency      o D                                                 s       Gru¨ neisen parameter         g , G                 g ¼  a V / k C V              l                                                                        2                                                                  a N A z þ z   e       Madelung  constant            a , M                 E coul ¼ 4 pe R               l                                                                     0 0                    1   3       Density of states             N E                   N E ¼ d N(E)/dE               J  m                                                                                              3       (Spectral) density of         N o , gNo                 ¼  d N ( o )/do           sm         vibrational modes        Resistivity tensor            r ik                  E ¼  r · j                    O m                                                                    1                         1       Conductivity tensor           s ik                  s ¼  r                        Sm                                                                                               1   1       Thermal conductivitytensor    l ik                  J q ¼ l  ·grad T              Wm     K       Residual resistivity          r R                                                 O m       Relaxation time               tt¼                        l/vF                     s       Lorenz coefﬁcient             LL¼                        l / s T                  V 2 K   2                                                                                           3    1       Hall coefﬁcient               A H , R H             E ¼  r · j þ R H ( B · j )mC       Thermoelectric force          E                                                   V       Peltier coefﬁcient            Q                                                   V       Thomson  coefﬁcient           m ,(t )VK                                                1        Work  function                FF¼                        E 1   E F                J       Number  density,number        n ,(p )m                                               3         concentration        Gap  energy                   E g                                                 J       Donor  ionization energy      E d                                                 J       Acceptor ionization energy    E a                                                 J       Fermi energy                  E F , e F                                           J       Circular wavevector,          k, q                  k ¼  2 p / l                  m   1         propagation vector                                                                                             3/2       Bloch function                u k ( r )             c ( r ) ¼ u k ( r )exp(ik · r )m       Charge density of electrons   rr(                     r ) ¼ e c *(r ) cc _ ( r )Cm      3       Effectivemass                 m *kg                                                                                           2    1   1       Mobility                      mm¼                        n drift/ E               m  V   s       Mobility ratio                bb¼                        m n / m p                l                                                                                           2   1       Diffusion coefﬁcient          D                     d N /dp t ﬃﬃﬃ¼ ﬃ DA(dn /dx )ms       Diffusion length              LL¼                        D t                      m        Characteristic (Weiss)        f , f W                                             K         temperature        Curie temperature             T C                                                 K       Ne´ el temperature            T N                                                 KMathematics, Symbols, and Physical Constants                                       II-13  Credits  Material in Section II was reprinted from the following sources:   D. R. Lide, Ed., CRC Handbook of Chemistry and Physics, 76th ed., Boca Raton, FL: CRCPress, 1992: International System of Units (SI), conversion constants and multipliers (conversion of temperatures), symbols and terminologyfor physical and chemical quantities, fundamental physical constants, classiﬁcation of electromagnetic radiation.   D. Zwillinger,Ed., CRC Standard Mathematical Tables and Formulae, 30th ed., Boca Raton, FL: CRCPress, 1996: Greek alphabet, conversion constants and multipliers (recommended decimal multiples and submultiples, metric to English, English to metric, general, temperature factors), physical constants, series expansion.II-14                                        Mathematics, Symbols, and Physical Constants  Probability    for Electrical   and  Computer      Engineers Charles   W.  Therrien The  Algebra  of Events The study of probabilityisbased upon experiments that haveuncertain outcomes. Collections of these outcomes comprise events and the collection of all possible outcomes of the experiment comprise what is called the sample space,denoted by S .Outcomes are members of the sample space and events of interest are represented as sets of outcomes (see Figure II.1).   The algebra A that deals with representing events is the usual set algebra. If A is an event, then A c (the complement of A )represents the event that ‘‘A did not occur.’’The complement of the sample space is the               c null event, ;¼S .The event that both event A 1 and event A 2 haveoccurred is the intersection, written as ‘‘A 1 · A 2 ’’ or ‘‘A 1 A 2 ’’ while the event that either A 1 or A 2 or both have occurred is the union, written as         1 ‘‘A 1 þ A 2 .’’   Table II.1 lists the twopostulates that deﬁne the algebra A ,while Table II.2 lists seven axioms that deﬁne properties of its operations. Together these tables can be used to show all of the properties of the algebra of events. Table II.3 lists some additional useful relations that can be derived from the axioms and the postulates.    Sincethe events ‘‘A 1 þ A 2 ’’ and ‘‘A 1 A 2 ’’ are included in the algebra, it follows by induction that for anyﬁnite number of events A 1 þ A 2 þ   þ A N and A 1 ·A2     A N are also included in the algebra. Since problems often involve the union or intersection of an inﬁnite number of events, however, the algebra of events must be deﬁned to include these inﬁnite intersections and unions. This extension to inﬁnite unions and intersections is known as asigma algebra.   Aset of events that satisﬁes the two conditions:     1. A i A j ¼;6¼ for 6¼ i 6¼ j    2. A 1 þ A 2 þ A 3 þ   ¼S is known as a partition and is importantfor the solution of problems in probability. The events of a partition are said to be mutually exclusive and collectively exhaustive.The most fundamental partition is the set outcomes deﬁning the random experiment, which comprise the sample spaceby deﬁnition.  Probability Probability measures the likelihood of occurrenceofevents represented on ascale of 0to1.Weoften estimate probabilitybymeasuring the relative frequency of an event, which is deﬁned as                                       number  of occurrences of the event                relative frequency ¼                                   number  of repetitions of the experiment  (for alarge number of repetitions). Probabilitycan be deﬁned formally by the following axioms:     (I) The probability of anyevent is nonnegative:                                          Pr½ A   > 0                               ð II: 1 Þ    (II) The probability of the universal event (i.e., the entiresample space) is 1:                                          Pr½S ¼1                                   ð II: 2 Þ    1 Some authors use ˙ and ¨ rather than · and þ ,respectively.Mathematics, Symbols, and Physical Constants                                       II-15                                             s                                         S           events                                           A 1                                               A 2    FIGURE II.1 Abstract representation of the sample space S with outcome s and sets A 1 and A 2 representing events.     (III) If A 1 and A 2 are mutually exclusive, i.e., A 1 A 2 ¼;,then                                Pr½ A 1 þ A 2  ¼Pr½ A 1  þPr½ A 2  ðII:                3 Þ    (IV) If f A i g represent acountably inﬁnite set of mutually exclusive events, then                                             X 1                 Pr½ A 1 þ A 2 þ A 3 þ    ¼    Pr½ A i  ðif A i A j ¼; i 6¼ j ÞðII:   4 Þ                                            i ¼ 1 Note that althoughthe additivityofprobabilityfor anyﬁnite set of disjoint events follows from (III), the property has to be stated explicitly for an inﬁnite set in (IV). These axioms and the algebra of events can be used to showanumber of other important properties which are summarized in Table II.4. The last item in the table is an especially important formula since it uses probabilistic information about                       TABLE II.1 Postulates for an Algebra of Events                     1.  If A 2 A then A c 2 A                      2.  If A 1 2 A and A 2 2 A then A 1 þ A 2 2 A                       TABLE II.2 Axioms of Operations on Events                        c                     A 1 A 1 ¼;                       Mutual exclusion                     A 1 S¼A 1                        Inclusion                      c c                     ð A 1 Þ ¼ A 1                    Double complement                     A 1 þ A 2 ¼ A 2 þ A 1            Commutativelaw                     A 1 þðA 2 þ A 3 Þ¼ðA 1 þ A 2 ÞþA 3 Associativelaw                     A 1 ð A 2 þ A 3 Þ¼A 1 A 2 þ A 1 A 3 Distributivelaw                         c   c   c                     ð A 1 A 2 Þ ¼ A 1 þ A 2          DeMorgan’s law                         TABLE II.3 Additional Identities in the Algebra of Events                       S c ¼;                        A 1 þ;¼ A 1                    Inclusion                       A 1 A 2 ¼ A 2 A 1              Commutativelaw                       A 1 ð A 2 A 3 Þ¼ðA 1 A 2 Þ A 3 Associativelaw                       A 1 þðA 2 A 3 Þ¼ðA 1 þ A 2 ÞðA 1 þ A 3 Þ Distributivelaw                              c   c  c                       ð A 1 þ A 2 Þ ¼ A 1 A 2        DeMorgan’s lawII-16                                        Mathematics, Symbols, and Physical Constants                        TABLE II.4 Some Corollaries Derived from the Axioms                       of Probability                       Pr½ A c  ¼1   Pr½ A                         0 < Pr½ A   < 1                        If A 1 ˝ A 2 then Pr½ A 1   < Pr½ A 2                         Pr½;  ¼ 0                        If A 1 A 2 ¼;  then ¼ Pr½ A 1 A 2  ¼0                       Pr½ A 1 þ A 2  ¼Pr½ A 1  þPr½ A 2   Pr½ A 1 A 2     individual events to compute the probabilityofthe union of twoevents. The term Pr½ A 1 A 2   is referred to as the joint probability of the twoevents. This last equation showsthat the probabilities of two events add as in Equation (II.3) only if their joint probabilityis0.The joint probabilityis0when the two events  havenointersection ( A 1 A 2 ¼;).   Twoevents are said to be statistically independent if and only if                       Pr½ A 1 A 2  ¼Pr½ A 1   · Pr½ A 2  ðindependent eventsÞðII:     5 Þ  This deﬁnition is not derived from the earlier properties of probability. An argument to givethis deﬁnition intuitivemeaning can be found in Ref. [1]. Independenceoccurs in problems wheretwo events are not inﬂuencedbyone another and Equation (II.5) simpliﬁes such problems considerably.   Aﬁnal  importantresult deals with partitions. Apartition is aﬁnite or countably inﬁnite set of  events A 1 ; A 2 ; A 3 ; ... that satisfy the twoconditions:                                     A i A j ¼;for i 6¼ j                                  A 1 þ A 2 þ A 3 þ   ¼S  The events in apartition satisfy the relation:                                      X                                         Pr½ A i  ¼1                               ð II: 6 Þ                                       i  Further,if B is any other event, then                                            X                                    Pr½ B  ¼   Pr½ A i B  ðII:                        7 Þ                                             i  The latter result is referred to as the principle of total probability and is frequently used in solving problems. The principle is illustrated by aVenn diagram in FigureII.2. The rectangle represents the sample spaceand other events are deﬁned therein. The event B is seen to be comprised of all of the pieces                                    A 1                  A n                                                            B                                         A 2 B                                  A 2                            S                   FIGURE II.2 Venn diagram illustrating the principle of total probability.Mathematics, Symbols, and Physical Constants                                       II-17   that represent intersections or overlap of event B with the events A i .This is the graphical interpretation of Equation (II.7).  An Example  Simon’s Surplus Warehouse has large barrelsofmixed electronic components (parts) that you can buy by the handful or by the pound. Youare not allowedtoselect parts individually.Based on yourprevious experience, you havedetermined that in one barrel,29% of the parts are bad (faulted), 3% are bad resistors, 12% are good resistors, 5% are bad capacitors, and 32% are diodes. Youdecide to assign probabilities based on these percentages. Let us deﬁne the following events:                                        Event          Symbol                                Bad (faulted) component B                               Good component          G                               Resistor                R                               Capacitor               C                               Diode                   D    AVenn diagram representing this situation is shown belowalong with probabilitiesofvarious events as given:                                                     B  Pr[B] =0.29                                  R                    Pr[BR] =0.03                                                       Pr[GR] =0.12                                                       Pr[BC] =0.05                             D          C                                                       Pr[D] =0.32                              G  Note that sinceany component must be aresistor,capacitor,ordiode, the region labeled D in the diagram represents everything in the sample spacewhich is not included in R or C .   We can answer anumber of questions.    1. What is the probabilitythat acomponent is aresistor (either good or bad)?      Since the events B and G form apartition of the sample space, we can use the principle of total      probabilityEquation (II.7) to write:                       Pr½ R  ¼Pr½ GR þPr½  BR ¼0  : 12 þ 0 : 03 ¼ 0 : 15     2. Arebad parts and resistors independent?      We know that Pr½ BR ¼0 : 03 and we can compute:                            Pr½ B   · Pr½ R  ¼ð 0 : 29Þð0 : 15Þ¼0 : 0435       Since Pr½ BR  6¼ Pr½ B   · Pr½ R   ,the events are not independent.    3. Youhavenouse for either bad parts or resistors. What is the probabilitythat apartiseither bad and/or      aresistor?II-18                                        Mathematics, Symbols, and Physical Constants       Using the formula from Table II.4 and the previous result we can write:               Pr½ B þ R  ¼Pr½ B  þPr½ R   Pr½ BR ¼0 : 29 þ 0 : 15   0 : 03 ¼ 0 : 41     4. What is the probabilitythat apartisuseful to you?      Let Urepresent the event that the partisuseful. Then (see Table II.4):                            Pr½ U  ¼1   Pr½ U c  ¼1   0 : 41 ¼ 0 : 59     5. What is the probabilityofabad diode?      Observe that the events R , C ,and D form apartition, since acomponent has to be one and only one      type of part. Then using Equation (II.7) we write:                             Pr½ B  ¼Pr½ BR þPr½ BC þPr½  BD        Substituting the known numerical values and solving yields                        0 : 29 ¼ 0 : 03 þ 0 : 05 þ Pr½ BD  or Pr½ BD ¼0 : 21  Conditional  Probability  and Bayes’  Rule  The conditional probabilityofanevent A 1 given that an event A 2 has occurred is deﬁned by                                               Pr½ A 1 A 2                                    Pr½ A 1 j A 2  ¼                                 ð II: 8 Þ                                                Pr½ A 2     (Pr½ A 1 j A 2   is read ‘‘probabilityof A 1 given A 2 .’’) As an illustration, let us compute the probabilitythat a component in the previous example is bad given that it is aresistor:                                         Pr½ BR   0 : 03                              Pr½ B j R  ¼     ¼      ¼ 0 : 2                                         Pr½ R   0 : 15   (The value for Pr[ R ]was computed in question 1ofthe example.) Frequently the statement of a problem is in terms of conditional probability rather than joint probability, so Equation (II.8) is used in the form:                      Pr½ A 1 A 2  ¼Pr½ A 1 j A 2   · Pr½ A 2  ¼Pr½ A 2 j A 1   · Pr½ A 1  ðII: 9 Þ   (The last expression follows because Pr½ A 1 A 2   and Pr½ A 2 A 1   are the same thing.) Using this result, the principle of total probabilityEquation (II.7) can be rewritten as                                        X                                Pr½ B  ¼   Pr½ B j A j   Pr½ A j  ðII:               10Þ                                         j   where B is anyevent and f A j g is aset of events that forms apartition.   Now, consider anyone of the events A i in the partition. It follows from Equation (II.9) that                                            Pr½ B j A   · Pr½ A                                  Pr½ A j B  ¼     i      i                                    i          Pr½ B  Mathematics, Symbols, and Physical Constants                                       II-19  Then substituting in Equation (II.10) yields:                                            Pr½ B j A i   · Pr½ A i                                 Pr½ A i j B  ¼P                                    ð II: 11Þ                                            j Pr½ B j A j   Pr½ A j    This result is known as Bayes’ theorem or Bayes’rule.Itisused in anumber of problems that commonly arise in electrical engineering.Weillustrate and end this section with an example from the ﬁeld of communications.  Communication Example The transmission of bits over abinarycommunication channel is represented in the drawing below:                     Pr[0S ]=0.5                                        Pr[0R |0S ]=0.95                                                                 0                         0 S                                     R                                           Pr[0R |1S ]=0.10                     Pr[1S ]=0.5                                          Pr[1R |0S ]=0.05                                                                 1                         1 S                                     R                                       Pr[1R |1S ]=0.90                  Transmitter             Channel                Receiver   whereweuse notation like 0 S ,0R ... to denote events ‘‘0sent,’’ ‘‘0received,’’ etc. When a0is transmitted, it is correctly received with probability0.95 or incorrectly received with probability0.05. That is, Pr½ 0 R j 0 S  ¼0 : 95 and Pr½ 1 R j 0 S  ¼0 : 05. When a1is transmitted, it is correctly received with probability0.90 and incorrectly received with probability0.10. The probabilities of sending a0ora1are denoted by Pr½ 0 S   and Pr½ 1 S   .Itis desired to compute the probability of error for the system.    This is an application of the principle of total probability. The twoevents 0 S and 1 S are mutually exclusive and collectively exhaustive and thus form apartition. Take the event Btobethe event that an error occurs. It follows from Equation (II.10) that                      Pr[error] ¼ Pr[errorj 0 S   Pr½ 0 S  þPr[errorj 1 S   Pr½ 1 S                                   ¼  Pr½ 1 R j 0 S   Pr½ 0 S  þPr½ 0 R j 1 S   Pr½ 1 S                                  ¼ð0 : 05Þð0 : 5 Þþð 0 : 10Þð0 : 5 Þ¼0 : 075  Next, given that an error has occurred, let us compute the probabilitythat a1was sent or a0was sent. This is an application of Bayes’ rule. Fora1, Equation (II.11) becomes                                            Pr½ errorj 1 S   Pr½ 1 S                     Pr½ 1 S j error ¼                                 Pr½ errorj 1 S   Pr½ 1 S  þPr½ errorj 0 S   Pr½ 0 S    Substituting the numerical values then yields:                                            ð 0 : 10Þð0 : 5 Þ                       Pr½ 1 j error ¼                     <  0 : 667                          S         ð 0 : 10Þð0 : 5 Þþð 0 : 05Þð0 : 5 ÞII-20                                        Mathematics, Symbols, and Physical Constants  Fora0, asimilar analysis applies:                                            Pr½ errorj 0 S   Pr½ 0 S                     Pr½ 0 S j error ¼                                 Pr½ errorj 1 S   Pr½ 1 S  þPr½ errorj 0 S   Pr½ 0 S                                          ð 0 : 05Þð0 : 5 Þ                               ¼                        <  0 : 333                                 ð 0 : 10Þð0 : 5 Þþð 0 : 05Þð0 : 5 Þ     The tworesulting probabilities sum to 1because 0 S and 1 S form apartition for the experiment.  Reference  1.   C. W. Therrien and M. Tummala, Probability for Electrical and Computer Engineers.Boca Raton, FL:      CRCPress, 2004.Indexes   Author Index ........................................................................................................................................................ A-1 Subject Index ........................................................................................................................................................ S-1This page intentionally left blank                                                                            Author Index    B                                      K                                       Passas, Nikos, Computer Networks,                                                                                         Mobile Internet,  Batalama, Stella N., Low Sample Support Kazakos, Dimitri, Low Sample Support                                                                                         4 -32 to 4 -50         Adaptive Parameter Estimation           Adaptive Parameter Estimation                                                                                 Poor,Vincent H., Information Theory ,         and Packet-Data Detection for           and Packet-Data Detection for                                                                                         Signal Detection, 6 -1 to         Mobile Communications,                  Mobile Communications,                                                                                         6 -10         9 -1 to 9 -30                           9 -1 to 9 -30                                         Khosravani, Reza, Optical  C                                              Communication,Photonic                                                 Networks, 3 -18 to 3 -29        Q  Clegg, Almon H., Broadcasting,Digital                                         Kosbar,KurtL., Computer-Aided Design    Qian, Haoli, Low Sample Support         Audio Broadcasting,                                                 and Analysis of Communication           Adaptive Parameter Estimation         1 -43 to 1 -56                                                 Systems, 13-1 to 13-17                  and Packet-Data Detection for  Cover, Thomas M., Information Theory ,                                                                                         Mobile Communications,         Data Compression,               L                                                                                         9 -1 to 9 -30         6 -49 to 6 -59                                         Looney,Carl G.  D                                         Information Theory ,Noise,                                                 6 -10 to 6 -23                  R  Daigle, John N., Computer Networks,       Information Theory ,Stochastic         Computer Communication                  Processes, 6 -23 to 6 -34       Rawat, Banmali S., Bandwidth Efﬁcient         Networks, 4 -1 to 4 -14                                                         Modulation in Optical  Darcie, Thomas E., Optical             M                                               Communications,         Communication,Lightwave                                                         10-1 to 10-21                                         Maddy,Steven L., Phase-Locked Loop,     Reed, Todd R., Digital Video Processing,         Technology for Video                    11-1 to 11-9                                                                                         8 -1 to 8 -31         Transmission, 3 -1 to 3 -10     Marks, RobertJ.II, Information Theory ,                                                                                 Robrock, Richard B., II, Computer  DiFonzo,Daniel F., Satellites and              The Sampling Theorem,                                                                                         Networks,The Intelligent         Aerospace, 7 -1 to 7 -18                6 -34 to 6 -41                                                                                         Network, 4 -23 to 4 -32 Dorf, RichardC.                         McClellan, Stan, Computer Networks,                                                                                 Roden, Martin S., Broadcasting,    Broadcasting,Modulation and                  Quality of Service in                                                                                         High-DeﬁnitionTelevision,         Demodulation,  1 -1 to 1 -10            Packet-Switched Networks,                                                                                         1 -38 to 1 -43    Equalization, 2 -1 to 2 -7                   4 -50 to 4 -67                                         Musa, Sarhan M., Computer Networks,  H                                                 Local Area Networks,            S                                                 4 -14 to 4 -23  Horan, Stephen, Telemetry,         12-1 to 12-11                   P                                       Sadiku, Matthew N. O., Computer                                                                                         Networks,Local Area Networks,  I                                      Palais, Joseph C., Optical                      4 -14 to 4 -23                                                 Communication,Long Distance     Salek, Stanley, Broadcasting,Digital  Ilyas, Mohammad, Ad Hoc Wireless               Fiber Optic Communications,             Audio Broadcasting,         Networks, 5 -1 to 5 -6                  3 -10 to 3 -18                          1 -43 to 1 -56                                                                                                                   A -1A -2                                                 Broadcasting    and  Optical  Communication        Technology   Salkintzis, Apostolis K., Computer     Therrien, Charles W., Probability for   W         Networks,Mobile  Internet,              Electrical and Computer         4 -32 to 4 -50                          Engineers, II-14 to II-20       Wan,  Zhen  Seker, Remzi, Computer Networks,       Thomas,  JoyA., Information Theory ,      Broadcasting,Modulation and         Quality of Service in                   Data Compression,                       Demodulation, 1 -1 to 1 -10         Packet-Switched Networks,               6 -49 to 6 -59                    Equalization, 2 -1 to 2 -7         4 -50 to 4 -67                  Tranter,William H., Computer-Aided      Whitaker,JerryC.                                                 Design and Analysis of            Broadcasting,Radio Broadcasting, T                                               Communication  Systems,                 1 -10 to 1 -24                                                 13-1 to 13-17                     Broadcasting,Television Systems,  Tallarida, Ronald J., Mathematics,                                                     1 -24 to 1 -38         Symbols, and PhysicalConstants, V                                       Willner,Alan E., Optical Communication,         II-1 to II-13                                                                   Photonic Networks,  Tayahi, Moncef B., Bandwidth Efﬁcient  Verdu´ ,Sergio, Information Theory ,            3 -18 to 3 -29         Modulation in Optical                   Channel Capacity,         Communications, 10-1 to 10-21           6 -41 to 6 -49                                                                           Subject Index    (Quantumefﬁciency)                        ampliﬁer, 1 -10 to 1 -11             Auxiliary-vector (AV) ﬁlters, 9 -6,    deﬁned, 3-18                            antenna systems, 1 -17 to 1 -22              9 -7 to 9 -10    for photodetectors, 3 -14 to 3 -15      class B, 1 -10, 1 -11                  deﬁned, 9-28  1992 World Administrative Radio           high-level, 1 -10 to 1 -11         Conference(WARC-92),   1 -45       history, 1 -10                       B  3-D Wigner distribution (WD) and          hybrid with IBOC, 1 -53 to 1 -55         optical ﬂow, 8 -23                 modulation mapping functions,        Bandwidth efﬁcient modulation (BEM),                                                 1 -2                                    10-1 to 10-20  A                                         radio broadcasting, 1 -10 to 1 -13   Best effort(BE) class, 4 -58, 4 -59                                                 classes, 1 -10                  Binaryshift registers (BSRs), 13-8  AAA, see Authentication, authorization Annular control electrode (ACE)         Bit error rate (BER), 7 -11, 9 -2; see also         and accounting (AAA)                    pulsing, 1 -32                          Symbol error rate Adaptivedelta pulse-code modulation     Antenna systems                           estimation problems, 13-11         (ADPCM),   1 -47                   height above average terrain (HAAT), Broadband integrated servicesdigital  Adaptivedifferential pulse code                1 -27                                   network (B-ISDN), deﬁned,         modulation (ADPCM),   6 -58     Application service provider (ASP), 4 -63       4-13  Add/drop multiplexer (OADM),           Assuredforwarding (AF) class,           Broadband intelligent network (BIN),         3 -21 to 3 -22                          4 -58 to 4 -59                          4 -30  Advanced audio coding (AAC), 1 -49                                             Broadcast Satellite Service (BSS), 7 -4                                         Asymmetric DSL (ADSL),   4 -11, 4 -48  Advanced intelligent networks (AIN),                                         Asynchronous transfer mode (ATM),         4 -28 to 4 -30                                                 3 -23, 4 -10                    C    standardinEurope, 4 -30                                            deﬁned, 4-13    versions, 4 -30                                                              Call session control function (CSCF),                                         ATMadaption layer (AAL),  4 -10  Advanced ResearchProject Agency                                                        4 -37                                         Attitude determination and control         (ARPA), 4 -3                                                            Carrier/noise ratio (CNR), 1 -46, 1 -50,                                                 system (ADCS), 7 -15  AIN, see Advanced intelligent networks                                                 3 -2, 7 -11         (AIN)                           Authentication, authorization and       Carrier sense multiple access networks  Alternate billing services(ABS),               accounting (AAA), 4 -39                 (CSMA),  4 -9         4 -27 to 4 -28                     mechanisms, 4 -49                    Carrier sense multiple access with  Always-best-connected (ABC),              protocol, 4 -63                              collision detection (CSMA/CD),         4 -42, 4 -47                       protocol and DSL, 4 -65                      3 -24; see also Ethernet networks  AM, see Amplitudemodulation (AM)          signaling, 4 -40                     CCS7 network,  see Signaling system 7  Ampliﬁed spontaneous emission (ASE)    Auxiliaryservice unit (ASU), 1 -54              (CCS7)         noise, 10-11                    Auxiliary-vector (AV) estimates, 9 -9   CDMA,   see Code-division multiple  Ampliﬁer spontaneous emission (ASE),      deﬁned, 9 -28                                access (CDMA)         10-10                           Auxiliary-vector (AV) estimators,       Central ofﬁce(CO), 3 -23  Amplitudemodulation (AM),  1 -10               9 -14 to 9 -15                  Class based weighted fair queuing         to 1 -11; see also Modulation      for known channels, 9 -11 to 9 -14           (CBWFQ),  4 -60     Page on which aterm is deﬁned is indicated in bold.                                                                                                                   S -1S -2                                                 Broadcasting    and  Optical  Communication        Technology   Class of service (CoS) and Quality of  Differentiated services (DiffServ),     F         service (QoS), 4 -57                    4 -57 to 4 -58 Class of service (CoS) approach, 4 -51,    model, 4 -60 to 4 -61                FCC  (Federal Communications         4 -53                           Digital audio broadcasting (DAB), 1 -43         Commission)    in broadband, 4 -63 to 4 -65                 to 1 -56                          antenna height regulations, 1 -27    congestion control, 4 -61            Digital Audio Radio Service (DARS),       television broadcast standards,    scalability, 4 -60                           7 -4                                    1 -26 CNR,  see Carrier/noise ratio (CNR)     Digital Radio Mondiale (DRM)            FDM,  see Frequency-division C/N  ratio, see Carrier/noise ratio (CNR)       consortium, 1 -45                       multiplexed (FDM) signals Code-division multiple access (CDMA),   Digital subscriber loop (DSL)           FDMA,   see Frequency-division multiple         7 -4, 7 -14                             technology, 4 -65                       access (FDMA) Code  excited linear predictive (CELP)     for home networks, 4 -11             FEC, see Forward-error correction         algorithm, 1 -47                   separately owned components,                 (FEC) Common    channel interofﬁce signaling          4 -65                           Federal Communications  Commission,         (CCIS), 4 -5                    Digital television (DTV), 1 -10                 see FCC (Federal Common    channel interofﬁce signaling  Direct broadcast satellites (DBS),              Communications  Commission)         (CCIS), deﬁned, 4-12                    7 -4, 7 -5                      Fiber delay line (FDL), 3 -26 Common-channel    interofﬁce signaling  Direct-sequencecode-division-multiple-  Fiber distributed digital interface         (CCIS) network, 4 -25                   access (DS-CDMA)systems                 (FDDI), 4 -9 Common-channel    signaling (CCS), 4 -25        receiver improvements, 9 -2     Fiber-to-the-curb(FTTC), 3 -23    deﬁned, 4-31                         Discrete-cosine transform (DCT), 3 -3   Fiber-to-the-home (FTTH), 3 -23 Composite  second-order (CSO)           Discrete cosine transform(DCT) for      Flat in-band spectral response (FBSR),         distortion, 3 -6 to 3 -7                image compression, 6 -58                10-9 to 10-10    deﬁned, 3-9                          Discrete time (DT), 13-6                FM,  see Frequency modulation (FM) Composite  triple beat (CTB), 3 -6 to 3 -7 Distributed queue, dual bus protocol Foreign agent (FA), 4 -43    deﬁned, 3-9                                  (DQDB),  4 -9                   Forward-error correction (FEC), 10-3, Compressed  digital video (CDV), 3 -3   Dynamic  packet scheduling (DPS), 4 -60         10-5 to 10-6, 10-10, 10-17 Consumer   Electronics Association              to 4 -61                          linear block-code, 10-19         (CEA), 1 -45                                                            Four-wave-mixing (FWM),  3 -28, 10-16 Continuous  time (CT)signals, 13-6      E                                       Fractionally spacedequalizer (FSE), 2 -4 Continuous  wave(CW)   probe, 3 -27                                                     to 2 -5 Contrast sensitivityfunction (CSF),     EDFA,  see Erbium-dopedﬁber  ampliﬁer   Frame  relay (FR) technology, 4 -10         8 -12                                   (EDFA)                                  to 4 -11 CoS, see Class of service (CoS)         Effectiveradiated power (ERP), 1 -14,   Frequency-division duplex (FDD), Cross absorption modulation (XAM),              1 -24                                   7 -3 to 7 -4         3 -28                              deﬁned, 1-36                         Frequency-division multiple access Cross-gain-modulation (XGM),  3 -27        factors for, 1 -24                           (FDMA),  7 -4, 7 -13 Cross phase modulation (XPM),              for US television frequency bands,   Frequency-division multiplexed         10-16                                   1 -27                                   (FDM)  signals, 3 -2, 7 -4, 7 -14 Cross-spectrametric (CSM) receiver, 9 -9 EIRP, see Equivalent isotropically       limitations, 10-2 Cross-validated minimum-output-                 radiated power (EIRP)           Frequency hopping spread spectrum         variance(CV-MOV)   rule,        Electronic Industries Association (EIA),        (FHSS), 4 -21         9 -14 to 9 -15                          1 -45                           Frequency modulation (FM); see also                                         Equivalent isotropically radiated power         Modulation D                                               (EIRP), 7 -8, 7 -10, 7 -15        antenna systems, 1 -22 to 1 -24                                         Erbium-doped  ﬁber ampliﬁer (EDFA),       directmodulation, 1 -15  Database administration system (DBAS),         3 -9, 10-10                       exciter, 1 -15 to 1 -16         4 -28                              deﬁned, 3-9                            history, 1 -10    deﬁned, 4-31                            in optical ﬁber transmission, 3 -19    hybrid with IBOC, 1 -50 to 1 -53  Data link service data unit (DLSDU),      in smart switches, 3 -26               modulation  mapping functions,         4 -8                            ERP, see Effectiveradiated power (ERP)          1 -2  Decision-feedback equalizer (DFE),     ETSI, see EuropeanTelecommunications      and phase-locked loop, 11-8         2 -3 to 2 -4, 2 -6                      Standards Institute (ETSI)        power  ampliﬁers, 1 -16 to 1 -17  Demand  assigned multiple access       European  Telecommunications              radio broadcasting, 1 -14 to 1 -17         (DAMA),   7 -14                         Standards Institute (ETSI),       for satellite communications, 7 -4  Desired impulse response (DIR),                4 -30                             video, 3 -2         2 -6 to 2 -7                       RF wireless LANs, 4 -22              Frequency-shift keying (FSK), Difference frequency generation (DFG),     TISPAN  project, 4 -34                       1 -6 to 1 -7         3 -28                           Expedited forwarding (EF) class, 4 -58    ISI in, 2 -1 Differential groupdelay(DGD),           Extensible authentication protocol      FSK, see Frequency-shift keying         10-7, 10-8                              (EAP), 4 -63                            (FSK)Subject Index                                                                                                    S -3   G                                      Inductiveoutput tube (IOT), 1 -32         parameters, 7 -13                                            deﬁned, 1-37                           satellites, 7 -4  GatewayGPRS support node (GGSN),          klystrons, 1 -34                     Low-pass equivalent (LPE) model, 13-7         4 -39                           Industry, scientiﬁc and medical (ISM)   Low-pass equivalent (LPE) model,  Generalizedlikelihood ratio test (GLRT)        bands, 12-7                             deﬁned, 13-15    deﬁned, 9-28                         Infrared (IR) for wireless LANs,        Low-pass equivalent (LPE) waveforms,    detectors, see GLRTdetectors                 4 -20 to 4 -21                          13-7 to 13-8  Generalizedsidelobe canceller (GSC),   Institute of Radio Engineers, see IRE   Low-power television (LPTV), 1 -28         9 -6 to 9 -7                            (Institute of Radio Engineers)    deﬁned, 1-37    estimators, 9 -10 to 9 -11           Integrated services(IntServ), 4 -57     LPC, see Linear predictivecoding (LPC)  Geostationaryearth orbit (GEO), 7 -12  Integrated servicesdigital networks  GeostationaryEarth orbit (GEO)                 (ISDNs), 4 -5                   M         satellite, 7 -1 to 7 -2, 7 -3, 7 -4, Intensity-modulated (IM) light, 3 -1         7 -10                           Intensity-modulation /direct detection  M-aryfrequency shift keying (MFSK),    orbital requirements, 7 -7                   (IM/DD) ﬁber optics, 10-2               10-6, 10-7    parameters, 7 -13                    Interexchange carriers (ICs), 4 -26     M-aryphase-shift keying (MPSK),  Global Positioning System (GPS)        Intermediate frequency (IF) band, 1 -5,         1 -7 to 1 -8         satellites, 7 -2, 12-4                  1 -33                             and optical bandwidth efﬁciency, 10-6  GPRS support node (SGSN),  4 -39       Intermediate power ampliﬁer (IPA),        and power efﬁciency in optical  GPS satellites, see Global Positioning         1 -32 to 1 -34                          systems, 10-6 to 10-7         System (GPS) satellites         InternationalStandards Organization     M-aryquadratureamplitude modulation  Greek alphabet (table), II-3                   (ISO) Reference Model, deﬁned,          (QAM-XX),   10-3  GSC, see Generalized sidelobe canceller        4-12                            Masking pattern-adapteduniversal         (GSC)                           InternationalSystem of units (SI),              subband integrated coding and                                                 II-3 to II-5                            multiplexing (MUSICAM),  H                                      InternationalTelecommunications                 1 -49 to 1 -50                                                 Union (ITU) frequency           Maximum eigenvector (MaxEV)  H (horizontal), deﬁned, 1-37                   allocations, 7 -14                      receiver, 9 -9  HDTV (High-deﬁnition television),      Internet protocol (IP), 3 -23, 4 -7     Maximum-likelihood sequence         1 -38 to 1 -43, 7 -4            Inter-range instrumentation group               estimation (MLSE), 2 -6 to 2 -7    compression, 8 -26                           (IRIG), 12-7                    Mean squared error (MSE), deﬁned, 2 -2    formats, 1 -39 to 1 -40              Intersatellite links (ISL), 7 -4        Media access control (MAC) layer, 4 -9    history, 1 -38 to 1 -39              Intersymbol interference (ISI), 2 -1, 9 -4, deﬁned, 4-13    resolution, 8 -25                            9 -5, 10-2 to 10-3              Media access control (MAC) number,  Heightabove average terrain (HAAT),       deﬁned, 2 -1, 2-7                            4 -15         1 -27                           IOT, see Inductiveoutput tube (IOT)     Medium Earth orbit (MEO),  7 -12  Hierarchical mobile IP (HMIP), 4 -44,  IP multimediacorenetwork subsystem        parameters, 7 -13         4 -45                                   (IMS), 4 -34                    Medium Earth orbit (MEO) satellites, Hierarchical MRSVP (HMRSVP),            IRE (Institute of Radio Engineers), units,      7 -2, 7 -10         4 -46                                   1 -29 to 1 -30                  MEO,  see Medium Earth orbit (MEO) High  deﬁnition coder(HDC),  1 -50      IRE (unit), deﬁned, 1-37                Metropolitan area network (MAN), High-deﬁnition television, see HDTV     ISI, see Intersymbol interference(ISI)          deﬁned, 4-12         (High-deﬁnition television)                                             Metropolitan area networks (MANs), High  Earth orbit (HEO) satellites, 7 -2 L                                              3 -21, 4 -9 Highly inclined elliptical orbits (HIEO),                                       Micro-electro-mechanical-system         7 -12                           Label-switched path (LSP), 3 -23, 4 -56         (MEMS) switches, 3 -25 Home agent (HA),  4 -43                 Least-mean-square (LMS) equalizer,      Minimum-mean-square-error (MMSE) Home subscriber server(HSS), 4 -40              2 -2 to 2 -3                            optimization, 9 -3 Human   visual system (HVS), 8 -1       Least-mean-square (LMS) optimization,   Minimum-variance-distortionless-                                                 9 -3                                    response (MVDR)  I                                      Linear congruential algorithms (LCAs),          optimization, 9 -4                                                 13-8                            Mobile RSVP (MRSVP),   4 -45 to 4 -46  i.i.d. (independentand identically     Linear predictive coding (LPC), 1 -47,  Mobile station (MS), 4 -40         distributedGaussian random              6 -58                             routing, 4 -43         variables), 6 -3, 6 -7, 6 -8    Line information database (LIDB), 4 -28 Mobility anchor point (MAP), 4 -45  In-band on-channel (IBOC), see IBOC       deﬁned, 4-31                         Modiﬁed discrete cosine transform  In-band on-channel (IBOC), deﬁned,     Local areanetwork (LAN), deﬁned, 4-12           (MDCT),  1 -49         1-56                            Local areanetworks (LANs), 4 -9 to 4 -10, Modulation transfer function (MTF),  Independentand identically distributed         4 -14 to 4 -23                          8 -12         (i.i.d.) Gaussian random        Logical link control (LLC), 4 -9        Monte Carlo (MC) simulation, 13-10 to         variables, 6 -3                 Low Earth orbit (LEO), 7 -2, 7 -10              13-11S -4                                                 Broadcasting    and  Optical  Communication        Technology   Monte Carlo (MC)  simulation, deﬁned,  Optical communications using            Precision adaptivesubband coding         13-15                                   bandwidth efﬁcient modulation           (PASC), 1 -47  MPSK, see M-aryphase-shift keying              (BEM), 10-1 to 10-20            Private virtual networks (PVNs), 4 -28         (MPSK)                          Optical crossconnects (OXC),            Probabilitydensity function (pdf), 6 -23  Multiple sub-Nyquistencoding (MUSE),           3 -22 to 3 -23                  Protocol data unit (PDU), deﬁned, 4-12         1 -39                           Optical frequency-division multiplexing Protocol data units (PDUs), 4 -8    deﬁned, 1 - 43                               (OFDM),  3 -15                  Proxy-call session control function  Multi-protocollabel switching (MPLS),  Optical network units (ONUs), 3 -23             (P-CSCF), 4 -37         3 -23, 4 -56 to 4 -57           Optical packet switching (OPS), 3 -25   psdf, see Powerspectral density function  Multistage depressed collector (MSDC)  Optical signal to noise ratio (OSNR),           (psdf)         klystron, 1 -32                         10-8                            Pseudorandom  (PN)  signal and noise    deﬁned, 1-37                         Orthogonal frequency-division                   generators, 13-7 to 13-9                                                 multiplex (OFDM),  1 -50        Pulse-amplitude modulation (PAM), 1 -6  N                                      Overlaylabel-switched paths (LSP),      Pulse amplitude modulation (PAM), 2 -1,                                                 4 -56                                   10-11, 10-11 to 10-16  National Television System Committee,                                          Pulse amplitude modulation (PAM)         see NTSC  (National Television  P                                               systems, 10-11 to 10-16         system Committee)                                                       Pulse-coded modulation (PCM),  12-1 Network  access provider (NAP), 4 -63   Packetdata gateway(PDG),  4 -40         Pulse-code modulation (PCM), 1 -6, 1 -47 Network  and service management         Packetdata network (PDN),  4 -41        Pulse codemodulation  (PCM), 10-3         system (NSMS),  4 -47 to 4 -48  Packet-error-rate (PER), 9 -2           Pulse-code modulation (PCM), 10-3,  Network architecture                   Packet-switched networks, quality of            12-1    alternate billing services (ABS), 4 -27      service (QoS), 4 -50 to 4 -66     data transmission, 12-3         to 4 -28                        PAL(Phase  alteration each line), 1 -39 Pulse-duration modulation (PDM),  Network interfacecard(NIC), 4 -15         standard, 1 -26                              see PDM  (Pulse-duration  Network service access point (NSAP),   PAM,  see Pulse amplitude modulation            modulation)         4 -65                                   (PAM)                           Pulse-width modulation (PWM),  Network service provider (NSP), 4 -63  Passiveoptical networks (PONs), 3 -23           see PDM  (Pulse-duration  Non-return-to-zero(NRZ)                PCM,  see Pulse-code modulation (PCM)           modulation)    codes, 3 -17                         PD, see Polarization division (PD)      PWM   (Pulse-width modulation),    modulation, 10-2                     pdf, see Probabilitydensity function            see PDM  (Pulse-duration  NTSC  (National Television System              (pdf)                                   modulation)         Committee)                      PDM   (Pulse-duration modulation), 1 -11    band-sharing colorsignal system, 1 -30       to 1 -13                        Q    resolution, 8 -2                     Perceptual audio coder(PAC), 1 -49    standard, 1 -26                      Per-hop behaviors (PHBs), 4 -57 to 4 -58 QAM, see Quadratureamplitude    standards, 1 -39                     Phase alteration each line, see PAL(Phase       modulation (QAM)    television broadcast standards,              alteration each line)           QPSK,  see Quadraturephase shift keying         1 -26                           Phase-locked loop (PLL), 1 -15,                 (QPSK) Numbering   plan areas (NPAs), 4 -27            11-1 to 11-8                    Quadratureamplitude  modulation Numericallycontrolledoscillator (NCO)   Phase modulation (PM)                           (QAM),  1 -8 to 1 -9, 1 -42, 10-3,         compiler, 10-17                    modulation mapping functions, 1 -2           10-7, 10-17 to 10-19                                            and phase-locked loop, 11-8            for CDV, 3 -3 O                                       Phase modulation (PM)  and phase-         ISI in, 2 -1                                                 lockedloop, 11-8                  for satellite communications, 7 -4  Offset quadraturephase-shift keyed     Physical media dependent (PMD)  layer,  Quadraturephase  shift keying (QPSK),         (OQPSK)   modulation, 13-12             4 -11                                   1 -46 On  and offkeying (OOK), 10-7, 10-11,   Plain old telephone (POTS) number,        modulation, 1 -50         10-18                                   4 -26 to 4 -27                    for satellite communications, 7 -4    data formats, 3 -27                  PM,  see Phase modulation (PM)          Quality of service (QoS)    modulation, 10-2, 10-17              Point-to-pointprotocol (PPP), 4 -11       and Access network, 4 -66 OOK,   see On and offkeying (OOK)       Polarization division (PD) for satellite  in broadband, 4 -63 to 4 -65 Open  shortest path ﬁrst (OSPF), 4 -55          communications, 7 -4              and Class of service (CoS), 4 -57 Open  systems interconnection (OSI)     Polarization mode dispersion (PMD),       congestion control, 4 -61         model, 4 -55                            10-7 to 10-9                      criteria, 4 -48 Operator servicessystem (OSS),          Power  spectral density function (psdf),  deﬁned, 4 -50         4 -27 to 4 -28                          6 -12, 6 -13, 6 -29               for mobile Internet, 4 -45 to 4 -46 Optical add dropmultiplexing (OADM),       deﬁned, 6-33                           in packet-switched networks,         3 -21 to 3 -22, 10-10              and linear transformations, 6 -12            4 -50 to 4 -66 Optical burst switching (OBS), 3 -25    Practical Internet referencemodel         per ﬂow  state, 4 -60 Optical carrier level-1 (OC-1), 3 -23           (PIRM), 4 -5                      protocols affecting, 4 -63Subject Index                                                                                                    S -5     provisioning, 4 -61 to 4 -62         Simple mail transfer protocol (SMTP),   Transaction capability application part    for wireless Internet, 4 -34                 4 -4                                    (TCAP), 4 -30    and WLAN technology,  4 -64 to 4 -65 Single-mode ﬁber (SMF), deﬁned, 3-18    Transmission control protocol (TCP),                                         Single sideband (SSB), 10-19; see also          4 -8  R                                              SSB                             Transmitter power output (TPO),                                         Single-sideband vestigial (VSB)ﬁltering,        1 -14  Radio frequency (RF) for wireless LANs,        3 -2                            Transport service data unit (TSDU), 4 -8         4 -20 to 4 -21                  Space-division multiple access (SDMA),  Traveling wavetube (TWT) ampliﬁer,  Random variable (rv), 6 -23                    7 -4, 7 -13                             7 -13, 7 -15  Receivers                              Spatial-hole burning (SHB), 3 -7        Type-of-service (ToS) byte, 4 -58    linear predictivecoding (LPC), 2 -5  Speciﬁcationsand quality of service  Receiving system ﬁgure of merit (G/T),         (QoS), 4 -53                    U         7 -8                            Spread spectrum (SS), 4 -20 to 4 -21, 9 -2  Recursive-least-square(RLS)            Standardsingle mode ﬁber (SSMF),         optimization, 9 -3                      10-12                           UMTS;  see also Universal mobile  Reference frequency (Fref), 11-8       Stimulated Brillouin scattering (SBS),          telecommunications system  Relative intensity noise (RIN), 3 -2, 3 -5     3 -9                                    (UMTS)    deﬁned, 3-9                          Subcarrier multiplexing (SCM), 10-16    UMTS terrestrial radio access (UTRA),  RequiredOSNR level (ROSNR),  10-10     SubsidiaryCommunications                        4 -39  Resonance-enhanced distortion (RD),            Authorization (SCA) services,   Units (SI), II-3 to II-5         3 -7, 3 -8                              1 -16                           Universal mobile telecommunications  Resourcereservation protocol (RSVP),   Surface-acoustic-wave (SAW) ﬁlter, 1 -33        system (UMTS),  4 -63 to 4 -65;         4 -45, 4 -45 to 4 -46, 4 -57    Switched multi-megabit data services            see also UMTS    mobility proxy, 4 -46                        (SMDS),  4 -9                   Universal personal communications    RESV message, 4 -46                  Symbol error rate; see also Bit error rate      services(UPCS), 4 -11    scalability, 4 -60                           (BER)                           Upper layer protocol (ULP), 4 -7  Return-to-zero(RZ) coding, 3 -17, 10-15 Synchronous digital hierarchy(SDH),  Root-mean-square (rms) error, 6 -23            3 -23                           V  Root-mean-square (rms) voltage, 6 -12  Synchronous optical network (SONET)  RSVP, see Resource reservation protocol        protocols, 3 -23                Vector quantization (VQ), 3 -3, 6 -56         (RSVP)                          Synchronous transport signal level-1      deﬁned, 6-59                                                 (STS-1), 3 -23                  Vector selectable excited linear predictive  S                                                                                      (VSELP) algorithm, 1 -47                                         T                                       Very small apertureterminals (VSAT),  Sample-matrix-inversion (SMI), 9 -3                                                    7 -3  SECAM  (Sequential colorwith (avec)    TDM,   see Time-division multiplexing   Virtual local area network (VLAN), 4 -18         memory),  1 -26, 1 -39                  (TDM)                           VoiceoverIP(VOIP),   4 -34  Semi-analytic (SA) technique, 13-11    TDMA,   see Time-division multiple      Voltage-controlled oscillator (VCO), Semiconductor optical ampliﬁers                 access (TDMA)                           11-1         (SOAs), 3 -27                   Telemetrytracking and command           VQ, see Vector quantization (VQ)  Sequentialcolorwith (avec) memory, see         (TT&C),  7 -15                  VSAT, see Very small aperture terminals         SECAM   (Sequential colorwith   Terminal station management system              (VSAT)         (avec) memory)                          (TSMS), 4 -47 to 4 -48  Servicecontrol points (SCPs), 4 -25, 4 -26 Third generation partnership program W    deﬁned, 4-31                                 (3GPP) technologies, 4 -64  Servicelevel agreement (SLA), 4 -51    Time-division duplex (TDD) signals, 7 -4 Wavelength-division-multiplexing  Servicemanagement system (SMS), 4 -27  Time-division multiple access (TDMA)            (WDM),   3 -14, 3 -15, 3 -19, 10-1,    deﬁned, 4-31                                 for satellite communications,           10-16  Serviceswitching point (SSP), 4 -26 to         7 -4, 7 -13 to 7 -14              bandwidth efﬁciency, 10-7         4 -27                           Time-division multiplexing (TDM),         limitations, 10-2  Serving call session control function          3 -3, 3 -15, 3 -20, 7 -4          MPLS technology,  3 -23         (S-CSCF), 4 -37                    bandwidth efﬁciency, 10-7              orthogonal to SCM, 10-18  Session initiation protocol (SIP), 4 -34  for CDV, 3 -3                        WDM,   see Wavelength-division-  Signaling system 7(CCS7), deﬁned, 4-31    and data transmission, 12-5                  multiplexing (WDM)  Signaling system 7(CCS7) protocol, 4 -25  limitations, 10-2                    Weakly stationary(ws) process, 6 -11,         to 4 -26                           orthogonal to SCM, 10-18                     6 -13  Signal-to-interference-plus-noise ratio Torroidal current transformer (TCT),     cross-correlation of, 6 -31         (SINR), 9 -2                            1 -20 to 1 -21                    linear ﬁltering of, 6 -28 to 6 -30  Signal transfer points (STPs), 4 -25 to Tracking and data relaysatellite system  models, 6 -28         4 -26                                   (TDRSS), 7 -4                   Weakly stationary(ws) random process    deﬁned, 4-31                         Trafﬁc speciﬁcation (TSPEC), 4 -42              (signal), deﬁned, 6-22S -6                                                 Broadcasting    and  Optical  Communication        Technology   Weighted fair queuing (WFQ), 4 -59 to     access gateway(WAG), 4 -40 to 4 -41  World Radiocommunications         4 -60                              with cellular networks,                      Conferences (WRC), 7 -14  Whitened matched ﬁlter (WMF), 2 -6             4 -39 to 4 -42                  ws process, see Weakly stationary (ws) Wide-area network (WAN),  deﬁned,          and quality of service (QoS), 4 -64 to       process         4-12                                    4 -65                           ws random  process (signal), deﬁned, Wide-area networks (WANs),  3 -18, 3 -21   topology, 4 -18 to 4 -23                     6-22 Wigner  distribution (WD), 8 -17, 8 -23    transmission, 4 -20 to 4 -21 Wireless Ethernet Compatibility Alliance Wireless LAN Alliance(WLANA),          Z         (WECA),  4 -22                          4 -22 Wireless intelligentnetwork (WIN), 4 -30 Wireless personal area networks        Zero-forcing (ZF) equalizer, Wireless LAN  (WLAN)                            (WPANs),  4 -32                         2 -1 to 2 -3